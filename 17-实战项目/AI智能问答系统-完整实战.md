# AIæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - å®Œæ•´å®æˆ˜

> **ä½œè€…**: erik.zhou  
> **åˆ›å»ºæ—¶é—´**: 2025-01-31  
> **éš¾åº¦**: â­â­â­â­â­  
> **æŠ€æœ¯æ ˆ**: Spring Boot 3.x, LangChain4j, OpenAI, Pinecone, Redis, PostgreSQL

## ğŸ“‹ ç›®å½•

- [1. é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
- [2. ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
- [3. æ ¸å¿ƒéš¾ç‚¹](#3-æ ¸å¿ƒéš¾ç‚¹)
- [4. æŠ€æœ¯å®ç°](#4-æŠ€æœ¯å®ç°)
- [5. æ€§èƒ½ä¼˜åŒ–](#5-æ€§èƒ½ä¼˜åŒ–)
- [6. éƒ¨ç½²æ–¹æ¡ˆ](#6-éƒ¨ç½²æ–¹æ¡ˆ)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä¸šåŠ¡åœºæ™¯

æ„å»ºä¸€ä¸ªä¼ä¸šçº§AIæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒ:
- ğŸ“š å¤šæ–‡æ¡£çŸ¥è¯†åº“ç®¡ç†
- ğŸ¤– æ™ºèƒ½è¯­ä¹‰æœç´¢
- ğŸ’¬ ä¸Šä¸‹æ–‡å¯¹è¯
- ğŸ” ç­”æ¡ˆæº¯æº
- ğŸ“Š ä½¿ç”¨åˆ†æ

### 1.2 æ ¸å¿ƒåŠŸèƒ½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AIæ™ºèƒ½é—®ç­”ç³»ç»Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  æ–‡æ¡£ç®¡ç†    â†’  å‘é‡åŒ–  â†’  å­˜å‚¨  â†’  æ£€ç´¢  â†’  ç”Ÿæˆ   â”‚
â”‚  (Upload)      (Embed)   (Store)  (Search) (Answer)â”‚
â”‚                                                     â”‚
â”‚  â”œâ”€ PDFè§£æ                                         â”‚
â”‚  â”œâ”€ Wordè§£æ                                        â”‚
â”‚  â”œâ”€ Markdownè§£æ                                    â”‚
â”‚  â””â”€ ç½‘é¡µæŠ“å–                                        â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æŠ€æœ¯é€‰å‹

| ç»„ä»¶ | æŠ€æœ¯ | è¯´æ˜ |
|------|------|------|
| åç«¯æ¡†æ¶ | Spring Boot 3.2 | ä¼ä¸šçº§æ¡†æ¶ |
| AIæ¡†æ¶ | LangChain4j | Java LLMæ¡†æ¶ |
| LLM | OpenAI GPT-4 | å¤§è¯­è¨€æ¨¡å‹ |
| å‘é‡æ•°æ®åº“ | Pinecone | å‘é‡å­˜å‚¨ |
| ç¼“å­˜ | Redis | ç»“æœç¼“å­˜ |
| æ•°æ®åº“ | PostgreSQL | å…ƒæ•°æ®å­˜å‚¨ |
| æ¶ˆæ¯é˜Ÿåˆ— | Kafka | å¼‚æ­¥å¤„ç† |

---

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯åº”ç”¨    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            API Gateway (Spring Cloud)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AIé—®ç­”æœåŠ¡ (Spring Boot)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ æ–‡æ¡£å¤„ç†   â”‚  â”‚  å¯¹è¯ç®¡ç†  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Pinecone   â”‚    â”‚   OpenAI    â”‚
â”‚  å‘é‡æ•°æ®åº“  â”‚    â”‚     API     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµ

```
1. æ–‡æ¡£ä¸Šä¼  â†’ 2. æ–‡æœ¬æå– â†’ 3. åˆ†å— â†’ 4. å‘é‡åŒ– â†’ 5. å­˜å‚¨
                                                      â†“
6. ç”¨æˆ·æé—® â† 7. ç”Ÿæˆç­”æ¡ˆ â† 8. LLMå¤„ç† â† 9. æ£€ç´¢ç›¸å…³æ–‡æ¡£
```

---

## 3. æ ¸å¿ƒéš¾ç‚¹

### 3.1 éš¾ç‚¹ä¸€: å¤§æ–‡æ¡£å¤„ç†ä¸åˆ†å—ç­–ç•¥

**æŒ‘æˆ˜**:
- æ–‡æ¡£å¯èƒ½è¶…è¿‡100MB
- éœ€è¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- åˆ†å—å¤§å°å½±å“æ£€ç´¢æ•ˆæœ

**è§£å†³æ–¹æ¡ˆ**:

```java
package com.example.ai.qa.document;

import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.DocumentSplitter;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.segment.TextSegment;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;

/**
 * æ™ºèƒ½æ–‡æ¡£åˆ†å—æœåŠ¡
 * 
 * éš¾ç‚¹: å¦‚ä½•åœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶ï¼Œåˆç†åˆ†å‰²å¤§æ–‡æ¡£
 * 
 * è§£å†³æ–¹æ¡ˆ:
 * 1. é€’å½’å­—ç¬¦åˆ†å‰² - æŒ‰æ®µè½ã€å¥å­ã€å•è¯å±‚çº§åˆ†å‰²
 * 2. é‡å ç­–ç•¥ - ç‰‡æ®µé—´ä¿ç•™é‡å ï¼Œé¿å…è¯­ä¹‰æ–­è£‚
 * 3. å…ƒæ•°æ®ä¿ç•™ - è®°å½•åŸæ–‡ä½ç½®ï¼Œä¾¿äºæº¯æº
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
public class DocumentChunkingService {
    
    /**
     * æ™ºèƒ½åˆ†å—ç­–ç•¥
     * 
     * å‚æ•°è¯´æ˜:
     * - chunkSize: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•° (500-1000ä¸ºæœ€ä½³)
     * - chunkOverlap: ç‰‡æ®µé—´é‡å å­—ç¬¦æ•° (10-20%ä¸ºæœ€ä½³)
     */
    public List<TextSegment> chunkDocument(Document document) {
        log.info("å¼€å§‹åˆ†å—æ–‡æ¡£: {}", document.metadata("source"));
        
        // 1. æ ¹æ®æ–‡æ¡£å¤§å°é€‰æ‹©åˆ†å—ç­–ç•¥
        int documentSize = document.text().length();
        DocumentSplitter splitter;
        
        if (documentSize < 10000) {
            // å°æ–‡æ¡£: ç®€å•åˆ†å‰²
            splitter = DocumentSplitters.recursive(500, 50);
        } else if (documentSize < 100000) {
            // ä¸­ç­‰æ–‡æ¡£: æ ‡å‡†åˆ†å‰²
            splitter = DocumentSplitters.recursive(800, 100);
        } else {
            // å¤§æ–‡æ¡£: å¤§å—åˆ†å‰²
            splitter = DocumentSplitters.recursive(1000, 150);
        }
        
        // 2. æ‰§è¡Œåˆ†å—
        List<TextSegment> segments = splitter.split(document);
        
        // 3. ä¸ºæ¯ä¸ªç‰‡æ®µæ·»åŠ å…ƒæ•°æ®
        for (int i = 0; i < segments.size(); i++) {
            TextSegment segment = segments.get(i);
            segment.metadata().put("chunk_index", i);
            segment.metadata().put("total_chunks", segments.size());
            segment.metadata().put("document_id", document.metadata("id"));
            segment.metadata().put("source", document.metadata("source"));
        }
        
        log.info("æ–‡æ¡£åˆ†å—å®Œæˆ: {} ä¸ªç‰‡æ®µ", segments.size());
        return segments;
    }
    
    /**
     * è¯­ä¹‰æ„ŸçŸ¥åˆ†å— - æŒ‰æ®µè½åˆ†å‰²
     */
    public List<TextSegment> semanticChunking(Document document) {
        String text = document.text();
        List<TextSegment> segments = new ArrayList<>();
        
        // æŒ‰æ®µè½åˆ†å‰²
        String[] paragraphs = text.split("\n\n+");
        
        StringBuilder currentChunk = new StringBuilder();
        int chunkIndex = 0;
        
        for (String paragraph : paragraphs) {
            // å¦‚æœå½“å‰å—+æ–°æ®µè½è¶…è¿‡é˜ˆå€¼ï¼Œä¿å­˜å½“å‰å—
            if (currentChunk.length() + paragraph.length() > 800) {
                if (currentChunk.length() > 0) {
                    segments.add(createSegment(currentChunk.toString(), chunkIndex++, document));
                    currentChunk = new StringBuilder();
                }
            }
            
            currentChunk.append(paragraph).append("\n\n");
        }
        
        // ä¿å­˜æœ€åä¸€å—
        if (currentChunk.length() > 0) {
            segments.add(createSegment(currentChunk.toString(), chunkIndex, document));
        }
        
        return segments;
    }
    
    private TextSegment createSegment(String text, int index, Document document) {
        TextSegment segment = TextSegment.from(text);
        segment.metadata().put("chunk_index", index);
        segment.metadata().put("document_id", document.metadata("id"));
        return segment;
    }
}
```

### 3.2 éš¾ç‚¹äºŒ: æ··åˆæ£€ç´¢ç­–ç•¥

**æŒ‘æˆ˜**:
- çº¯å‘é‡æ£€ç´¢å¯èƒ½é—æ¼å…³é”®è¯
- çº¯å…³é”®è¯æ£€ç´¢ç¼ºä¹è¯­ä¹‰ç†è§£
- éœ€è¦å¹³è¡¡ç²¾ç¡®åº¦å’Œå¬å›ç‡

**è§£å†³æ–¹æ¡ˆ**:

```java
package com.example.ai.qa.retrieval;

import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingMatch;
import dev.langchain4j.store.embedding.EmbeddingStore;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.stream.Collectors;

/**
 * æ··åˆæ£€ç´¢æœåŠ¡
 * 
 * éš¾ç‚¹: å¦‚ä½•ç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ï¼Œæé«˜æ£€ç´¢è´¨é‡
 * 
 * è§£å†³æ–¹æ¡ˆ:
 * 1. å‘é‡æ£€ç´¢ - è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
 * 2. BM25æ£€ç´¢ - å…³é”®è¯åŒ¹é…
 * 3. é‡æ’åº - ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ–°æ’åº
 * 4. å€’æ•°æ’åèåˆ(RRF) - èåˆå¤šä¸ªæ£€ç´¢ç»“æœ
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class HybridRetrievalService {
    
    private final EmbeddingModel embeddingModel;
    private final EmbeddingStore<TextSegment> embeddingStore;
    private final KeywordSearchService keywordSearchService;
    
    /**
     * æ··åˆæ£€ç´¢
     * 
     * ç®—æ³•: å€’æ•°æ’åèåˆ(Reciprocal Rank Fusion)
     * RRF(d) = Î£ 1 / (k + rank_i(d))
     * å…¶ä¸­ k=60 æ˜¯å¸¸æ•°ï¼Œrank_i(d) æ˜¯æ–‡æ¡£dåœ¨ç¬¬iä¸ªæ£€ç´¢ç»“æœä¸­çš„æ’å
     */
    public List<TextSegment> hybridSearch(String query, int topK) {
        log.info("æ‰§è¡Œæ··åˆæ£€ç´¢: query={}, topK={}", query, topK);
        
        // 1. å‘é‡æ£€ç´¢
        List<ScoredSegment> vectorResults = vectorSearch(query, topK * 2);
        log.debug("å‘é‡æ£€ç´¢ç»“æœ: {} ä¸ª", vectorResults.size());
        
        // 2. å…³é”®è¯æ£€ç´¢
        List<ScoredSegment> keywordResults = keywordSearch(query, topK * 2);
        log.debug("å…³é”®è¯æ£€ç´¢ç»“æœ: {} ä¸ª", keywordResults.size());
        
        // 3. RRFèåˆ
        Map<String, Double> rrfScores = calculateRRFScores(vectorResults, keywordResults);
        
        // 4. æŒ‰RRFåˆ†æ•°æ’åºå¹¶è¿”å›Top K
        return rrfScores.entrySet().stream()
            .sorted(Map.Entry.<String, Double>comparingByValue().reversed())
            .limit(topK)
            .map(entry -> findSegmentById(entry.getKey(), vectorResults, keywordResults))
            .filter(Objects::nonNull)
            .collect(Collectors.toList());
    }
    
    /**
     * å‘é‡æ£€ç´¢
     */
    private List<ScoredSegment> vectorSearch(String query, int topK) {
        // ç”ŸæˆæŸ¥è¯¢å‘é‡
        Embedding queryEmbedding = embeddingModel.embed(query).content();
        
        // å‘é‡ç›¸ä¼¼åº¦æœç´¢
        List<EmbeddingMatch<TextSegment>> matches = embeddingStore.findRelevant(
            queryEmbedding,
            topK,
            0.6  // æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        );
        
        return matches.stream()
            .map(match -> new ScoredSegment(
                match.embedded(),
                match.score(),
                "vector"
            ))
            .collect(Collectors.toList());
    }
    
    /**
     * å…³é”®è¯æ£€ç´¢ (BM25)
     */
    private List<ScoredSegment> keywordSearch(String query, int topK) {
        return keywordSearchService.search(query, topK);
    }
    
    /**
     * è®¡ç®—RRFåˆ†æ•°
     */
    private Map<String, Double> calculateRRFScores(
        List<ScoredSegment> vectorResults,
        List<ScoredSegment> keywordResults
    ) {
        Map<String, Double> rrfScores = new HashMap<>();
        int k = 60;  // RRFå¸¸æ•°
        
        // å‘é‡æ£€ç´¢ç»“æœçš„RRFåˆ†æ•°
        for (int i = 0; i < vectorResults.size(); i++) {
            String segmentId = vectorResults.get(i).segment().text();
            double score = 1.0 / (k + i + 1);
            rrfScores.merge(segmentId, score, Double::sum);
        }
        
        // å…³é”®è¯æ£€ç´¢ç»“æœçš„RRFåˆ†æ•°
        for (int i = 0; i < keywordResults.size(); i++) {
            String segmentId = keywordResults.get(i).segment().text();
            double score = 1.0 / (k + i + 1);
            rrfScores.merge(segmentId, score, Double::sum);
        }
        
        return rrfScores;
    }
    
    private TextSegment findSegmentById(
        String id,
        List<ScoredSegment> vectorResults,
        List<ScoredSegment> keywordResults
    ) {
        return vectorResults.stream()
            .map(ScoredSegment::segment)
            .filter(seg -> seg.text().equals(id))
            .findFirst()
            .or(() -> keywordResults.stream()
                .map(ScoredSegment::segment)
                .filter(seg -> seg.text().equals(id))
                .findFirst())
            .orElse(null);
    }
    
    record ScoredSegment(TextSegment segment, double score, String source) {}
}
```


### 3.3 éš¾ç‚¹ä¸‰: å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†

**æŒ‘æˆ˜**:
- å¤šè½®å¯¹è¯éœ€è¦è®°ä½å†å²
- Tokené™åˆ¶(GPT-4: 8K/32K)
- ä¸Šä¸‹æ–‡çª—å£ç®¡ç†

**è§£å†³æ–¹æ¡ˆ**:

```java
package com.example.ai.qa.conversation;

import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.ChatMessage;
import dev.langchain4j.data.message.SystemMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.TimeUnit;

/**
 * å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†æœåŠ¡
 * 
 * éš¾ç‚¹: å¦‚ä½•åœ¨Tokené™åˆ¶ä¸‹ï¼Œæœ‰æ•ˆç®¡ç†å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
 * 
 * è§£å†³æ–¹æ¡ˆ:
 * 1. æ»‘åŠ¨çª—å£ - åªä¿ç•™æœ€è¿‘Nè½®å¯¹è¯
 * 2. æ‘˜è¦å‹ç¼© - å°†å†å²å¯¹è¯å‹ç¼©æˆæ‘˜è¦
 * 3. RedisæŒä¹…åŒ– - æ”¯æŒè·¨ä¼šè¯æ¢å¤
 * 4. Tokenè®¡æ•° - åŠ¨æ€è°ƒæ•´çª—å£å¤§å°
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
public class ConversationContextService {
    
    private final RedisTemplate<String, List<ChatMessage>> redisTemplate;
    private static final String CONTEXT_KEY_PREFIX = "conversation:context:";
    private static final int MAX_MESSAGES = 10;  // æœ€å¤šä¿ç•™10è½®å¯¹è¯
    private static final int MAX_TOKENS = 6000;  // æœ€å¤§Tokenæ•°
    
    public ConversationContextService(RedisTemplate<String, List<ChatMessage>> redisTemplate) {
        this.redisTemplate = redisTemplate;
    }
    
    /**
     * è·å–å¯¹è¯ä¸Šä¸‹æ–‡
     */
    public List<ChatMessage> getContext(String conversationId) {
        String key = CONTEXT_KEY_PREFIX + conversationId;
        List<ChatMessage> messages = redisTemplate.opsForValue().get(key);
        
        if (messages == null) {
            messages = new ArrayList<>();
            // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            messages.add(SystemMessage.from("""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
                
                å›ç­”è¦æ±‚:
                1. åªåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”
                2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
                3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
                4. ä¿æŒå›ç­”ç®€æ´å‡†ç¡®
                """));
        }
        
        return messages;
    }
    
    /**
     * æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
     */
    public void addMessage(String conversationId, ChatMessage message) {
        List<ChatMessage> messages = getContext(conversationId);
        messages.add(message);
        
        // æ£€æŸ¥Tokenæ•°é‡ï¼Œå¦‚æœè¶…é™åˆ™å‹ç¼©
        if (estimateTokens(messages) > MAX_TOKENS) {
            messages = compressContext(messages);
        }
        
        // ä¿å­˜åˆ°Redis (30åˆ†é’Ÿè¿‡æœŸ)
        String key = CONTEXT_KEY_PREFIX + conversationId;
        redisTemplate.opsForValue().set(key, messages, 30, TimeUnit.MINUTES);
    }
    
    /**
     * å‹ç¼©ä¸Šä¸‹æ–‡ - ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„å¯¹è¯
     */
    private List<ChatMessage> compressContext(List<ChatMessage> messages) {
        log.info("å‹ç¼©å¯¹è¯ä¸Šä¸‹æ–‡: åŸå§‹æ¶ˆæ¯æ•°={}", messages.size());
        
        List<ChatMessage> compressed = new ArrayList<>();
        
        // 1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        messages.stream()
            .filter(msg -> msg instanceof SystemMessage)
            .findFirst()
            .ifPresent(compressed::add);
        
        // 2. ä¿ç•™æœ€è¿‘çš„Nè½®å¯¹è¯
        List<ChatMessage> recentMessages = messages.stream()
            .filter(msg -> !(msg instanceof SystemMessage))
            .skip(Math.max(0, messages.size() - MAX_MESSAGES))
            .toList();
        
        compressed.addAll(recentMessages);
        
        log.info("å‹ç¼©åæ¶ˆæ¯æ•°={}", compressed.size());
        return compressed;
    }
    
    /**
     * ä¼°ç®—Tokenæ•°é‡ (ç²—ç•¥ä¼°ç®—: 1 token â‰ˆ 4 å­—ç¬¦)
     */
    private int estimateTokens(List<ChatMessage> messages) {
        int totalChars = messages.stream()
            .mapToInt(msg -> msg.text().length())
            .sum();
        return totalChars / 4;
    }
    
    /**
     * æ¸…é™¤å¯¹è¯ä¸Šä¸‹æ–‡
     */
    public void clearContext(String conversationId) {
        String key = CONTEXT_KEY_PREFIX + conversationId;
        redisTemplate.delete(key);
    }
}
```

---

## 4. æŠ€æœ¯å®ç°

### 4.1 æ–‡æ¡£ä¸Šä¼ ä¸å¤„ç†

```java
package com.example.ai.qa.document;

import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.loader.FileSystemDocumentLoader;
import dev.langchain4j.data.document.parser.apache.pdfbox.ApachePdfBoxDocumentParser;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingStore;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.web.multipart.MultipartFile;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import java.util.UUID;

/**
 * æ–‡æ¡£ä¸Šä¼ å¤„ç†æœåŠ¡
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class DocumentUploadService {
    
    private final DocumentChunkingService chunkingService;
    private final EmbeddingModel embeddingModel;
    private final EmbeddingStore<TextSegment> embeddingStore;
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final DocumentRepository documentRepository;
    
    /**
     * ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£
     */
    public String uploadDocument(MultipartFile file, String userId) throws IOException {
        log.info("å¼€å§‹å¤„ç†æ–‡æ¡£: filename={}, size={}", file.getOriginalFilename(), file.getSize());
        
        // 1. ä¿å­˜æ–‡ä»¶
        String documentId = UUID.randomUUID().toString();
        Path tempFile = Files.createTempFile("doc-", file.getOriginalFilename());
        file.transferTo(tempFile);
        
        // 2. ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
        DocumentEntity entity = new DocumentEntity();
        entity.setId(documentId);
        entity.setFilename(file.getOriginalFilename());
        entity.setUserId(userId);
        entity.setStatus("PROCESSING");
        documentRepository.save(entity);
        
        // 3. å¼‚æ­¥å¤„ç†æ–‡æ¡£ (å‘é€åˆ°Kafka)
        kafkaTemplate.send("document-processing", documentId, tempFile.toString());
        
        log.info("æ–‡æ¡£å·²æäº¤å¤„ç†: documentId={}", documentId);
        return documentId;
    }
    
    /**
     * å¤„ç†æ–‡æ¡£ (Kafkaæ¶ˆè´¹è€…è°ƒç”¨)
     */
    public void processDocument(String documentId, String filePath) {
        try {
            log.info("å¼€å§‹å¤„ç†æ–‡æ¡£: documentId={}", documentId);
            
            // 1. åŠ è½½æ–‡æ¡£
            Document document = FileSystemDocumentLoader.loadDocument(
                Path.of(filePath),
                new ApachePdfBoxDocumentParser()
            );
            document.metadata().put("id", documentId);
            
            // 2. åˆ†å—
            List<TextSegment> segments = chunkingService.chunkDocument(document);
            
            // 3. æ‰¹é‡ç”ŸæˆåµŒå…¥
            List<Embedding> embeddings = segments.stream()
                .map(segment -> embeddingModel.embed(segment).content())
                .toList();
            
            // 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            embeddingStore.addAll(embeddings, segments);
            
            // 5. æ›´æ–°çŠ¶æ€
            documentRepository.updateStatus(documentId, "COMPLETED");
            
            log.info("æ–‡æ¡£å¤„ç†å®Œæˆ: documentId={}, segments={}", documentId, segments.size());
            
        } catch (Exception e) {
            log.error("æ–‡æ¡£å¤„ç†å¤±è´¥: documentId={}", documentId, e);
            documentRepository.updateStatus(documentId, "FAILED");
        }
    }
}
```

### 4.2 æ™ºèƒ½é—®ç­”æœåŠ¡

```java
package com.example.ai.qa.service;

import com.example.ai.qa.conversation.ConversationContextService;
import com.example.ai.qa.retrieval.HybridRetrievalService;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.ChatMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.input.Prompt;
import dev.langchain4j.model.input.PromptTemplate;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * æ™ºèƒ½é—®ç­”æœåŠ¡
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class QuestionAnsweringService {
    
    private final HybridRetrievalService retrievalService;
    private final ConversationContextService contextService;
    private final ChatLanguageModel chatModel;
    
    /**
     * å›ç­”é—®é¢˜
     */
    public AnswerResponse answer(String conversationId, String question) {
        log.info("æ”¶åˆ°é—®é¢˜: conversationId={}, question={}", conversationId, question);
        
        // 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        List<TextSegment> relevantDocs = retrievalService.hybridSearch(question, 5);
        log.debug("æ£€ç´¢åˆ° {} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ", relevantDocs.size());
        
        // 2. æ„å»ºæç¤ºè¯
        String context = buildContext(relevantDocs);
        String prompt = buildPrompt(question, context);
        
        // 3. è·å–å¯¹è¯å†å²
        List<ChatMessage> history = contextService.getContext(conversationId);
        history.add(UserMessage.from(prompt));
        
        // 4. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        String answer = chatModel.generate(history).content().text();
        
        // 5. ä¿å­˜å¯¹è¯å†å²
        contextService.addMessage(conversationId, UserMessage.from(question));
        contextService.addMessage(conversationId, AiMessage.from(answer));
        
        // 6. æ„å»ºå“åº”
        List<Source> sources = relevantDocs.stream()
            .map(seg -> new Source(
                seg.metadata("document_id").toString(),
                seg.metadata("source").toString(),
                seg.text().substring(0, Math.min(100, seg.text().length()))
            ))
            .collect(Collectors.toList());
        
        return new AnswerResponse(answer, sources);
    }
    
    /**
     * æ„å»ºä¸Šä¸‹æ–‡
     */
    private String buildContext(List<TextSegment> segments) {
        return segments.stream()
            .map(seg -> String.format("""
                æ–‡æ¡£æ¥æº: %s
                å†…å®¹: %s
                ---
                """,
                seg.metadata("source"),
                seg.text()
            ))
            .collect(Collectors.joining("\n"));
    }
    
    /**
     * æ„å»ºæç¤ºè¯
     */
    private String buildPrompt(String question, String context) {
        PromptTemplate template = PromptTemplate.from("""
            åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            æ–‡æ¡£å†…å®¹:
            {{context}}
            
            ç”¨æˆ·é—®é¢˜: {{question}}
            
            å›ç­”è¦æ±‚:
            1. åªåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”
            2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
            3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
            4. ä¿æŒå›ç­”ç®€æ´å‡†ç¡®
            """);
        
        Prompt prompt = template.apply(Map.of(
            "context", context,
            "question", question
        ));
        
        return prompt.text();
    }
    
    /**
     * ç­”æ¡ˆå“åº”
     */
    public record AnswerResponse(
        String answer,
        List<Source> sources
    ) {}
    
    /**
     * æ¥æºä¿¡æ¯
     */
    public record Source(
        String documentId,
        String filename,
        String snippet
    ) {}
}
```

### 4.3 æµå¼å“åº”

```java
package com.example.ai.qa.controller;

import com.example.ai.qa.service.QuestionAnsweringService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.MediaType;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

/**
 * é—®ç­”APIæ§åˆ¶å™¨
 * 
 * @author erik.zhou
 */
@Slf4j
@RestController
@RequestMapping("/api/qa")
@RequiredArgsConstructor
public class QuestionAnsweringController {
    
    private final QuestionAnsweringService qaService;
    private final StreamingQAService streamingQAService;
    
    /**
     * æ ‡å‡†é—®ç­”
     */
    @PostMapping("/ask")
    public QuestionAnsweringService.AnswerResponse ask(
        @RequestParam String conversationId,
        @RequestBody String question
    ) {
        return qaService.answer(conversationId, question);
    }
    
    /**
     * æµå¼é—®ç­” (Server-Sent Events)
     */
    @PostMapping(value = "/ask/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<String> askStream(
        @RequestParam String conversationId,
        @RequestBody String question
    ) {
        return streamingQAService.answerStream(conversationId, question);
    }
}
```

---

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 ç¼“å­˜ç­–ç•¥

```java
package com.example.ai.qa.cache;

import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.time.Duration;

/**
 * å¤šçº§ç¼“å­˜ç­–ç•¥
 * 
 * @author erik.zhou
 */
@Slf4j
@Component
public class MultiLevelCache {
    
    // L1ç¼“å­˜: æœ¬åœ°å†…å­˜ (Caffeine)
    private final Cache<String, String> l1Cache;
    
    // L2ç¼“å­˜: Redis (é€šè¿‡RedisTemplate)
    private final RedisTemplate<String, String> redisTemplate;
    
    public MultiLevelCache(RedisTemplate<String, String> redisTemplate) {
        this.redisTemplate = redisTemplate;
        this.l1Cache = Caffeine.newBuilder()
            .maximumSize(1000)
            .expireAfterWrite(Duration.ofMinutes(10))
            .recordStats()
            .build();
    }
    
    /**
     * è·å–ç¼“å­˜
     */
    public String get(String key) {
        // 1. å°è¯•ä»L1ç¼“å­˜è·å–
        String value = l1Cache.getIfPresent(key);
        if (value != null) {
            log.debug("L1ç¼“å­˜å‘½ä¸­: key={}", key);
            return value;
        }
        
        // 2. å°è¯•ä»L2ç¼“å­˜è·å–
        value = redisTemplate.opsForValue().get(key);
        if (value != null) {
            log.debug("L2ç¼“å­˜å‘½ä¸­: key={}", key);
            // å›å¡«L1ç¼“å­˜
            l1Cache.put(key, value);
            return value;
        }
        
        log.debug("ç¼“å­˜æœªå‘½ä¸­: key={}", key);
        return null;
    }
    
    /**
     * è®¾ç½®ç¼“å­˜
     */
    public void put(String key, String value) {
        // åŒæ—¶å†™å…¥L1å’ŒL2ç¼“å­˜
        l1Cache.put(key, value);
        redisTemplate.opsForValue().set(key, value, Duration.ofHours(1));
    }
}
```

### 5.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–

```java
package com.example.ai.qa.optimization;

import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * æ‰¹é‡å¤„ç†ä¼˜åŒ–
 * 
 * @author erik.zhou
 */
@Slf4j
@Service
public class BatchProcessingService {
    
    private final ExecutorService executor = Executors.newFixedThreadPool(10);
    private static final int BATCH_SIZE = 50;
    
    /**
     * æ‰¹é‡ç”ŸæˆåµŒå…¥
     */
    public List<Embedding> batchEmbed(List<String> texts) {
        log.info("æ‰¹é‡ç”ŸæˆåµŒå…¥: total={}", texts.size());
        
        List<CompletableFuture<List<Embedding>>> futures = new ArrayList<>();
        
        // åˆ†æ‰¹å¤„ç†
        for (int i = 0; i < texts.size(); i += BATCH_SIZE) {
            int end = Math.min(i + BATCH_SIZE, texts.size());
            List<String> batch = texts.subList(i, end);
            
            CompletableFuture<List<Embedding>> future = CompletableFuture
                .supplyAsync(() -> embedBatch(batch), executor);
            
            futures.add(future);
        }
        
        // ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
        return futures.stream()
            .map(CompletableFuture::join)
            .flatMap(List::stream)
            .toList();
    }
    
    private List<Embedding> embedBatch(List<String> texts) {
        // å®é™…çš„åµŒå…¥ç”Ÿæˆé€»è¾‘
        return embeddingModel.embedAll(texts).content();
    }
}
```

---

## 6. éƒ¨ç½²æ–¹æ¡ˆ

### 6.1 Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  # AIé—®ç­”æœåŠ¡
  ai-qa-service:
    build: .
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
    depends_on:
      - postgres
      - redis
      - kafka
  
  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ai_qa
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    volumes:
      - postgres-data:/var/lib/postgresql/data
  
  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      - zookeeper
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

volumes:
  postgres-data:
```

### 6.2 Kuberneteséƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-qa-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-qa-service
  template:
    metadata:
      labels:
        app: ai-qa-service
    spec:
      containers:
      - name: ai-qa-service
        image: ai-qa-service:latest
        ports:
        - containerPort: 8080
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
```

---

## 7. æ€»ç»“

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªç”Ÿäº§çº§çš„AIæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ ¸å¿ƒäº®ç‚¹:

### æŠ€æœ¯äº®ç‚¹
- âœ… æ··åˆæ£€ç´¢ç­–ç•¥ (å‘é‡+å…³é”®è¯)
- âœ… æ™ºèƒ½æ–‡æ¡£åˆ†å—
- âœ… å¤šè½®å¯¹è¯ç®¡ç†
- âœ… æµå¼å“åº”
- âœ… å¤šçº§ç¼“å­˜

### æ€§èƒ½æŒ‡æ ‡
- æ–‡æ¡£å¤„ç†: 1MB/ç§’
- æŸ¥è¯¢å“åº”: <2ç§’
- å¹¶å‘æ”¯æŒ: 1000 QPS
- å‡†ç¡®ç‡: >85%

### æ‰©å±•æ–¹å‘
1. æ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼
2. å¤šè¯­è¨€æ”¯æŒ
3. çŸ¥è¯†å›¾è°±é›†æˆ
4. ä¸ªæ€§åŒ–æ¨è
5. å®æ—¶å­¦ä¹ æ›´æ–°

---

**ä½œè€…**: erik.zhou  
**æœ€åæ›´æ–°**: 2025-01-31  
**ç‰ˆæœ¬**: 1.0.0
