# Kafka å®Œæ•´æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [æŠ€æœ¯æ¦‚è¿°](#æŠ€æœ¯æ¦‚è¿°)
- [å­¦ä¹ ç›®æ ‡](#å­¦ä¹ ç›®æ ‡)
- [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å®æˆ˜åº”ç”¨](#å®æˆ˜åº”ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ç›¸å…³èµ„æº](#ç›¸å…³èµ„æº)

## ğŸ“š æŠ€æœ¯æ¦‚è¿°
- **ç‰ˆæœ¬**: 3.6.x
- **å®˜æ–¹æ–‡æ¡£**: [https://kafka.apache.org](https://kafka.apache.org)
- **å­¦ä¹ éš¾åº¦**: â­â­â­â­ (1-5æ˜Ÿ)
- **é‡è¦ç¨‹åº¦**: â­â­â­â­â­ (1-5æ˜Ÿ)
- **å‰ç½®çŸ¥è¯†**: JavaåŸºç¡€ã€åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€ã€ç½‘ç»œç¼–ç¨‹
- **æ–‡æ¡£æ¥æº**: Apache Kafkaå®˜æ–¹æ–‡æ¡£
- **æ›´æ–°æ—¶é—´**: 2024-12-31

### ä»€ä¹ˆæ˜¯Kafka
Apache Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æµå¤„ç†å¹³å°å’Œæ¶ˆæ¯ç³»ç»Ÿï¼Œæœ€åˆç”±LinkedInå¼€å‘å¹¶å¼€æºã€‚Kafkaè¢«è®¾è®¡ä¸ºé«˜ååé‡ã€å¯æŒä¹…åŒ–ã€å¯æ°´å¹³æ‰©å±•çš„åˆ†å¸ƒå¼å‘å¸ƒ-è®¢é˜…æ¶ˆæ¯ç³»ç»Ÿã€‚å®ƒèƒ½å¤Ÿå¤„ç†æ•°ä¸‡äº¿çº§åˆ«çš„äº‹ä»¶ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå®æ—¶æ•°æ®ç®¡é“ã€æµå¼å¤„ç†ã€æ—¥å¿—èšåˆç­‰åœºæ™¯ã€‚

### æ ¸å¿ƒä»·å€¼
- **é«˜ååé‡**: å•æœºå¯è¾¾ç™¾ä¸‡çº§TPS
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ°´å¹³æ‰©å±•ï¼Œè½»æ¾åº”å¯¹æ•°æ®å¢é•¿
- **æŒä¹…åŒ–**: æ¶ˆæ¯æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œæ”¯æŒæ•°æ®å›æº¯
- **å®¹é”™æ€§**: åˆ†å¸ƒå¼æ¶æ„ï¼Œæ”¯æŒå‰¯æœ¬æœºåˆ¶
- **å®æ—¶æ€§**: æ¯«ç§’çº§å»¶è¿Ÿï¼Œæ”¯æŒå®æ—¶æµå¤„ç†

## ğŸ¯ å­¦ä¹ ç›®æ ‡
- [ ] ç†è§£Kafkaçš„æ ¸å¿ƒæ¦‚å¿µï¼ˆTopicã€Partitionã€Replicaï¼‰
- [ ] æŒæ¡ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…çš„ä½¿ç”¨
- [ ] ç†è§£åˆ†åŒºæœºåˆ¶å’Œè´Ÿè½½å‡è¡¡
- [ ] æŒæ¡å‰¯æœ¬æœºåˆ¶å’Œé«˜å¯ç”¨é…ç½®
- [ ] ç†è§£æ¶ˆæ¯é¡ºåºæ€§ä¿è¯æœºåˆ¶
- [ ] æŒæ¡å¹‚ç­‰æ€§å’Œäº‹åŠ¡ç‰¹æ€§
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜å’Œç›‘æ§


## ğŸ“– åŸºç¡€æ¦‚å¿µ

### 1.1 æ ¸å¿ƒç»„ä»¶

#### Brokerï¼ˆä»£ç†æœåŠ¡å™¨ï¼‰
- Kafkaé›†ç¾¤ç”±å¤šä¸ªBrokerç»„æˆ
- æ¯ä¸ªBrokeræ˜¯ä¸€ä¸ªç‹¬ç«‹çš„KafkaæœåŠ¡å™¨
- Brokerè´Ÿè´£æ¥æ”¶ã€å­˜å‚¨å’Œè½¬å‘æ¶ˆæ¯
- é€šè¿‡Broker IDå”¯ä¸€æ ‡è¯†

#### Topicï¼ˆä¸»é¢˜ï¼‰
- æ¶ˆæ¯çš„é€»è¾‘åˆ†ç±»
- ç”Ÿäº§è€…å‘é€æ¶ˆæ¯åˆ°Topic
- æ¶ˆè´¹è€…ä»Topicè®¢é˜…æ¶ˆæ¯
- ä¸€ä¸ªTopicå¯ä»¥æœ‰å¤šä¸ªåˆ†åŒº

#### Partitionï¼ˆåˆ†åŒºï¼‰
- Topicçš„ç‰©ç†åˆ†ç»„
- æ¯ä¸ªåˆ†åŒºæ˜¯ä¸€ä¸ªæœ‰åºçš„ã€ä¸å¯å˜çš„æ¶ˆæ¯åºåˆ—
- åˆ†åŒºå†…æ¶ˆæ¯æœ‰åºï¼Œåˆ†åŒºé—´æ— åº
- åˆ†åŒºæ˜¯Kafkaå¹¶è¡Œå¤„ç†çš„åŸºæœ¬å•ä½

#### Replicaï¼ˆå‰¯æœ¬ï¼‰
- æ¯ä¸ªåˆ†åŒºå¯ä»¥æœ‰å¤šä¸ªå‰¯æœ¬
- å‰¯æœ¬åˆ†ä¸ºLeaderå’ŒFollower
- Leaderå¤„ç†æ‰€æœ‰è¯»å†™è¯·æ±‚
- FolloweråŒæ­¥Leaderçš„æ•°æ®ï¼Œæä¾›å®¹é”™

#### Producerï¼ˆç”Ÿäº§è€…ï¼‰
- å‘Kafkaå‘é€æ¶ˆæ¯çš„å®¢æˆ·ç«¯
- å†³å®šæ¶ˆæ¯å‘é€åˆ°å“ªä¸ªåˆ†åŒº
- æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥å‘é€

#### Consumerï¼ˆæ¶ˆè´¹è€…ï¼‰
- ä»Kafkaè¯»å–æ¶ˆæ¯çš„å®¢æˆ·ç«¯
- æ¶ˆè´¹è€…é€šè¿‡Consumer Groupå®ç°è´Ÿè½½å‡è¡¡
- æ”¯æŒè‡ªåŠ¨å’Œæ‰‹åŠ¨æäº¤offset

#### Consumer Groupï¼ˆæ¶ˆè´¹è€…ç»„ï¼‰
- å¤šä¸ªæ¶ˆè´¹è€…ç»„æˆä¸€ä¸ªæ¶ˆè´¹è€…ç»„
- åŒä¸€æ¶ˆè´¹è€…ç»„å†…çš„æ¶ˆè´¹è€…å…±åŒæ¶ˆè´¹Topic
- æ¯ä¸ªåˆ†åŒºåªèƒ½è¢«ç»„å†…ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹
- ä¸åŒæ¶ˆè´¹è€…ç»„å¯ä»¥ç‹¬ç«‹æ¶ˆè´¹åŒä¸€Topic

#### Offsetï¼ˆåç§»é‡ï¼‰
- æ¶ˆæ¯åœ¨åˆ†åŒºä¸­çš„å”¯ä¸€æ ‡è¯†
- æ¶ˆè´¹è€…é€šè¿‡offsetè¿½è¸ªæ¶ˆè´¹è¿›åº¦
- offsetå­˜å‚¨åœ¨Kafkaå†…éƒ¨Topicä¸­

### 1.2 æ¶æ„å›¾

```
Producer1 â”€â”€â”
Producer2 â”€â”€â”¼â”€â”€> Broker1 (Leader P0, Follower P1)
Producer3 â”€â”€â”˜    Broker2 (Follower P0, Leader P1)
                 Broker3 (Follower P0, Follower P1)
                      â”‚
                      â”œâ”€â”€> Consumer Group 1
                      â”‚    â”œâ”€ Consumer1 (P0)
                      â”‚    â””â”€ Consumer2 (P1)
                      â”‚
                      â””â”€â”€> Consumer Group 2
                           â”œâ”€ Consumer3 (P0)
                           â””â”€ Consumer4 (P1)
```

### 1.3 åº”ç”¨åœºæ™¯
- **æ¶ˆæ¯ç³»ç»Ÿ**: è§£è€¦ç³»ç»Ÿç»„ä»¶ï¼Œå¼‚æ­¥å¤„ç†
- **æ—¥å¿—èšåˆ**: æ”¶é›†åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ—¥å¿—
- **æµå¼å¤„ç†**: å®æ—¶æ•°æ®å¤„ç†å’Œåˆ†æ
- **äº‹ä»¶æº¯æº**: è®°å½•ç³»ç»ŸçŠ¶æ€å˜æ›´
- **æ•°æ®ç®¡é“**: åœ¨ä¸åŒç³»ç»Ÿé—´ä¼ è¾“æ•°æ®
- **æŒ‡æ ‡ç›‘æ§**: æ”¶é›†å’Œå¤„ç†ç›‘æ§æŒ‡æ ‡


## ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ (é‡ç‚¹)

### 2.1 ç”Ÿäº§è€…ï¼ˆProducerï¼‰ğŸ”¥

#### åŸºæœ¬ä½¿ç”¨

**ä»£ç ç¤ºä¾‹**:
```java
/**
 * Kafkaç”Ÿäº§è€…ç¤ºä¾‹
 * @author erik.zhou
 */
public class KafkaProducerExample {
    
    public void sendMessage() {
        // 1. é…ç½®ç”Ÿäº§è€…å±æ€§
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        // 2. åˆ›å»ºç”Ÿäº§è€…
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        try {
            // 3. åˆ›å»ºæ¶ˆæ¯è®°å½•
            ProducerRecord<String, String> record = 
                new ProducerRecord<>("my-topic", "key", "value");
            
            // 4. å‘é€æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰
            producer.send(record, (metadata, exception) -> {
                if (exception == null) {
                    System.out.println("æ¶ˆæ¯å‘é€æˆåŠŸ: " + 
                        "topic=" + metadata.topic() + 
                        ", partition=" + metadata.partition() + 
                        ", offset=" + metadata.offset());
                } else {
                    System.err.println("æ¶ˆæ¯å‘é€å¤±è´¥: " + exception.getMessage());
                }
            });
            
        } finally {
            // 5. å…³é—­ç”Ÿäº§è€…
            producer.close();
        }
    }
    
    // åŒæ­¥å‘é€
    public void sendMessageSync() throws Exception {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        try {
            ProducerRecord<String, String> record = 
                new ProducerRecord<>("my-topic", "key", "value");
            
            // åŒæ­¥å‘é€ï¼Œç­‰å¾…ç»“æœ
            RecordMetadata metadata = producer.send(record).get();
            System.out.println("æ¶ˆæ¯å‘é€æˆåŠŸ: offset=" + metadata.offset());
            
        } finally {
            producer.close();
        }
    }
}
```

#### åˆ†åŒºç­–ç•¥

**é»˜è®¤åˆ†åŒºå™¨**:
- å¦‚æœæŒ‡å®šäº†partitionï¼Œç›´æ¥ä½¿ç”¨
- å¦‚æœæŒ‡å®šäº†keyï¼Œæ ¹æ®keyçš„hashå€¼é€‰æ‹©åˆ†åŒº
- å¦‚æœéƒ½æ²¡æŒ‡å®šï¼Œè½®è¯¢é€‰æ‹©åˆ†åŒº

**è‡ªå®šä¹‰åˆ†åŒºå™¨**:
```java
/**
 * è‡ªå®šä¹‰åˆ†åŒºå™¨
 * @author erik.zhou
 */
public class CustomPartitioner implements Partitioner {
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, 
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        
        if (key == null) {
            // æ²¡æœ‰keyï¼Œéšæœºé€‰æ‹©åˆ†åŒº
            return ThreadLocalRandom.current().nextInt(numPartitions);
        }
        
        // æ ¹æ®ä¸šåŠ¡é€»è¾‘è‡ªå®šä¹‰åˆ†åŒºç­–ç•¥
        // ä¾‹å¦‚ï¼šæŒ‰ç”¨æˆ·IDåˆ†åŒº
        if (key instanceof String) {
            String userId = (String) key;
            return Math.abs(userId.hashCode()) % numPartitions;
        }
        
        return Math.abs(key.hashCode()) % numPartitions;
    }
    
    @Override
    public void close() {}
    
    @Override
    public void configure(Map<String, ?> configs) {}
}

// ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨
props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, CustomPartitioner.class.getName());
```

### 2.2 æ¶ˆè´¹è€…ï¼ˆConsumerï¼‰ğŸ”¥

#### åŸºæœ¬ä½¿ç”¨

**ä»£ç ç¤ºä¾‹**:
```java
/**
 * Kafkaæ¶ˆè´¹è€…ç¤ºä¾‹
 * @author erik.zhou
 */
public class KafkaConsumerExample {
    
    public void consumeMessage() {
        // 1. é…ç½®æ¶ˆè´¹è€…å±æ€§
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        // 2. åˆ›å»ºæ¶ˆè´¹è€…
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        try {
            // 3. è®¢é˜…ä¸»é¢˜
            consumer.subscribe(Collections.singletonList("my-topic"));
            
            // 4. æ‹‰å–æ¶ˆæ¯
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("æ¥æ”¶æ¶ˆæ¯: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), 
                        record.key(), record.value());
                }
            }
            
        } finally {
            // 5. å…³é—­æ¶ˆè´¹è€…
            consumer.close();
        }
    }
}
```

#### æ‰‹åŠ¨æäº¤Offset

**ä»£ç ç¤ºä¾‹**:
```java
/**
 * æ‰‹åŠ¨æäº¤Offsetç¤ºä¾‹
 * @author erik.zhou
 */
public class ManualCommitExample {
    
    public void consumeWithManualCommit() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        // å…³é—­è‡ªåŠ¨æäº¤
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("my-topic"));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    try {
                        // å¤„ç†æ¶ˆæ¯
                        processRecord(record);
                        
                        // åŒæ­¥æäº¤å½“å‰offset
                        consumer.commitSync(Collections.singletonMap(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1)
                        ));
                        
                    } catch (Exception e) {
                        System.err.println("å¤„ç†æ¶ˆæ¯å¤±è´¥: " + e.getMessage());
                        // å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–é‡è¯•
                    }
                }
            }
            
        } finally {
            consumer.close();
        }
    }
    
    // å¼‚æ­¥æäº¤
    public void consumeWithAsyncCommit() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("my-topic"));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    processRecord(record);
                }
                
                // å¼‚æ­¥æäº¤offset
                consumer.commitAsync((offsets, exception) -> {
                    if (exception != null) {
                        System.err.println("æäº¤offsetå¤±è´¥: " + exception.getMessage());
                    } else {
                        System.out.println("æäº¤offsetæˆåŠŸ: " + offsets);
                    }
                });
            }
            
        } finally {
            // å…³é—­å‰åŒæ­¥æäº¤ï¼Œç¡®ä¿offsetæäº¤æˆåŠŸ
            try {
                consumer.commitSync();
            } finally {
                consumer.close();
            }
        }
    }
    
    private void processRecord(ConsumerRecord<String, String> record) {
        // å¤„ç†æ¶ˆæ¯é€»è¾‘
        System.out.println("å¤„ç†æ¶ˆæ¯: " + record.value());
    }
}
```


### 2.3 åˆ†åŒºä¸å‰¯æœ¬æœºåˆ¶ ğŸ”¥

#### åˆ†åŒºï¼ˆPartitionï¼‰

**åˆ†åŒºçš„ä½œç”¨**:
1. **è´Ÿè½½å‡è¡¡**: æ•°æ®åˆ†æ•£åˆ°å¤šä¸ªåˆ†åŒºï¼Œæé«˜å¹¶è¡Œåº¦
2. **æ°´å¹³æ‰©å±•**: å¢åŠ åˆ†åŒºæ•°é‡å¯ä»¥æé«˜ååé‡
3. **é¡ºåºä¿è¯**: åŒä¸€åˆ†åŒºå†…æ¶ˆæ¯æœ‰åº

**åˆ†åŒºåˆ†é…ç­–ç•¥**:
- **Range**: æŒ‰åˆ†åŒºèŒƒå›´åˆ†é…ï¼ˆé»˜è®¤ï¼‰
- **RoundRobin**: è½®è¯¢åˆ†é…
- **Sticky**: ç²˜æ€§åˆ†é…ï¼Œå°½é‡ä¿æŒåŸæœ‰åˆ†é…

**ä»£ç ç¤ºä¾‹**:
```java
/**
 * åˆ†åŒºé…ç½®ç¤ºä¾‹
 * @author erik.zhou
 */
public class PartitionExample {
    
    // åˆ›å»ºå¤šåˆ†åŒºTopic
    public void createTopic() {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        
        try (AdminClient adminClient = AdminClient.create(props)) {
            NewTopic newTopic = new NewTopic("my-topic", 3, (short) 2);
            // 3ä¸ªåˆ†åŒºï¼Œ2ä¸ªå‰¯æœ¬
            
            adminClient.createTopics(Collections.singleton(newTopic));
            System.out.println("Topicåˆ›å»ºæˆåŠŸ");
        }
    }
    
    // æŒ‡å®šåˆ†åŒºå‘é€æ¶ˆæ¯
    public void sendToPartition() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        // æŒ‡å®šå‘é€åˆ°åˆ†åŒº0
        ProducerRecord<String, String> record = 
            new ProducerRecord<>("my-topic", 0, "key", "value");
        
        producer.send(record);
        producer.close();
    }
}
```

#### å‰¯æœ¬ï¼ˆReplicaï¼‰

**å‰¯æœ¬æœºåˆ¶**:
- **Leaderå‰¯æœ¬**: å¤„ç†æ‰€æœ‰è¯»å†™è¯·æ±‚
- **Followerå‰¯æœ¬**: åŒæ­¥Leaderæ•°æ®ï¼Œä¸å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
- **ISRï¼ˆIn-Sync Replicasï¼‰**: ä¸Leaderä¿æŒåŒæ­¥çš„å‰¯æœ¬é›†åˆ

**å‰¯æœ¬åŒæ­¥æµç¨‹**:
```
1. Producerå‘é€æ¶ˆæ¯åˆ°Leader
2. Leaderå†™å…¥æœ¬åœ°æ—¥å¿—
3. Followerä»Leaderæ‹‰å–æ¶ˆæ¯
4. Followerå†™å…¥æœ¬åœ°æ—¥å¿—
5. Followerå‘é€ACKç»™Leader
6. Leaderæ”¶åˆ°æ‰€æœ‰ISRçš„ACKåï¼Œæ›´æ–°HWï¼ˆHigh Watermarkï¼‰
7. Leaderè¿”å›ACKç»™Producer
```

### 2.4 æ¶ˆæ¯é¡ºåºæ€§ ğŸ”¥ (âš ï¸ éš¾ç‚¹)

#### é¡ºåºä¿è¯è§„åˆ™
1. **åˆ†åŒºå†…æœ‰åº**: åŒä¸€åˆ†åŒºå†…çš„æ¶ˆæ¯ä¸¥æ ¼æœ‰åº
2. **åˆ†åŒºé—´æ— åº**: ä¸åŒåˆ†åŒºä¹‹é—´çš„æ¶ˆæ¯æ— åº
3. **Keyç›¸åŒ**: ç›¸åŒkeyçš„æ¶ˆæ¯ä¼šå‘é€åˆ°åŒä¸€åˆ†åŒº

#### ä¿è¯é¡ºåºæ€§çš„æ–¹æ³•

**æ–¹æ³•1: å•åˆ†åŒº**
```java
/**
 * å•åˆ†åŒºä¿è¯å…¨å±€é¡ºåº
 * @author erik.zhou
 */
public class SinglePartitionOrder {
    
    public void sendOrdered() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        // è®¾ç½®ä¸º1ï¼Œä¿è¯æ¶ˆæ¯é¡ºåº
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        // æ‰€æœ‰æ¶ˆæ¯å‘é€åˆ°åŒä¸€åˆ†åŒº
        for (int i = 0; i < 10; i++) {
            ProducerRecord<String, String> record = 
                new ProducerRecord<>("my-topic", 0, null, "message-" + i);
            producer.send(record);
        }
        
        producer.close();
    }
}
```

**æ–¹æ³•2: ç›¸åŒKey**
```java
/**
 * ä½¿ç”¨ç›¸åŒKeyä¿è¯å±€éƒ¨é¡ºåº
 * @author erik.zhou
 */
public class KeyBasedOrder {
    
    public void sendOrderedByKey(String userId, List<String> messages) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        // ç›¸åŒuserIdçš„æ¶ˆæ¯ä¼šå‘é€åˆ°åŒä¸€åˆ†åŒºï¼Œä¿è¯é¡ºåº
        for (String message : messages) {
            ProducerRecord<String, String> record = 
                new ProducerRecord<>("my-topic", userId, message);
            producer.send(record);
        }
        
        producer.close();
    }
}
```

### 2.5 å¹‚ç­‰æ€§ä¸äº‹åŠ¡ ğŸ”¥ (âš ï¸ éš¾ç‚¹)

#### å¹‚ç­‰æ€§ï¼ˆIdempotenceï¼‰

**ä½œç”¨**: é˜²æ­¢æ¶ˆæ¯é‡å¤å‘é€

**åŸç†**:
- Producerä¸ºæ¯æ¡æ¶ˆæ¯åˆ†é…å”¯ä¸€çš„åºåˆ—å·
- Brokeræ£€æµ‹åˆ°é‡å¤åºåˆ—å·æ—¶ï¼Œä¸¢å¼ƒé‡å¤æ¶ˆæ¯
- åªä¿è¯å•ä¸ªProducerå•ä¸ªåˆ†åŒºçš„å¹‚ç­‰æ€§

**é…ç½®**:
```java
/**
 * å¹‚ç­‰æ€§ç”Ÿäº§è€…
 * @author erik.zhou
 */
public class IdempotentProducer {
    
    public void sendIdempotent() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        // å¼€å¯å¹‚ç­‰æ€§
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        // å¹‚ç­‰æ€§è¦æ±‚ä»¥ä¸‹é…ç½®
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        ProducerRecord<String, String> record = 
            new ProducerRecord<>("my-topic", "key", "value");
        producer.send(record);
        
        producer.close();
    }
}
```

#### äº‹åŠ¡ï¼ˆTransactionï¼‰

**ä½œç”¨**: ä¿è¯å¤šæ¡æ¶ˆæ¯çš„åŸå­æ€§

**ç‰¹æ€§**:
- **åŸå­æ€§**: å¤šæ¡æ¶ˆæ¯è¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å¤±è´¥
- **è·¨åˆ†åŒº**: æ”¯æŒè·¨å¤šä¸ªåˆ†åŒºçš„äº‹åŠ¡
- **Exactly-Once**: ç»“åˆå¹‚ç­‰æ€§å®ç°ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰

**ä»£ç ç¤ºä¾‹**:
```java
/**
 * äº‹åŠ¡ç”Ÿäº§è€…
 * @author erik.zhou
 */
public class TransactionalProducer {
    
    public void sendTransactional() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        // å¼€å¯äº‹åŠ¡
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "my-transactional-id");
        
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        
        // åˆå§‹åŒ–äº‹åŠ¡
        producer.initTransactions();
        
        try {
            // å¼€å§‹äº‹åŠ¡
            producer.beginTransaction();
            
            // å‘é€å¤šæ¡æ¶ˆæ¯
            producer.send(new ProducerRecord<>("topic1", "key1", "value1"));
            producer.send(new ProducerRecord<>("topic2", "key2", "value2"));
            producer.send(new ProducerRecord<>("topic3", "key3", "value3"));
            
            // æäº¤äº‹åŠ¡
            producer.commitTransaction();
            System.out.println("äº‹åŠ¡æäº¤æˆåŠŸ");
            
        } catch (Exception e) {
            // å›æ»šäº‹åŠ¡
            producer.abortTransaction();
            System.err.println("äº‹åŠ¡å›æ»š: " + e.getMessage());
        } finally {
            producer.close();
        }
    }
    
    // äº‹åŠ¡æ¶ˆè´¹-è½¬æ¢-ç”Ÿäº§æ¨¡å¼
    public void consumeTransformProduce() {
        // æ¶ˆè´¹è€…é…ç½®
        Properties consumerProps = new Properties();
        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                         StringDeserializer.class.getName());
        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                         StringDeserializer.class.getName());
        consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        consumerProps.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        
        // ç”Ÿäº§è€…é…ç½®
        Properties producerProps = new Properties();
        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                         StringSerializer.class.getName());
        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                         StringSerializer.class.getName());
        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        producerProps.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "my-tx-id");
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps);
        KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);
        
        consumer.subscribe(Collections.singletonList("input-topic"));
        producer.initTransactions();
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(100));
                
                if (!records.isEmpty()) {
                    producer.beginTransaction();
                    
                    try {
                        for (ConsumerRecord<String, String> record : records) {
                            // è½¬æ¢æ¶ˆæ¯
                            String transformedValue = transform(record.value());
                            
                            // å‘é€åˆ°è¾“å‡ºTopic
                            producer.send(new ProducerRecord<>(
                                "output-topic", record.key(), transformedValue));
                        }
                        
                        // æäº¤æ¶ˆè´¹è€…offsetåˆ°äº‹åŠ¡
                        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                        for (ConsumerRecord<String, String> record : records) {
                            offsets.put(
                                new TopicPartition(record.topic(), record.partition()),
                                new OffsetAndMetadata(record.offset() + 1)
                            );
                        }
                        producer.sendOffsetsToTransaction(offsets, 
                            consumer.groupMetadata());
                        
                        // æäº¤äº‹åŠ¡
                        producer.commitTransaction();
                        
                    } catch (Exception e) {
                        producer.abortTransaction();
                        System.err.println("äº‹åŠ¡å¤±è´¥: " + e.getMessage());
                    }
                }
            }
        } finally {
            consumer.close();
            producer.close();
        }
    }
    
    private String transform(String value) {
        // è½¬æ¢é€»è¾‘
        return value.toUpperCase();
    }
}
```


## ğŸ’» å®æˆ˜åº”ç”¨

### 3.1 ç¯å¢ƒæ­å»º

#### Docker Composeéƒ¨ç½²
```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

#### Mavenä¾èµ–
```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version>
</dependency>
```

### 3.2 Spring Booté›†æˆ

#### é…ç½®æ–‡ä»¶
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
    consumer:
      group-id: my-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      auto-offset-reset: earliest
      enable-auto-commit: false
      properties:
        spring.json.trusted.packages: "*"
```

#### ç”Ÿäº§è€…
```java
/**
 * Spring Kafkaç”Ÿäº§è€…
 * @author erik.zhou
 */
@Service
@Slf4j
public class OrderProducer {
    
    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;
    
    public void sendOrder(Order order) {
        ListenableFuture<SendResult<String, Order>> future = 
            kafkaTemplate.send("order-topic", order.getOrderId(), order);
        
        future.addCallback(
            result -> log.info("å‘é€æˆåŠŸ: {}", result.getRecordMetadata()),
            ex -> log.error("å‘é€å¤±è´¥", ex)
        );
    }
}
```

#### æ¶ˆè´¹è€…
```java
/**
 * Spring Kafkaæ¶ˆè´¹è€…
 * @author erik.zhou
 */
@Component
@Slf4j
public class OrderConsumer {
    
    @KafkaListener(topics = "order-topic", groupId = "order-group")
    public void consume(ConsumerRecord<String, Order> record,
                       Acknowledgment ack) {
        try {
            log.info("æ¥æ”¶è®¢å•: {}", record.value());
            processOrder(record.value());
            ack.acknowledge();  // æ‰‹åŠ¨æäº¤
        } catch (Exception e) {
            log.error("å¤„ç†å¤±è´¥", e);
            // ä¸æäº¤offsetï¼Œæ¶ˆæ¯ä¼šé‡æ–°æ¶ˆè´¹
        }
    }
    
    private void processOrder(Order order) {
        // ä¸šåŠ¡å¤„ç†
    }
}
```

## âœ¨ æœ€ä½³å®è·µ

### 4.1 æ€§èƒ½ä¼˜åŒ–

#### ç”Ÿäº§è€…ä¼˜åŒ–
```java
/**
 * ç”Ÿäº§è€…æ€§èƒ½ä¼˜åŒ–
 * @author erik.zhou
 */
public class ProducerOptimization {
    
    public KafkaProducer<String, String> createOptimizedProducer() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  StringSerializer.class.getName());
        
        // æ‰¹é‡å‘é€ä¼˜åŒ–
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);  // 16KB
        props.put(ProducerConfig.LINGER_MS_CONFIG, 10);      // ç­‰å¾…10ms
        
        // å‹ç¼©
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");
        
        // ç¼“å†²åŒºå¤§å°
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);  // 32MB
        
        // å¹¶å‘è¯·æ±‚æ•°
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        
        return new KafkaProducer<>(props);
    }
}
```

#### æ¶ˆè´¹è€…ä¼˜åŒ–
```java
/**
 * æ¶ˆè´¹è€…æ€§èƒ½ä¼˜åŒ–
 * @author erik.zhou
 */
public class ConsumerOptimization {
    
    public KafkaConsumer<String, String> createOptimizedConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        
        // æ‰¹é‡æ‹‰å–
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);     // 1KB
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);    // 500ms
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);     // æ¯æ¬¡æœ€å¤š500æ¡
        
        // ä¼šè¯è¶…æ—¶
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);  // 30s
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000); // 3s
        
        return new KafkaConsumer<>(props);
    }
}
```

### 4.2 å¸¸è§é™·é˜±

#### âš ï¸ é™·é˜±1: æ¶ˆæ¯ä¸¢å¤±
**åŸå› **: acksé…ç½®ä¸å½“ã€æœªå¼€å¯å¹‚ç­‰æ€§
**è§£å†³æ–¹æ¡ˆ**:
```java
props.put(ProducerConfig.ACKS_CONFIG, "all");
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
```

#### âš ï¸ é™·é˜±2: æ¶ˆæ¯é‡å¤
**åŸå› **: æ¶ˆè´¹è€…é‡å¤æ¶ˆè´¹ã€æœªå®ç°å¹‚ç­‰æ€§
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨Redisæˆ–æ•°æ®åº“å®ç°æ¶ˆè´¹å¹‚ç­‰æ€§

#### âš ï¸ é™·é˜±3: æ¶ˆè´¹è€…Rebalance
**åŸå› **: æ¶ˆè´¹è€…å¤„ç†æ—¶é—´è¿‡é•¿ã€å¿ƒè·³è¶…æ—¶
**è§£å†³æ–¹æ¡ˆ**:
```java
props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000);  // 5åˆ†é’Ÿ
props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);     // 30ç§’
```

## â“ å¸¸è§é—®é¢˜

### Q1: Kafkaå¦‚ä½•ä¿è¯é«˜ååé‡ï¼Ÿ
**A**: 
1. **é¡ºåºå†™ç£ç›˜**: åˆ©ç”¨ç£ç›˜é¡ºåºå†™çš„é«˜æ€§èƒ½
2. **é›¶æ‹·è´**: ä½¿ç”¨sendfileç³»ç»Ÿè°ƒç”¨
3. **æ‰¹é‡å¤„ç†**: æ‰¹é‡å‘é€å’Œæ‰¹é‡æ‹‰å–
4. **å‹ç¼©**: æ”¯æŒå¤šç§å‹ç¼©ç®—æ³•
5. **åˆ†åŒºå¹¶è¡Œ**: å¤šåˆ†åŒºå¹¶è¡Œå¤„ç†

### Q2: Kafkaå’ŒRabbitMQå¦‚ä½•é€‰æ‹©ï¼Ÿ
**A**: 
- **Kafka**: å¤§æ•°æ®é‡ã€æ—¥å¿—æ”¶é›†ã€æµå¤„ç†ã€éœ€è¦æ¶ˆæ¯å›æº¯
- **RabbitMQ**: å¤æ‚è·¯ç”±ã€æ¶ˆæ¯ä¼˜å…ˆçº§ã€å»¶è¿Ÿé˜Ÿåˆ—ã€ä¸­å°æ•°æ®é‡

### Q3: å¦‚ä½•ä¿è¯Exactly-Onceï¼Ÿ
**A**: 
1. ç”Ÿäº§è€…å¼€å¯å¹‚ç­‰æ€§å’Œäº‹åŠ¡
2. æ¶ˆè´¹è€…ä½¿ç”¨äº‹åŠ¡æ¶ˆè´¹æ¨¡å¼
3. ä¸šåŠ¡å±‚å®ç°å¹‚ç­‰æ€§

### Q4: åˆ†åŒºæ•°å¦‚ä½•è®¾ç½®ï¼Ÿ
**A**: 
- è€ƒè™‘ååé‡éœ€æ±‚
- è€ƒè™‘æ¶ˆè´¹è€…å¹¶è¡Œåº¦
- ä¸€èˆ¬è®¾ç½®ä¸ºæ¶ˆè´¹è€…æ•°é‡çš„æ•´æ•°å€
- å»ºè®®ä¸è¶…è¿‡1000ä¸ªåˆ†åŒº

## ğŸ”— ç›¸å…³èµ„æº

- [Kafkaå®˜æ–¹æ–‡æ¡£](https://kafka.apache.org/documentation/)
- [Kafka GitHub](https://github.com/apache/kafka)
- [Confluentæ–‡æ¡£](https://docs.confluent.io/)

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Kafkaæ ¸å¿ƒæ¦‚å¿µ
- [ ] æŒæ¡ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…API
- [ ] ç†è§£åˆ†åŒºå’Œå‰¯æœ¬æœºåˆ¶
- [ ] æŒæ¡æ¶ˆæ¯é¡ºåºæ€§ä¿è¯
- [ ] ç†è§£å¹‚ç­‰æ€§å’Œäº‹åŠ¡
- [ ] æŒæ¡æ€§èƒ½ä¼˜åŒ–æ–¹æ³•
- [ ] èƒ½å¤Ÿè§£å†³å¸¸è§é—®é¢˜

---

**@author erik.zhou**  
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2024-12-31
