# Hadoop å®Œæ•´æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [æŠ€æœ¯æ¦‚è¿°](#æŠ€æœ¯æ¦‚è¿°)
- [å­¦ä¹ ç›®æ ‡](#å­¦ä¹ ç›®æ ‡)
- [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å®æˆ˜åº”ç”¨](#å®æˆ˜åº”ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ç›¸å…³èµ„æº](#ç›¸å…³èµ„æº)

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

- **ç‰ˆæœ¬**: 3.3.6 (LTS)
- **å®˜æ–¹æ–‡æ¡£**: https://hadoop.apache.org/
- **å­¦ä¹ éš¾åº¦**: â­â­â­â­ (4/5æ˜Ÿ)
- **é‡è¦ç¨‹åº¦**: â­â­â­ (3/5æ˜Ÿ)
- **å‰ç½®çŸ¥è¯†**: 
  - JavaåŸºç¡€
  - LinuxåŸºæœ¬æ“ä½œ
  - åˆ†å¸ƒå¼ç³»ç»ŸåŸºæœ¬æ¦‚å¿µ
  - ç½‘ç»œç¼–ç¨‹åŸºç¡€

**æ–‡æ¡£æ¥æº**: Apache Hadoopå®˜æ–¹æ–‡æ¡£ + ç¤¾åŒºæœ€ä½³å®è·µ

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- [ ] ç†è§£Hadoopçš„æ ¸å¿ƒæ¶æ„å’Œè®¾è®¡ç†å¿µ
- [ ] æŒæ¡HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿçš„åŸç†å’Œä½¿ç”¨
- [ ] ç†è§£MapReduceç¼–ç¨‹æ¨¡å‹
- [ ] æŒæ¡YARNèµ„æºç®¡ç†æ¡†æ¶
- [ ] èƒ½å¤Ÿæ­å»ºHadoopé›†ç¾¤ç¯å¢ƒ
- [ ] äº†è§£Hadoopåœ¨Javaåç«¯çš„åº”ç”¨åœºæ™¯

## ğŸ“– åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯Hadoop

Hadoopæ˜¯Apacheè½¯ä»¶åŸºé‡‘ä¼šå¼€å‘çš„ä¸€ä¸ªå¼€æºåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œç”¨äºå­˜å‚¨å’Œå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®ƒçš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ï¼š
- **åˆ†å¸ƒå¼å­˜å‚¨**: å°†å¤§æ–‡ä»¶åˆ†å—å­˜å‚¨åœ¨å¤šå°æœºå™¨ä¸Š
- **åˆ†å¸ƒå¼è®¡ç®—**: å°†è®¡ç®—ä»»åŠ¡åˆ†å‘åˆ°æ•°æ®æ‰€åœ¨çš„èŠ‚ç‚¹
- **å®¹é”™æ€§**: é€šè¿‡æ•°æ®å‰¯æœ¬æœºåˆ¶ä¿è¯å¯é æ€§
- **æ¨ªå‘æ‰©å±•**: é€šè¿‡å¢åŠ æœºå™¨æ¥æå‡å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›

### 1.2 æ ¸å¿ƒç»„ä»¶

Hadoopç”Ÿæ€ç³»ç»ŸåŒ…å«ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼š

1. **HDFS (Hadoop Distributed File System)**: åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
2. **MapReduce**: åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
3. **YARN (Yet Another Resource Negotiator)**: èµ„æºç®¡ç†æ¡†æ¶

### 1.3 åº”ç”¨åœºæ™¯

- **æ—¥å¿—åˆ†æ**: å¤„ç†æµ·é‡æœåŠ¡å™¨æ—¥å¿—
- **æ•°æ®ä»“åº“**: æ„å»ºä¼ä¸šçº§æ•°æ®ä»“åº“
- **æ¨èç³»ç»Ÿ**: ç¦»çº¿è®¡ç®—ç”¨æˆ·æ¨è
- **æ•°æ®å¤‡ä»½**: å¤§è§„æ¨¡æ•°æ®çš„å¯é å­˜å‚¨
- **ETLå¤„ç†**: æ•°æ®æŠ½å–ã€è½¬æ¢ã€åŠ è½½

## ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ (é‡ç‚¹)

### 2.1 HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ ğŸ”¥

HDFSæ˜¯Hadoopçš„æ ¸å¿ƒå­˜å‚¨ç³»ç»Ÿï¼Œä¸“ä¸ºå¤§æ–‡ä»¶å­˜å‚¨å’Œæ‰¹é‡è¯»å–è®¾è®¡ã€‚

#### 2.1.1 æ¶æ„è®¾è®¡

```
Client
  â†“
NameNode (ä¸»èŠ‚ç‚¹)
  â†“
DataNode1  DataNode2  DataNode3 (ä»èŠ‚ç‚¹)
```

**æ ¸å¿ƒè§’è‰²**:
- **NameNode**: ç®¡ç†æ–‡ä»¶ç³»ç»Ÿçš„å‘½åç©ºé—´å’Œå…ƒæ•°æ®
  - ç»´æŠ¤æ–‡ä»¶ç›®å½•æ ‘
  - è®°å½•æ¯ä¸ªæ–‡ä»¶çš„å—ä¿¡æ¯
  - ç®¡ç†DataNodeçš„å¿ƒè·³å’Œå—æŠ¥å‘Š
  
- **DataNode**: å­˜å‚¨å®é™…çš„æ•°æ®å—
  - å®šæœŸå‘NameNodeå‘é€å¿ƒè·³
  - æ‰§è¡Œæ•°æ®å—çš„è¯»å†™æ“ä½œ
  - è´Ÿè´£æ•°æ®å—çš„å¤åˆ¶

- **SecondaryNameNode**: è¾…åŠ©NameNode
  - å®šæœŸåˆå¹¶ç¼–è¾‘æ—¥å¿—
  - ä¸æ˜¯NameNodeçš„çƒ­å¤‡ä»½

#### 2.1.2 æ•°æ®å­˜å‚¨æœºåˆ¶ (âš ï¸ éš¾ç‚¹)

**å—å­˜å‚¨**:
- é»˜è®¤å—å¤§å°: 128MB (Hadoop 2.x+)
- å¤§æ–‡ä»¶è¢«åˆ‡åˆ†æˆå¤šä¸ªå—
- æ¯ä¸ªå—é»˜è®¤å¤åˆ¶3ä»½å­˜å‚¨åœ¨ä¸åŒèŠ‚ç‚¹

**å‰¯æœ¬æ”¾ç½®ç­–ç•¥**:
```
ç¬¬ä¸€ä¸ªå‰¯æœ¬: å†™å…¥å®¢æˆ·ç«¯æ‰€åœ¨èŠ‚ç‚¹
ç¬¬äºŒä¸ªå‰¯æœ¬: ä¸åŒæœºæ¶çš„éšæœºèŠ‚ç‚¹
ç¬¬ä¸‰ä¸ªå‰¯æœ¬: ä¸ç¬¬äºŒä¸ªå‰¯æœ¬åŒæœºæ¶çš„ä¸åŒèŠ‚ç‚¹
```

è¿™ç§ç­–ç•¥å¹³è¡¡äº†å¯é æ€§ã€å†™å…¥æ€§èƒ½å’Œç½‘ç»œå¸¦å®½ã€‚

#### 2.1.3 è¯»å†™æµç¨‹

**å†™å…¥æµç¨‹**:
```java
1. Clientå‘NameNodeè¯·æ±‚ä¸Šä¼ æ–‡ä»¶
2. NameNodeæ£€æŸ¥æƒé™å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. NameNodeè¿”å›å¯ç”¨çš„DataNodeåˆ—è¡¨
4. Clientå°†æ–‡ä»¶åˆ‡åˆ†æˆå—ï¼Œä¾æ¬¡å†™å…¥DataNode
5. DataNodeä¹‹é—´å»ºç«‹Pipelineè¿›è¡Œå‰¯æœ¬å¤åˆ¶
6. æ‰€æœ‰å‰¯æœ¬å†™å…¥å®Œæˆåï¼Œé€šçŸ¥NameNode
```

**è¯»å–æµç¨‹**:
```java
1. Clientå‘NameNodeè¯·æ±‚æ–‡ä»¶ä½ç½®
2. NameNodeè¿”å›æ–‡ä»¶å—æ‰€åœ¨çš„DataNodeåˆ—è¡¨
3. Clienté€‰æ‹©æœ€è¿‘çš„DataNodeè¯»å–æ•°æ®
4. å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–å‰¯æœ¬
```

### 2.2 MapReduceè®¡ç®—æ¡†æ¶ ğŸ”¥

MapReduceæ˜¯ä¸€ç§ç¼–ç¨‹æ¨¡å‹ï¼Œç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„å¹¶è¡Œè®¡ç®—ã€‚

#### 2.2.1 ç¼–ç¨‹æ¨¡å‹

MapReduceå°†è®¡ç®—åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

**Mapé˜¶æ®µ**: å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¾“å‡ºé”®å€¼å¯¹
```java
map(K1 key, V1 value) -> list(K2, V2)
```

**Reduceé˜¶æ®µ**: åˆå¹¶ç›¸åŒkeyçš„value
```java
reduce(K2 key, list(V2) values) -> list(K3, V3)
```

#### 2.2.2 æ‰§è¡Œæµç¨‹ (âš ï¸ éš¾ç‚¹)

```
Input â†’ Split â†’ Map â†’ Shuffle â†’ Sort â†’ Reduce â†’ Output
```

1. **Input**: è¯»å–HDFSä¸Šçš„è¾“å…¥æ–‡ä»¶
2. **Split**: å°†è¾“å…¥æ–‡ä»¶åˆ‡åˆ†æˆå¤šä¸ªInputSplit
3. **Map**: æ¯ä¸ªSplitç”±ä¸€ä¸ªMapä»»åŠ¡å¤„ç†
4. **Shuffle**: å°†Mapè¾“å‡ºæŒ‰keyåˆ†ç»„ï¼Œä¼ è¾“åˆ°ReduceèŠ‚ç‚¹
5. **Sort**: å¯¹æ¯ä¸ªReduceçš„è¾“å…¥æŒ‰keyæ’åº
6. **Reduce**: å¤„ç†æ¯ä¸ªkeyçš„æ‰€æœ‰value
7. **Output**: å°†ç»“æœå†™å…¥HDFS

**Shuffleæœºåˆ¶**æ˜¯MapReduceçš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯æ€§èƒ½ç“¶é¢ˆï¼š
- Mapç«¯: å†…å­˜ç¼“å†²åŒº â†’ æº¢å†™åˆ°ç£ç›˜ â†’ åˆå¹¶æ’åº
- Reduceç«¯: æ‹‰å–æ•°æ® â†’ åˆå¹¶æ’åº â†’ ä¼ ç»™Reduceå‡½æ•°

### 2.3 YARNèµ„æºç®¡ç† ğŸ”¥

YARNæ˜¯Hadoop 2.xå¼•å…¥çš„èµ„æºç®¡ç†æ¡†æ¶ï¼Œå°†èµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦åˆ†ç¦»ã€‚

#### 2.3.1 æ¶æ„ç»„ä»¶

```
Client
  â†“
ResourceManager (å…¨å±€èµ„æºç®¡ç†)
  â†“
NodeManager1  NodeManager2  NodeManager3 (èŠ‚ç‚¹èµ„æºç®¡ç†)
  â†“
Container (èµ„æºå®¹å™¨)
```

**æ ¸å¿ƒè§’è‰²**:
- **ResourceManager**: å…¨å±€èµ„æºè°ƒåº¦å™¨
  - å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
  - å¯åŠ¨å’Œç›‘æ§ApplicationMaster
  - åˆ†é…èµ„æºç»™å„ä¸ªåº”ç”¨
  
- **NodeManager**: èŠ‚ç‚¹èµ„æºç®¡ç†å™¨
  - ç®¡ç†å•ä¸ªèŠ‚ç‚¹çš„èµ„æº
  - å¯åŠ¨å’Œç›‘æ§Container
  - å‘ResourceManageræ±‡æŠ¥èµ„æºä½¿ç”¨æƒ…å†µ
  
- **ApplicationMaster**: åº”ç”¨ç®¡ç†å™¨
  - æ¯ä¸ªåº”ç”¨æœ‰ä¸€ä¸ªAM
  - å‘RMç”³è¯·èµ„æº
  - ä¸NMé€šä¿¡å¯åŠ¨Container
  
- **Container**: èµ„æºå®¹å™¨
  - å°è£…CPUã€å†…å­˜ç­‰èµ„æº
  - è¿è¡Œå…·ä½“çš„ä»»åŠ¡

#### 2.3.2 ä»»åŠ¡æäº¤æµç¨‹

```java
1. Clientæäº¤åº”ç”¨åˆ°ResourceManager
2. RMåˆ†é…Containerå¯åŠ¨ApplicationMaster
3. AMå‘RMæ³¨å†Œå¹¶ç”³è¯·èµ„æº
4. RMåˆ†é…Containerç»™AM
5. AMä¸NMé€šä¿¡ï¼Œåœ¨Containerä¸­å¯åŠ¨ä»»åŠ¡
6. ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ŒAMå‘RMæ³¨é”€
```

## ğŸ’» å®æˆ˜åº”ç”¨

### 3.1 ç¯å¢ƒæ­å»º

#### 3.1.1 å•æœºä¼ªåˆ†å¸ƒå¼æ¨¡å¼

**ç³»ç»Ÿè¦æ±‚**:
- Linuxç³»ç»Ÿ (æ¨èCentOS 7+)
- JDK 8+
- SSHå…å¯†ç™»å½•

**å®‰è£…æ­¥éª¤**:

```bash
# 1. ä¸‹è½½Hadoop
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -zxvf hadoop-3.3.6.tar.gz
mv hadoop-3.3.6 /usr/local/hadoop

# 2. é…ç½®ç¯å¢ƒå˜é‡
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 3. é…ç½®core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/hadoop/tmp</value>
    </property>
</configuration>

# 4. é…ç½®hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

# 5. æ ¼å¼åŒ–NameNode
hdfs namenode -format

# 6. å¯åŠ¨HDFS
start-dfs.sh

# 7. éªŒè¯
jps  # åº”è¯¥çœ‹åˆ°NameNodeã€DataNodeã€SecondaryNameNode
```

#### 3.1.2 å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤

**é›†ç¾¤è§„åˆ’** (3èŠ‚ç‚¹ç¤ºä¾‹):
```
hadoop01: NameNode, ResourceManager
hadoop02: DataNode, NodeManager
hadoop03: DataNode, NodeManager
```

**é…ç½®è¦ç‚¹**:
- é…ç½®ä¸»æœºåå’Œhostsæ˜ å°„
- é…ç½®SSHå…å¯†ç™»å½•
- åŒæ­¥é…ç½®æ–‡ä»¶åˆ°æ‰€æœ‰èŠ‚ç‚¹
- åœ¨slavesæ–‡ä»¶ä¸­é…ç½®DataNodeåˆ—è¡¨

### 3.2 HDFSåŸºæœ¬æ“ä½œ

#### 3.2.1 å‘½ä»¤è¡Œæ“ä½œ

```bash
# åˆ›å»ºç›®å½•
hdfs dfs -mkdir /user/hadoop

# ä¸Šä¼ æ–‡ä»¶
hdfs dfs -put localfile.txt /user/hadoop/

# æŸ¥çœ‹æ–‡ä»¶
hdfs dfs -ls /user/hadoop
hdfs dfs -cat /user/hadoop/localfile.txt

# ä¸‹è½½æ–‡ä»¶
hdfs dfs -get /user/hadoop/localfile.txt ./

# åˆ é™¤æ–‡ä»¶
hdfs dfs -rm /user/hadoop/localfile.txt

# æŸ¥çœ‹æ–‡ä»¶å—ä¿¡æ¯
hdfs fsck /user/hadoop/localfile.txt -files -blocks -locations
```

#### 3.2.2 Java APIæ“ä½œ

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.io.IOUtils;

/**
 * HDFSæ–‡ä»¶æ“ä½œå·¥å…·ç±»
 * 
 * @author erik.zhou
 */
public class HdfsUtil {
    
    private static final String HDFS_URI = "hdfs://localhost:9000";
    
    /**
     * è·å–FileSystemå®ä¾‹
     */
    public static FileSystem getFileSystem() throws Exception {
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", HDFS_URI);
        return FileSystem.get(conf);
    }
    
    /**
     * ä¸Šä¼ æ–‡ä»¶åˆ°HDFS
     */
    public static void uploadFile(String localPath, String hdfsPath) throws Exception {
        FileSystem fs = getFileSystem();
        Path src = new Path(localPath);
        Path dst = new Path(hdfsPath);
        fs.copyFromLocalFile(src, dst);
        fs.close();
    }
    
    /**
     * ä»HDFSä¸‹è½½æ–‡ä»¶
     */
    public static void downloadFile(String hdfsPath, String localPath) throws Exception {
        FileSystem fs = getFileSystem();
        Path src = new Path(hdfsPath);
        Path dst = new Path(localPath);
        fs.copyToLocalFile(src, dst);
        fs.close();
    }
    
    /**
     * è¯»å–HDFSæ–‡ä»¶å†…å®¹
     */
    public static String readFile(String hdfsPath) throws Exception {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FSDataInputStream in = fs.open(path);
        
        byte[] buffer = new byte[1024];
        StringBuilder content = new StringBuilder();
        int bytesRead;
        
        while ((bytesRead = in.read(buffer)) > 0) {
            content.append(new String(buffer, 0, bytesRead));
        }
        
        IOUtils.closeStream(in);
        fs.close();
        
        return content.toString();
    }
    
    /**
     * å†™å…¥å†…å®¹åˆ°HDFSæ–‡ä»¶
     */
    public static void writeFile(String hdfsPath, String content) throws Exception {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FSDataOutputStream out = fs.create(path);
        out.write(content.getBytes());
        out.close();
        fs.close();
    }
    
    /**
     * åˆ é™¤HDFSæ–‡ä»¶æˆ–ç›®å½•
     */
    public static boolean deleteFile(String hdfsPath) throws Exception {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        boolean result = fs.delete(path, true);
        fs.close();
        return result;
    }
}
```

### 3.3 MapReduceç¼–ç¨‹å®æˆ˜

#### 3.3.1 WordCountç¤ºä¾‹

ç»å…¸çš„å•è¯è®¡æ•°ç¨‹åºï¼Œç»Ÿè®¡æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ã€‚

**Mapperç±»**:
```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * WordCount Mapper
 * 
 * @author erik.zhou
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    
    private final static IntWritable ONE = new IntWritable(1);
    private Text word = new Text();
    
    @Override
    protected void map(LongWritable key, Text value, Context context) 
            throws IOException, InterruptedException {
        // è·å–ä¸€è¡Œæ–‡æœ¬
        String line = value.toString();
        
        // åˆ†å‰²å•è¯
        String[] words = line.split("\\s+");
        
        // è¾“å‡ºæ¯ä¸ªå•è¯å’Œè®¡æ•°1
        for (String w : words) {
            word.set(w);
            context.write(word, ONE);
        }
    }
}
```

**Reducerç±»**:
```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * WordCount Reducer
 * 
 * @author erik.zhou
 */
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    
    private IntWritable result = new IntWritable();
    
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) 
            throws IOException, InterruptedException {
        // ç´¯åŠ ç›¸åŒå•è¯çš„è®¡æ•°
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }
        
        result.set(sum);
        context.write(key, result);
    }
}
```

**Driverç±»**:
```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * WordCountä¸»ç¨‹åº
 * 
 * @author erik.zhou
 */
public class WordCountDriver {
    
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver <input path> <output path>");
            System.exit(-1);
        }
        
        // åˆ›å»ºé…ç½®
        Configuration conf = new Configuration();
        
        // åˆ›å»ºJob
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCountDriver.class);
        
        // è®¾ç½®Mapperå’ŒReducer
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);
        
        // è®¾ç½®è¾“å‡ºç±»å‹
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        // è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        
        // æäº¤ä»»åŠ¡
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

**è¿è¡Œä»»åŠ¡**:
```bash
# ç¼–è¯‘
javac -classpath `hadoop classpath` WordCount*.java
jar -cvf wordcount.jar WordCount*.class

# å‡†å¤‡è¾“å…¥æ•°æ®
echo "hello world hello hadoop" > input.txt
hdfs dfs -put input.txt /input/

# è¿è¡ŒMapReduceä»»åŠ¡
hadoop jar wordcount.jar WordCountDriver /input /output

# æŸ¥çœ‹ç»“æœ
hdfs dfs -cat /output/part-r-00000
```

### 3.4 ä¸Javaåç«¯é›†æˆ

#### 3.4.1 Spring Booté›†æˆHadoop

**Mavenä¾èµ–**:
```xml
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.3.6</version>
</dependency>
```

**é…ç½®ç±»**:
```java
import org.apache.hadoop.fs.FileSystem;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * Hadoopé…ç½®ç±»
 * 
 * @author erik.zhou
 */
@Configuration
public class HadoopConfig {
    
    @Value("${hadoop.hdfs.uri}")
    private String hdfsUri;
    
    @Bean
    public FileSystem fileSystem() throws Exception {
        org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration();
        conf.set("fs.defaultFS", hdfsUri);
        conf.set("dfs.client.use.datanode.hostname", "true");
        return FileSystem.get(conf);
    }
}
```

**Serviceå±‚**:
```java
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.web.multipart.MultipartFile;

import java.io.IOException;

/**
 * HDFSæ–‡ä»¶æœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class HdfsService {
    
    @Autowired
    private FileSystem fileSystem;
    
    /**
     * ä¸Šä¼ æ–‡ä»¶åˆ°HDFS
     */
    public String uploadFile(MultipartFile file, String destPath) throws IOException {
        Path path = new Path(destPath);
        fileSystem.copyFromLocalFile(false, true, 
            new Path(file.getOriginalFilename()), path);
        return destPath;
    }
    
    /**
     * åˆ é™¤HDFSæ–‡ä»¶
     */
    public boolean deleteFile(String filePath) throws IOException {
        return fileSystem.delete(new Path(filePath), true);
    }
    
    /**
     * æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
     */
    public boolean exists(String filePath) throws IOException {
        return fileSystem.exists(new Path(filePath));
    }
}
```

## âœ¨ æœ€ä½³å®è·µ

### 4.1 HDFSä½¿ç”¨å»ºè®®

#### 4.1.1 æ–‡ä»¶å¤§å°ä¼˜åŒ–

**æ¨èåšæ³•**:
- å•ä¸ªæ–‡ä»¶å¤§å°: 128MB - 1GB
- é¿å…å¤§é‡å°æ–‡ä»¶ (< 10MB)
- ä½¿ç”¨SequenceFileæˆ–Avroåˆå¹¶å°æ–‡ä»¶

**åŸå› **:
- NameNodeå†…å­˜æœ‰é™ï¼Œæ¯ä¸ªæ–‡ä»¶/å—éƒ½å ç”¨å†…å­˜
- å°æ–‡ä»¶ä¼šå¯¼è‡´å¤§é‡çš„å…ƒæ•°æ®å¼€é”€
- MapReduceå¤„ç†å°æ–‡ä»¶æ•ˆç‡ä½

**å°æ–‡ä»¶åˆå¹¶ç¤ºä¾‹**:
```java
/**
 * åˆå¹¶å°æ–‡ä»¶åˆ°SequenceFile
 * 
 * @author erik.zhou
 */
public class SmallFileMerger {
    
    public static void mergeFiles(String inputDir, String outputFile) throws Exception {
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(conf);
        
        // åˆ›å»ºSequenceFile Writer
        SequenceFile.Writer writer = SequenceFile.createWriter(
            conf,
            SequenceFile.Writer.file(new Path(outputFile)),
            SequenceFile.Writer.keyClass(Text.class),
            SequenceFile.Writer.valueClass(BytesWritable.class)
        );
        
        // éå†è¾“å…¥ç›®å½•
        FileStatus[] files = fs.listStatus(new Path(inputDir));
        for (FileStatus file : files) {
            if (file.isFile()) {
                // è¯»å–æ–‡ä»¶å†…å®¹
                FSDataInputStream in = fs.open(file.getPath());
                byte[] buffer = new byte[(int) file.getLen()];
                in.readFully(buffer);
                in.close();
                
                // å†™å…¥SequenceFile
                writer.append(new Text(file.getPath().getName()), 
                             new BytesWritable(buffer));
            }
        }
        
        writer.close();
        fs.close();
    }
}
```

#### 4.1.2 å‰¯æœ¬æ•°é‡è®¾ç½®

**æ¨èé…ç½®**:
- é‡è¦æ•°æ®: 3å‰¯æœ¬ (é»˜è®¤)
- ä¸´æ—¶æ•°æ®: 1-2å‰¯æœ¬
- å†·æ•°æ®: 2å‰¯æœ¬

**åŠ¨æ€è°ƒæ•´å‰¯æœ¬æ•°**:
```bash
# è®¾ç½®æ–‡ä»¶å‰¯æœ¬æ•°ä¸º2
hdfs dfs -setrep -w 2 /user/hadoop/data.txt
```

#### 4.1.3 æ•°æ®å‹ç¼©

**æ¨èå‹ç¼©æ ¼å¼**:
- **Snappy**: å‹ç¼©é€Ÿåº¦å¿«ï¼Œé€‚åˆä¸­é—´æ•°æ®
- **LZO**: æ”¯æŒåˆ†ç‰‡ï¼Œé€‚åˆMapReduceè¾“å…¥
- **Gzip**: å‹ç¼©ç‡é«˜ï¼Œé€‚åˆå½’æ¡£æ•°æ®

**å¯ç”¨å‹ç¼©**:
```java
// åœ¨MapReduceä¸­å¯ç”¨å‹ç¼©
Configuration conf = new Configuration();
conf.setBoolean("mapreduce.map.output.compress", true);
conf.setClass("mapreduce.map.output.compress.codec", 
              SnappyCodec.class, CompressionCodec.class);
```

### 4.2 MapReduceæ€§èƒ½ä¼˜åŒ–

#### 4.2.1 Combinerä¼˜åŒ–

ä½¿ç”¨Combineråœ¨Mapç«¯è¿›è¡Œå±€éƒ¨èšåˆï¼Œå‡å°‘ç½‘ç»œä¼ è¾“ï¼š

```java
// åœ¨Driverä¸­è®¾ç½®Combiner
job.setCombinerClass(WordCountReducer.class);
```

#### 4.2.2 åˆ†åŒºä¼˜åŒ–

è‡ªå®šä¹‰Partitionerå®ç°æ•°æ®å‡è¡¡åˆ†å¸ƒï¼š

```java
/**
 * è‡ªå®šä¹‰åˆ†åŒºå™¨
 * 
 * @author erik.zhou
 */
public class CustomPartitioner extends Partitioner<Text, IntWritable> {
    
    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        // æ ¹æ®keyçš„é¦–å­—æ¯åˆ†åŒº
        char firstChar = key.toString().charAt(0);
        return (firstChar % numPartitions);
    }
}
```

#### 4.2.3 å†…å­˜è°ƒä¼˜

**å…³é”®å‚æ•°**:
```xml
<!-- Mapä»»åŠ¡å†…å­˜ -->
<property>
    <name>mapreduce.map.memory.mb</name>
    <value>2048</value>
</property>

<!-- Reduceä»»åŠ¡å†…å­˜ -->
<property>
    <name>mapreduce.reduce.memory.mb</name>
    <value>4096</value>
</property>

<!-- Mapè¾“å‡ºç¼“å†²åŒºå¤§å° -->
<property>
    <name>mapreduce.task.io.sort.mb</name>
    <value>200</value>
</property>
```

### 4.3 é›†ç¾¤è¿ç»´å»ºè®®

#### 4.3.1 ç›‘æ§æŒ‡æ ‡

**HDFSç›‘æ§**:
- NameNodeå †å†…å­˜ä½¿ç”¨ç‡
- DataNodeç£ç›˜ä½¿ç”¨ç‡
- å—æŸåæ•°é‡
- å‰¯æœ¬ä¸è¶³çš„å—æ•°é‡

**YARNç›‘æ§**:
- é›†ç¾¤èµ„æºä½¿ç”¨ç‡
- ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦
- ä»»åŠ¡å¤±è´¥ç‡
- èŠ‚ç‚¹å¥åº·çŠ¶æ€

**ç›‘æ§å·¥å…·**:
- Hadoopè‡ªå¸¦Web UI (NameNode: 9870, ResourceManager: 8088)
- Ambari
- Cloudera Manager
- Prometheus + Grafana

#### 4.3.2 å®¹é‡è§„åˆ’

**å­˜å‚¨å®¹é‡**:
```
å®é™…å¯ç”¨å®¹é‡ = æ€»ç£ç›˜å®¹é‡ Ã— (1 - é¢„ç•™ç©ºé—´) / å‰¯æœ¬æ•°
```

**è®¡ç®—èµ„æº**:
- CPU: æ¯ä¸ªèŠ‚ç‚¹8-16æ ¸
- å†…å­˜: æ¯ä¸ªèŠ‚ç‚¹64-128GB
- ç£ç›˜: æ¯ä¸ªèŠ‚ç‚¹12-24å—SATAç›˜

#### 4.3.3 å¤‡ä»½ç­–ç•¥

**NameNodeå…ƒæ•°æ®å¤‡ä»½**:
```bash
# é…ç½®å¤šä¸ªå…ƒæ•°æ®å­˜å‚¨ç›®å½•
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///data1/hadoop/name,file:///data2/hadoop/name</value>
</property>

# å®šæœŸå¤‡ä»½fsimageå’Œedits
hdfs dfsadmin -fetchImage /backup/fsimage
```

## âš ï¸ å¸¸è§é™·é˜±

### é™·é˜±1: NameNodeå•ç‚¹æ•…éšœ

**é—®é¢˜**: NameNodeå®•æœºå¯¼è‡´æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆ**:
- é…ç½®NameNode HA (High Availability)
- ä½¿ç”¨ZooKeeperå®ç°è‡ªåŠ¨æ•…éšœè½¬ç§»
- å®šæœŸå¤‡ä»½å…ƒæ•°æ®

### é™·é˜±2: æ•°æ®å€¾æ–œ

**é—®é¢˜**: æŸäº›Reduceä»»åŠ¡å¤„ç†çš„æ•°æ®é‡è¿œå¤§äºå…¶ä»–ä»»åŠ¡

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨è‡ªå®šä¹‰Partitioner
- å¯¹çƒ­ç‚¹keyè¿›è¡Œé¢„å¤„ç†
- å¢åŠ Reduceä»»åŠ¡æ•°é‡

### é™·é˜±3: å°æ–‡ä»¶é—®é¢˜

**é—®é¢˜**: å¤§é‡å°æ–‡ä»¶å¯¼è‡´NameNodeå†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨HAR (Hadoop Archive)å½’æ¡£å°æ–‡ä»¶
- ä½¿ç”¨SequenceFileåˆå¹¶å°æ–‡ä»¶
- åœ¨æ•°æ®å†™å…¥æ—¶å°±é¿å…äº§ç”Ÿå°æ–‡ä»¶

### é™·é˜±4: ç£ç›˜ç©ºé—´ä¸å‡è¡¡

**é—®é¢˜**: æŸäº›DataNodeç£ç›˜æ»¡ï¼Œå…¶ä»–èŠ‚ç‚¹ç©ºé—²

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯åŠ¨æ•°æ®å‡è¡¡å™¨
hdfs balancer -threshold 10
```

## â“ å¸¸è§é—®é¢˜

### Q1: Hadoopé€‚åˆä»€ä¹ˆæ ·çš„åœºæ™¯ï¼Ÿ

**A**: Hadoopé€‚åˆä»¥ä¸‹åœºæ™¯ï¼š
- **å¤§è§„æ¨¡æ•°æ®å­˜å‚¨**: TB/PBçº§åˆ«çš„æ•°æ®
- **æ‰¹å¤„ç†è®¡ç®—**: ç¦»çº¿æ•°æ®åˆ†æã€ETL
- **ä¸€æ¬¡å†™å…¥å¤šæ¬¡è¯»å–**: æ•°æ®ä¸é¢‘ç¹ä¿®æ”¹
- **é«˜ååé‡**: å¯¹å»¶è¿Ÿè¦æ±‚ä¸é«˜

**ä¸é€‚åˆçš„åœºæ™¯**:
- å®æ—¶è®¡ç®— (ä½¿ç”¨Spark/Flink)
- å°æ–‡ä»¶å­˜å‚¨
- éšæœºè¯»å†™ (ä½¿ç”¨HBase)
- ä½å»¶è¿ŸæŸ¥è¯¢ (ä½¿ç”¨Elasticsearch)

### Q2: HDFSå’Œä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸»è¦åŒºåˆ«ï¼š
- **æ–‡ä»¶å¤§å°**: HDFSé€‚åˆå¤§æ–‡ä»¶ï¼Œä¼ ç»ŸFSé€‚åˆå„ç§å¤§å°
- **è®¿é—®æ¨¡å¼**: HDFSä¸€æ¬¡å†™å…¥å¤šæ¬¡è¯»å–ï¼Œä¼ ç»ŸFSæ”¯æŒéšæœºè¯»å†™
- **æ•°æ®ä½ç½®**: HDFSæ•°æ®åˆ†å¸ƒåœ¨å¤šå°æœºå™¨ï¼Œä¼ ç»ŸFSåœ¨å•æœº
- **å®¹é”™æ€§**: HDFSé€šè¿‡å‰¯æœ¬æœºåˆ¶ä¿è¯å¯é æ€§
- **æ€§èƒ½**: HDFSä¼˜åŒ–äº†å¤§æ–‡ä»¶çš„é¡ºåºè¯»å†™

### Q3: MapReduceå’ŒSparkæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸»è¦åŒºåˆ«ï¼š
- **è®¡ç®—æ¨¡å‹**: MapReduceåŸºäºç£ç›˜ï¼ŒSparkåŸºäºå†…å­˜
- **æ€§èƒ½**: Sparkæ¯”MapReduceå¿«10-100å€
- **æ˜“ç”¨æ€§**: Spark APIæ›´ç®€æ´ï¼Œæ”¯æŒå¤šç§è¯­è¨€
- **é€‚ç”¨åœºæ™¯**: MapReduceé€‚åˆæ‰¹å¤„ç†ï¼ŒSparké€‚åˆè¿­ä»£è®¡ç®—å’Œå®æ—¶å¤„ç†

**å»ºè®®**: æ–°é¡¹ç›®ä¼˜å…ˆè€ƒè™‘Sparkï¼Œä½†Hadoopç”Ÿæ€ä»ç„¶é‡è¦ã€‚

### Q4: å¦‚ä½•é€‰æ‹©åˆé€‚çš„å—å¤§å°ï¼Ÿ

**A**: å—å¤§å°é€‰æ‹©è€ƒè™‘å› ç´ ï¼š
- **æ–‡ä»¶å¤§å°**: å—å¤§å°åº”è¯¥æ˜¯æ–‡ä»¶å¤§å°çš„1/100åˆ°1/10
- **ç½‘ç»œå¸¦å®½**: å—è¶Šå¤§ï¼Œç½‘ç»œä¼ è¾“æ—¶é—´å æ¯”è¶Šå°
- **Mapä»»åŠ¡æ•°**: å—æ•°é‡å†³å®šMapä»»åŠ¡æ•°é‡

**æ¨èé…ç½®**:
- é»˜è®¤128MBé€‚åˆå¤§å¤šæ•°åœºæ™¯
- è¶…å¤§æ–‡ä»¶ (>10GB) å¯ä»¥ä½¿ç”¨256MBæˆ–512MB
- å°æ–‡ä»¶åœºæ™¯è€ƒè™‘ä½¿ç”¨64MB

### Q5: Hadoopåœ¨Javaåç«¯é¡¹ç›®ä¸­çš„åº”ç”¨åœºæ™¯ï¼Ÿ

**A**: å…¸å‹åº”ç”¨åœºæ™¯ï¼š
1. **æ—¥å¿—åˆ†æ**: æ”¶é›†å’Œåˆ†æåº”ç”¨æ—¥å¿—
2. **æ•°æ®ä»“åº“**: æ„å»ºç¦»çº¿æ•°æ®ä»“åº“
3. **æŠ¥è¡¨ç”Ÿæˆ**: ç”Ÿæˆæ¯æ—¥/æ¯æœˆä¸šåŠ¡æŠ¥è¡¨
4. **æ¨èç³»ç»Ÿ**: ç¦»çº¿è®¡ç®—ç”¨æˆ·æ¨è
5. **æ•°æ®å¤‡ä»½**: å¤§è§„æ¨¡æ•°æ®çš„å¯é å­˜å‚¨

**é›†æˆæ–¹å¼**:
- ä½¿ç”¨Hadoop Client APIç›´æ¥æ“ä½œHDFS
- é€šè¿‡Hiveè¿›è¡ŒSQLæŸ¥è¯¢
- ä½¿ç”¨Sqoopå¯¼å…¥å¯¼å‡ºæ•°æ®
- ç»“åˆSparkè¿›è¡Œæ•°æ®å¤„ç†

## ğŸ”— ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Apache Hadoopå®˜ç½‘](https://hadoop.apache.org/)
- [Hadoopæ–‡æ¡£](https://hadoop.apache.org/docs/stable/)
- [HDFSæ¶æ„æŒ‡å—](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
- [MapReduceæ•™ç¨‹](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)

### æ¨èä¹¦ç±
- ã€ŠHadoopæƒå¨æŒ‡å—ã€‹(ç¬¬4ç‰ˆ)
- ã€ŠHadoopå®æˆ˜ã€‹
- ã€Šå¤§æ•°æ®æŠ€æœ¯åŸç†ä¸åº”ç”¨ã€‹

### åœ¨çº¿èµ„æº
- [Hadoopä¸­æ–‡ç¤¾åŒº](http://hadoop.apache.org/zh-cn/)
- [Clouderaæ–‡æ¡£](https://docs.cloudera.com/)
- [Hortonworksæ–‡æ¡£](https://docs.hortonworks.com/)

### ç›¸å…³æŠ€æœ¯
- **Hive**: æ•°æ®ä»“åº“å·¥å…·ï¼Œæä¾›SQLæ¥å£
- **HBase**: åˆ†å¸ƒå¼NoSQLæ•°æ®åº“
- **Spark**: å†…å­˜è®¡ç®—æ¡†æ¶
- **Flink**: æµå¤„ç†æ¡†æ¶
- **Sqoop**: æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Hadoopçš„æ ¸å¿ƒæ¶æ„å’Œè®¾è®¡ç†å¿µ
- [ ] æŒæ¡HDFSçš„è¯»å†™æµç¨‹å’Œå‰¯æœ¬æœºåˆ¶
- [ ] ç†è§£MapReduceçš„æ‰§è¡Œæµç¨‹å’ŒShuffleæœºåˆ¶
- [ ] æŒæ¡YARNçš„èµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦
- [ ] èƒ½å¤Ÿç¼–å†™ç®€å•çš„MapReduceç¨‹åº
- [ ] äº†è§£Hadoopçš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•
- [ ] æŒæ¡Hadoopé›†ç¾¤çš„è¿ç»´è¦ç‚¹
- [ ] äº†è§£Hadoopåœ¨Javaåç«¯çš„åº”ç”¨åœºæ™¯

---

**@author** erik.zhou
**æœ€åæ›´æ–°**: 2024-01-04
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
