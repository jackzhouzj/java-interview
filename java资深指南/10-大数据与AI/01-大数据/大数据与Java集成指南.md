# å¤§æ•°æ®ä¸Javaé›†æˆæŒ‡å—

## ğŸ“‹ ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [é›†æˆæ¶æ„](#é›†æˆæ¶æ„)
- [Hadoopé›†æˆ](#hadoopé›†æˆ)
- [Sparké›†æˆ](#sparké›†æˆ)
- [Flinké›†æˆ](#flinké›†æˆ)
- [Hiveé›†æˆ](#hiveé›†æˆ)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨Javaåç«¯é¡¹ç›®ä¸­é›†æˆå¤§æ•°æ®æŠ€æœ¯ï¼ŒåŒ…æ‹¬Hadoopã€Sparkã€Flinkå’ŒHiveï¼Œå¸®åŠ©å¼€å‘è€…æ„å»ºæ•°æ®å¯†é›†å‹åº”ç”¨ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦å¤§æ•°æ®é›†æˆï¼Ÿ

- **æµ·é‡æ•°æ®å¤„ç†**: å¤„ç†TB/PBçº§æ•°æ®
- **å®æ—¶æ•°æ®åˆ†æ**: å®æ—¶ç›‘æ§å’Œåˆ†æ
- **ç¦»çº¿æ•°æ®ä»“åº“**: æ„å»ºä¼ä¸šçº§æ•°æ®ä»“åº“
- **æœºå™¨å­¦ä¹ **: å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
- **æ—¥å¿—åˆ†æ**: åˆ†ææµ·é‡æ—¥å¿—æ•°æ®

## ğŸ—ï¸ é›†æˆæ¶æ„

### å…¸å‹æ¶æ„

```
Javaåç«¯åº”ç”¨
  â†“
æ•°æ®é‡‡é›†å±‚ (Kafka, Flume)
  â†“
æ•°æ®å­˜å‚¨å±‚ (HDFS, HBase)
  â†“
æ•°æ®å¤„ç†å±‚ (Spark, Flink, Hive)
  â†“
æ•°æ®å±•ç¤ºå±‚ (API, Dashboard)
```

### æŠ€æœ¯é€‰å‹

| åœºæ™¯ | æ¨èæŠ€æœ¯ | åŸå›  |
|------|---------|------|
| æ‰¹å¤„ç† | Spark, Hive | é«˜æ€§èƒ½ï¼Œæ˜“ç”¨ |
| æµå¤„ç† | Flink, Spark Streaming | ä½å»¶è¿Ÿï¼Œé«˜åå |
| æ•°æ®å­˜å‚¨ | HDFS, HBase | å¯é ï¼Œå¯æ‰©å±• |
| æ•°æ®æŸ¥è¯¢ | Hive, Presto | SQLæ¥å£ï¼Œæ˜“ç”¨ |

## ğŸ’» Hadoopé›†æˆ

### Mavenä¾èµ–

```xml
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.3.6</version>
</dependency>
```

### Spring Booté…ç½®

```java
import org.apache.hadoop.fs.FileSystem;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * Hadoopé…ç½®
 * 
 * @author erik.zhou
 */
@Configuration
public class HadoopConfig {
    
    @Value("${hadoop.hdfs.uri}")
    private String hdfsUri;
    
    @Bean
    public FileSystem fileSystem() throws Exception {
        org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration();
        conf.set("fs.defaultFS", hdfsUri);
        return FileSystem.get(conf);
    }
}
```

### ä½¿ç”¨ç¤ºä¾‹

```java
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

/**
 * HDFSæœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class HdfsService {
    
    @Autowired
    private FileSystem fileSystem;
    
    public void uploadFile(String localPath, String hdfsPath) throws Exception {
        fileSystem.copyFromLocalFile(new Path(localPath), new Path(hdfsPath));
    }
    
    public void downloadFile(String hdfsPath, String localPath) throws Exception {
        fileSystem.copyToLocalFile(new Path(hdfsPath), new Path(localPath));
    }
}
```

## ğŸ’» Sparké›†æˆ

### Mavenä¾èµ–

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.12</artifactId>
    <version>3.5.0</version>
</dependency>
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.12</artifactId>
    <version>3.5.0</version>
</dependency>
```

### Spring Booté…ç½®

```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * Sparké…ç½®
 * 
 * @author erik.zhou
 */
@Configuration
public class SparkConfig {
    
    @Bean
    public SparkConf sparkConf() {
        return new SparkConf()
            .setAppName("JavaBackendApp")
            .setMaster("local[*]");
    }
    
    @Bean
    public JavaSparkContext javaSparkContext() {
        return new JavaSparkContext(sparkConf());
    }
    
    @Bean
    public SparkSession sparkSession() {
        return SparkSession.builder()
            .config(sparkConf())
            .getOrCreate();
    }
}
```

### ä½¿ç”¨ç¤ºä¾‹

```java
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

/**
 * Sparkæ•°æ®å¤„ç†æœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class SparkDataService {
    
    @Autowired
    private SparkSession sparkSession;
    
    public long analyzeUserBehavior(String date) {
        Dataset<Row> logs = sparkSession.read()
            .parquet("hdfs://logs/user_behavior/" + date);
        
        return logs.filter("action = 'purchase'").count();
    }
}
```

## ğŸ’» Flinké›†æˆ

### Mavenä¾èµ–

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java</artifactId>
    <version>1.18.0</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-clients</artifactId>
    <version>1.18.0</version>
</dependency>
```

### Spring Booté…ç½®

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * Flinké…ç½®
 * 
 * @author erik.zhou
 */
@Configuration
public class FlinkConfig {
    
    @Bean
    public StreamExecutionEnvironment streamExecutionEnvironment() {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(5000);
        return env;
    }
}
```

### ä½¿ç”¨ç¤ºä¾‹

```java
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

/**
 * Flinkæµå¤„ç†æœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class FlinkStreamService {
    
    @Autowired
    private StreamExecutionEnvironment env;
    
    public void processRealTimeData() throws Exception {
        DataStream<String> stream = env.socketTextStream("localhost", 9999);
        
        stream
            .filter(line -> line.contains("ERROR"))
            .print();
        
        env.execute("Real-time Error Detection");
    }
}
```

## ğŸ’» Hiveé›†æˆ

### Mavenä¾èµ–

```xml
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>3.1.3</version>
</dependency>
```

### Spring Booté…ç½®

```java
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.jdbc.datasource.DriverManagerDataSource;

import javax.sql.DataSource;

/**
 * Hiveé…ç½®
 * 
 * @author erik.zhou
 */
@Configuration
public class HiveConfig {
    
    @Value("${hive.url}")
    private String url;
    
    @Bean
    public DataSource hiveDataSource() {
        DriverManagerDataSource dataSource = new DriverManagerDataSource();
        dataSource.setDriverClassName("org.apache.hive.jdbc.HiveDriver");
        dataSource.setUrl(url);
        return dataSource;
    }
    
    @Bean
    public JdbcTemplate hiveJdbcTemplate() {
        return new JdbcTemplate(hiveDataSource());
    }
}
```

### ä½¿ç”¨ç¤ºä¾‹

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.Map;

/**
 * HiveæŸ¥è¯¢æœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class HiveQueryService {
    
    @Autowired
    private JdbcTemplate hiveJdbcTemplate;
    
    public List<Map<String, Object>> queryUserStats(String date) {
        String sql = "SELECT city, COUNT(*) as user_count " +
                    "FROM users " +
                    "WHERE date = '" + date + "' " +
                    "GROUP BY city";
        return hiveJdbcTemplate.queryForList(sql);
    }
}
```

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: å®æ—¶æ—¥å¿—åˆ†æç³»ç»Ÿ

**æ¶æ„**:
```
åº”ç”¨æ—¥å¿— â†’ Kafka â†’ Flink â†’ Elasticsearch â†’ Kibana
```

**å®ç°**:
```java
/**
 * å®æ—¶æ—¥å¿—åˆ†æ
 * 
 * @author erik.zhou
 */
@Service
public class RealTimeLogAnalysis {
    
    @Autowired
    private StreamExecutionEnvironment env;
    
    public void startAnalysis() throws Exception {
        // ä»Kafkaè¯»å–æ—¥å¿—
        FlinkKafkaConsumer<String> kafkaSource = new FlinkKafkaConsumer<>(
            "logs",
            new SimpleStringSchema(),
            getKafkaProperties()
        );
        
        DataStream<String> logs = env.addSource(kafkaSource);
        
        // åˆ†æé”™è¯¯æ—¥å¿—
        DataStream<ErrorLog> errors = logs
            .filter(line -> line.contains("ERROR"))
            .map(this::parseErrorLog);
        
        // å†™å…¥Elasticsearch
        errors.addSink(new ElasticsearchSink<>(getEsConfig()));
        
        env.execute("Real-time Log Analysis");
    }
}
```

### æ¡ˆä¾‹2: ç¦»çº¿æ•°æ®ä»“åº“

**æ¶æ„**:
```
ä¸šåŠ¡æ•°æ®åº“ â†’ Sqoop â†’ HDFS â†’ Hive â†’ BIå·¥å…·
```

**å®ç°**:
```java
/**
 * ç¦»çº¿æ•°æ®ä»“åº“ETL
 * 
 * @author erik.zhou
 */
@Service
public class DataWarehouseETL {
    
    @Autowired
    private SparkSession spark;
    
    @Scheduled(cron = "0 0 2 * * ?")  // æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    public void dailyETL() {
        // ä»MySQLè¯»å–æ•°æ®
        Dataset<Row> orders = spark.read()
            .format("jdbc")
            .option("url", "jdbc:mysql://localhost:3306/db")
            .option("dbtable", "orders")
            .load();
        
        // æ•°æ®æ¸…æ´—å’Œè½¬æ¢
        Dataset<Row> cleaned = orders
            .filter("status = 'completed'")
            .withColumn("date", date_format(col("create_time"), "yyyy-MM-dd"));
        
        // å†™å…¥Hive
        cleaned.write()
            .mode("append")
            .partitionBy("date")
            .saveAsTable("dw.orders");
    }
}
```

### æ¡ˆä¾‹3: ç”¨æˆ·è¡Œä¸ºåˆ†æ

**æ¶æ„**:
```
ç”¨æˆ·è¡Œä¸º â†’ Kafka â†’ Spark Streaming â†’ Redis â†’ API
```

**å®ç°**:
```java
/**
 * ç”¨æˆ·è¡Œä¸ºå®æ—¶åˆ†æ
 * 
 * @author erik.zhou
 */
@Service
public class UserBehaviorAnalysis {
    
    @Autowired
    private JavaSparkContext sc;
    
    public void analyzeUserBehavior() {
        // ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        JavaInputDStream<String> stream = KafkaUtils.createDirectStream(
            sc,
            LocationStrategies.PreferConsistent(),
            ConsumerStrategies.Subscribe(Arrays.asList("user_behavior"), getKafkaParams())
        );
        
        // ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ¬¡æ•°
        stream
            .map(record -> parseUserBehavior(record.value()))
            .mapToPair(behavior -> new Tuple2<>(behavior.userId, 1))
            .reduceByKey((a, b) -> a + b)
            .foreachRDD(rdd -> {
                // å†™å…¥Redis
                rdd.foreach(tuple -> {
                    redisTemplate.opsForValue().set(
                        "user:" + tuple._1 + ":count",
                        tuple._2.toString()
                    );
                });
            });
    }
}
```

## âœ¨ æœ€ä½³å®è·µ

### 1. æ¶æ„è®¾è®¡

- **åˆ†å±‚æ¶æ„**: æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€å¤„ç†ã€å±•ç¤ºåˆ†å±‚
- **è§£è€¦è®¾è®¡**: ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—è§£è€¦å„ä¸ªç»„ä»¶
- **å®¹é”™è®¾è®¡**: è€ƒè™‘å„ä¸ªç¯èŠ‚çš„å®¹é”™æœºåˆ¶
- **å¯æ‰©å±•æ€§**: è®¾è®¡æ”¯æŒæ°´å¹³æ‰©å±•çš„æ¶æ„

### 2. æ€§èƒ½ä¼˜åŒ–

- **æ‰¹é‡å¤„ç†**: æ‰¹é‡è¯»å†™æ•°æ®ï¼Œå‡å°‘ç½‘ç»œå¼€é”€
- **æ•°æ®åˆ†åŒº**: åˆç†åˆ†åŒºæ•°æ®ï¼Œæé«˜å¹¶è¡Œåº¦
- **ç¼“å­˜ç­–ç•¥**: ç¼“å­˜çƒ­ç‚¹æ•°æ®ï¼Œå‡å°‘é‡å¤è®¡ç®—
- **èµ„æºé…ç½®**: åˆç†é…ç½®å†…å­˜ã€CPUç­‰èµ„æº

### 3. ç›‘æ§è¿ç»´

- **æ—¥å¿—ç›‘æ§**: æ”¶é›†å’Œåˆ†æå„ç»„ä»¶æ—¥å¿—
- **æ€§èƒ½ç›‘æ§**: ç›‘æ§ä½œä¸šæ‰§è¡Œæ—¶é—´ã€èµ„æºä½¿ç”¨
- **å‘Šè­¦æœºåˆ¶**: è®¾ç½®å‘Šè­¦è§„åˆ™ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
- **å¤‡ä»½æ¢å¤**: å®šæœŸå¤‡ä»½é‡è¦æ•°æ®

### 4. å®‰å…¨è€ƒè™‘

- **è®¤è¯æˆæƒ**: é…ç½®Kerberosç­‰è®¤è¯æœºåˆ¶
- **æ•°æ®åŠ å¯†**: æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨å’Œä¼ è¾“
- **è®¿é—®æ§åˆ¶**: é™åˆ¶æ•°æ®è®¿é—®æƒé™
- **å®¡è®¡æ—¥å¿—**: è®°å½•æ•°æ®è®¿é—®å’Œæ“ä½œæ—¥å¿—

## ğŸ“ æ€»ç»“

å¤§æ•°æ®æŠ€æœ¯ä¸Javaåç«¯çš„é›†æˆä¸ºå¤„ç†æµ·é‡æ•°æ®æä¾›äº†å¼ºå¤§çš„èƒ½åŠ›ã€‚é€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡å’ŒæŠ€æœ¯é€‰å‹ï¼Œå¯ä»¥æ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„æ•°æ®å¯†é›†å‹åº”ç”¨ã€‚

**å…³é”®è¦ç‚¹**:
1. æ ¹æ®ä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„å¤§æ•°æ®æŠ€æœ¯
2. ä½¿ç”¨Spring Bootç®€åŒ–é›†æˆé…ç½®
3. æ³¨é‡æ€§èƒ½ä¼˜åŒ–å’Œå®¹é”™è®¾è®¡
4. å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œè¿ç»´ä½“ç³»

---

**@author** erik.zhou
**æœ€åæ›´æ–°**: 2024-01-04
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
