# ELK å®Œæ•´æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- åŸºç¡€æ¦‚å¿µ
- æ ¸å¿ƒç‰¹æ€§
- å®æˆ˜åº”ç”¨
- æœ€ä½³å®è·µ
- å¸¸è§é—®é¢˜

## ğŸ“š æŠ€æœ¯æ¦‚è¿°
- **ç‰ˆæœ¬**: Elasticsearch 8.x / Logstash 8.x / Kibana 8.x
- **å®˜æ–¹æ–‡æ¡£**: https://www.elastic.co/
- **å­¦ä¹ éš¾åº¦**: â­â­â­â­ (1-5æ˜Ÿ)
- **é‡è¦ç¨‹åº¦**: â­â­â­â­â­ (1-5æ˜Ÿ)
- **å‰ç½®çŸ¥è¯†**: LinuxåŸºç¡€ã€æ—¥å¿—æ¦‚å¿µã€JSONã€RESTful API
- **æ–‡æ¡£æ¥æº**: Context7 + å®˜æ–¹æ–‡æ¡£
- **æ›´æ–°æ—¶é—´**: 2024-01-04
- **ä½œè€…**: @author erik.zhou

## ğŸ¯ å­¦ä¹ ç›®æ ‡
- [ ] ç†è§£ELK Stackçš„æ¶æ„å’Œå·¥ä½œåŸç†
- [ ] æŒæ¡Elasticsearchçš„å®‰è£…å’Œé…ç½®
- [ ] æŒæ¡Logstashçš„æ—¥å¿—æ”¶é›†å’Œå¤„ç†
- [ ] æŒæ¡Kibanaçš„æ•°æ®å¯è§†åŒ–å’ŒæŸ¥è¯¢
- [ ] æŒæ¡Filebeatçš„æ—¥å¿—é‡‡é›†
- [ ] ç†è§£ELKåœ¨ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²æ–¹æ¡ˆ
- [ ] æŒæ¡æ—¥å¿—åˆ†æå’Œé—®é¢˜æ’æŸ¥æŠ€å·§

## ğŸ“– åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ELK Stack

ELK Stackæ˜¯ç”±Elasticsearchã€Logstashã€Kibanaä¸‰ä¸ªå¼€æºé¡¹ç›®ç»„æˆçš„æ—¥å¿—åˆ†æè§£å†³æ–¹æ¡ˆï¼š

- **Elasticsearch**: åˆ†å¸ƒå¼æœç´¢å’Œåˆ†æå¼•æ“ï¼Œè´Ÿè´£å­˜å‚¨å’Œæ£€ç´¢æ—¥å¿—æ•°æ®
- **Logstash**: æ•°æ®æ”¶é›†å’Œå¤„ç†ç®¡é“ï¼Œè´Ÿè´£æ”¶é›†ã€è½¬æ¢ã€è¿‡æ»¤æ—¥å¿—
- **Kibana**: æ•°æ®å¯è§†åŒ–å¹³å°ï¼Œæä¾›æ—¥å¿—æŸ¥è¯¢å’Œå›¾è¡¨å±•ç¤º

**ç°ä»£ELKæ¶æ„**ï¼š
- **Beats**: è½»é‡çº§æ•°æ®é‡‡é›†å™¨ï¼ˆFilebeatã€Metricbeatç­‰ï¼‰
- **Elasticsearch**: æ ¸å¿ƒå­˜å‚¨å’Œæœç´¢å¼•æ“
- **Kibana**: å¯è§†åŒ–å’Œç®¡ç†ç•Œé¢

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

#### Elasticsearchæ ¸å¿ƒæ¦‚å¿µ

- **Indexï¼ˆç´¢å¼•ï¼‰**: ç±»ä¼¼æ•°æ®åº“çš„è¡¨ï¼Œå­˜å‚¨ç›¸åŒç±»å‹çš„æ–‡æ¡£
- **Documentï¼ˆæ–‡æ¡£ï¼‰**: åŸºæœ¬æ•°æ®å•å…ƒï¼Œä»¥JSONæ ¼å¼å­˜å‚¨
- **Fieldï¼ˆå­—æ®µï¼‰**: æ–‡æ¡£ä¸­çš„é”®å€¼å¯¹
- **Mappingï¼ˆæ˜ å°„ï¼‰**: å®šä¹‰æ–‡æ¡£å­—æ®µçš„ç±»å‹å’Œå±æ€§
- **Shardï¼ˆåˆ†ç‰‡ï¼‰**: ç´¢å¼•çš„æ°´å¹³åˆ†å‰²ï¼Œæé«˜å¹¶å‘æ€§èƒ½
- **Replicaï¼ˆå‰¯æœ¬ï¼‰**: åˆ†ç‰‡çš„å¤‡ä»½ï¼Œæé«˜å¯ç”¨æ€§

#### Logstashæ ¸å¿ƒæ¦‚å¿µ

- **Inputï¼ˆè¾“å…¥ï¼‰**: æ•°æ®æºï¼ˆæ–‡ä»¶ã€TCPã€Beatsç­‰ï¼‰
- **Filterï¼ˆè¿‡æ»¤å™¨ï¼‰**: æ•°æ®å¤„ç†å’Œè½¬æ¢ï¼ˆgrokã€mutateã€dateç­‰ï¼‰
- **Outputï¼ˆè¾“å‡ºï¼‰**: æ•°æ®ç›®çš„åœ°ï¼ˆElasticsearchã€æ–‡ä»¶ç­‰ï¼‰
- **Pipelineï¼ˆç®¡é“ï¼‰**: Input â†’ Filter â†’ Outputçš„å®Œæ•´æµç¨‹

#### Kibanaæ ¸å¿ƒæ¦‚å¿µ

- **Discover**: æ—¥å¿—æœç´¢å’Œæµè§ˆ
- **Visualize**: åˆ›å»ºå›¾è¡¨å’Œå¯è§†åŒ–
- **Dashboard**: ç»„åˆå¤šä¸ªå¯è§†åŒ–çš„ä»ªè¡¨ç›˜
- **Dev Tools**: ElasticsearchæŸ¥è¯¢å·¥å…·

### 1.3 åº”ç”¨åœºæ™¯
- é›†ä¸­å¼æ—¥å¿—ç®¡ç†å’Œåˆ†æ
- åº”ç”¨æ€§èƒ½ç›‘æ§ï¼ˆAPMï¼‰
- å®‰å…¨æ—¥å¿—å®¡è®¡å’Œåˆ†æ
- ä¸šåŠ¡æ•°æ®åˆ†æå’ŒæŠ¥è¡¨
- å®æ—¶ç›‘æ§å’Œå‘Šè­¦
- å…¨æ–‡æœç´¢å’Œæ•°æ®æŒ–æ˜

## ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ (é‡ç‚¹)

### 2.1 ELKæ¶æ„ ğŸ”¥

```
åº”ç”¨æœåŠ¡å™¨ â†’ Filebeat â†’ Logstash â†’ Elasticsearch â†’ Kibana
                â†“           â†“            â†“           â†“
              é‡‡é›†æ—¥å¿—    å¤„ç†è¿‡æ»¤      å­˜å‚¨ç´¢å¼•    å¯è§†åŒ–æŸ¥è¯¢
```

**æ•°æ®æµè½¬è¿‡ç¨‹**ï¼š
1. **Filebeat**: ç›‘æ§æ—¥å¿—æ–‡ä»¶ï¼Œå®æ—¶é‡‡é›†æ–°å¢æ—¥å¿—
2. **Logstash**: æ¥æ”¶æ—¥å¿—ï¼Œè¿›è¡Œè§£æã€è¿‡æ»¤ã€è½¬æ¢
3. **Elasticsearch**: å­˜å‚¨å¤„ç†åçš„æ—¥å¿—ï¼Œå»ºç«‹ç´¢å¼•
4. **Kibana**: æä¾›æŸ¥è¯¢ç•Œé¢å’Œå¯è§†åŒ–å±•ç¤º

### 2.2 Elasticsearchæ ¸å¿ƒç‰¹æ€§ ğŸ”¥

#### 1. åˆ†å¸ƒå¼æ¶æ„

- **æ°´å¹³æ‰©å±•**: é€šè¿‡æ·»åŠ èŠ‚ç‚¹æ‰©å±•å®¹é‡
- **è‡ªåŠ¨åˆ†ç‰‡**: æ•°æ®è‡ªåŠ¨åˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹
- **é«˜å¯ç”¨**: å‰¯æœ¬æœºåˆ¶ä¿è¯æ•°æ®ä¸ä¸¢å¤±

#### 2. è¿‘å®æ—¶æœç´¢

- **å€’æ’ç´¢å¼•**: å¿«é€Ÿå…¨æ–‡æ£€ç´¢
- **ç§’çº§å»¶è¿Ÿ**: æ•°æ®å†™å…¥å1ç§’å†…å¯æœç´¢
- **å¤æ‚æŸ¥è¯¢**: æ”¯æŒå¸ƒå°”æŸ¥è¯¢ã€èŒƒå›´æŸ¥è¯¢ã€èšåˆåˆ†æ

#### 3. RESTful API

```bash
# åˆ›å»ºç´¢å¼•
PUT /logs-2024.01.04

# å†™å…¥æ–‡æ¡£
POST /logs-2024.01.04/_doc
{
  "timestamp": "2024-01-04T10:00:00",
  "level": "ERROR",
  "message": "Database connection failed"
}

# æœç´¢æ–‡æ¡£
GET /logs-2024.01.04/_search
{
  "query": {
    "match": {
      "level": "ERROR"
    }
  }
}
```

### 2.3 Logstashé…ç½® (âš ï¸ éš¾ç‚¹)

Logstashä½¿ç”¨é…ç½®æ–‡ä»¶å®šä¹‰æ•°æ®å¤„ç†ç®¡é“ï¼š

#### Inputæ’ä»¶

```ruby
input {
  # ä»Filebeatæ¥æ”¶æ•°æ®
  beats {
    port => 5044
  }
  
  # ä»æ–‡ä»¶è¯»å–
  file {
    path => "/var/log/app/*.log"
    start_position => "beginning"
  }
  
  # ä»TCPæ¥æ”¶
  tcp {
    port => 5000
    codec => json
  }
}
```

#### Filteræ’ä»¶ï¼ˆæ ¸å¿ƒéš¾ç‚¹ï¼‰

```ruby
filter {
  # Grokè§£ææ—¥å¿—
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{GREEDYDATA:msg}"
    }
  }
  
  # æ—¥æœŸè§£æ
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss.SSS"]
    target => "@timestamp"
  }
  
  # å­—æ®µå¤„ç†
  mutate {
    remove_field => ["message"]
    add_field => {
      "env" => "production"
    }
  }
  
  # æ¡ä»¶åˆ¤æ–­
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }
}
```

#### Outputæ’ä»¶

```ruby
output {
  # è¾“å‡ºåˆ°Elasticsearch
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆè°ƒè¯•ç”¨ï¼‰
  stdout {
    codec => rubydebug
  }
}
```

### 2.4 KibanaæŸ¥è¯¢è¯­è¨€ï¼ˆKQLï¼‰

Kibana Query Languageç”¨äºåœ¨Kibanaä¸­æœç´¢æ—¥å¿—ï¼š

```
# ç²¾ç¡®åŒ¹é…
level: "ERROR"

# æ¨¡ç³ŠåŒ¹é…
message: *connection*

# èŒƒå›´æŸ¥è¯¢
response_time > 1000

# å¸ƒå°”æŸ¥è¯¢
level: "ERROR" and service: "user-service"

# æ—¶é—´èŒƒå›´
@timestamp >= "2024-01-04T00:00:00" and @timestamp < "2024-01-05T00:00:00"
```

### 2.5 ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆILMï¼‰

ILMè‡ªåŠ¨ç®¡ç†ç´¢å¼•çš„ç”Ÿå‘½å‘¨æœŸï¼Œä¼˜åŒ–å­˜å‚¨å’Œæ€§èƒ½ï¼š

**ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ**ï¼š
1. **Hot**: é¢‘ç¹å†™å…¥å’ŒæŸ¥è¯¢ï¼ˆSSDå­˜å‚¨ï¼‰
2. **Warm**: åªè¯»ï¼Œå¶å°”æŸ¥è¯¢ï¼ˆæ™®é€šç£ç›˜ï¼‰
3. **Cold**: å¾ˆå°‘æŸ¥è¯¢ï¼ˆä½æˆæœ¬å­˜å‚¨ï¼‰
4. **Delete**: è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®


## ğŸ’» å®æˆ˜åº”ç”¨

### 3.1 ç¯å¢ƒæ­å»º

#### Docker Composeå¿«é€Ÿéƒ¨ç½²

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - node.name=es-node-1
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "5000:5000/udp"
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - elk
    depends_on:
      - logstash

volumes:
  elasticsearch-data:
    driver: local

networks:
  elk:
    driver: bridge
```

#### Filebeaté…ç½®ï¼ˆfilebeat.ymlï¼‰

```yaml
filebeat.inputs:
  # ç›‘æ§åº”ç”¨æ—¥å¿—
  - type: log
    enabled: true
    paths:
      - /var/log/app/*.log
    fields:
      app: myapp
      env: production
    multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
    multiline.negate: true
    multiline.match: after

  # ç›‘æ§Dockerå®¹å™¨æ—¥å¿—
  - type: container
    enabled: true
    paths:
      - '/var/lib/docker/containers/*/*.log'

# è¾“å‡ºåˆ°Logstash
output.logstash:
  hosts: ["logstash:5044"]

# æ—¥å¿—çº§åˆ«
logging.level: info
```

### 3.2 å¿«é€Ÿå¼€å§‹

#### Logstashç®¡é“é…ç½®ï¼ˆlogstash.confï¼‰

```ruby
input {
  beats {
    port => 5044
  }
}

filter {
  # è§£æJSONæ—¥å¿—
  if [message] =~ /^\{/ {
    json {
      source => "message"
    }
  }
  
  # è§£æJavaæ—¥å¿—
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:msg}"
    }
  }
  
  # è§£ææ—¶é—´æˆ³
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss.SSS", "ISO8601"]
    target => "@timestamp"
  }
  
  # æ·»åŠ å­—æ®µ
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "logs"
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
  }
  
  # è°ƒè¯•è¾“å‡º
  stdout {
    codec => rubydebug
  }
}
```

### 3.3 è¿›é˜¶é…ç½®

#### Javaåº”ç”¨æ—¥å¿—æ”¶é›†å®Œæ•´æ–¹æ¡ˆ

**1. Logbacké…ç½®ï¼ˆè¾“å‡ºJSONæ ¼å¼ï¼‰**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/var/log/app/application.log</file>
        
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>/var/log/app/application.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>7</maxHistory>
        </rollingPolicy>
        
        <!-- JSONæ ¼å¼è¾“å‡º -->
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app":"myapp","env":"production"}</customFields>
        </encoder>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="FILE"/>
    </root>
</configuration>
```

**Mavenä¾èµ–**ï¼š

```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```


**2. Filebeaté…ç½®ï¼ˆé‡‡é›†æ—¥å¿—ï¼‰**

```yaml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/app/application*.log
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      app: myapp
      env: production

output.logstash:
  hosts: ["logstash:5044"]
```

**3. Logstashé…ç½®ï¼ˆå¤„ç†æ—¥å¿—ï¼‰**

```ruby
input {
  beats {
    port => 5044
  }
}

filter {
  # JSONå·²ç»è¢«Filebeatè§£æï¼Œç›´æ¥ä½¿ç”¨
  
  # æ·»åŠ åœ°ç†ä½ç½®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰IPå­—æ®µï¼‰
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # é”™è¯¯æ—¥å¿—æ·»åŠ æ ‡ç­¾
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error", "alert"]
    }
  }
  
  # æ…¢æŸ¥è¯¢æ ‡è®°
  if [response_time] and [response_time] > 1000 {
    mutate {
      add_tag => ["slow_query"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{[app]}-%{+YYYY.MM.dd}"
    template_name => "logs"
    template_pattern => "logs-*"
  }
}
```

#### Elasticsearchç´¢å¼•æ¨¡æ¿é…ç½®

```json
PUT _index_template/logs-template
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-policy",
      "index.lifecycle.rollover_alias": "logs"
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "level": {
          "type": "keyword"
        },
        "logger": {
          "type": "keyword"
        },
        "message": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "thread": {
          "type": "keyword"
        },
        "response_time": {
          "type": "long"
        }
      }
    }
  }
}
```

#### ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç­–ç•¥é…ç½®

```json
PUT _ilm/policy/logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          },
          "set_priority": {
            "priority": 50
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "set_priority": {
            "priority": 0
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

#### Kibanaå¯è§†åŒ–é…ç½®

**1. åˆ›å»ºç´¢å¼•æ¨¡å¼**

```
Management â†’ Stack Management â†’ Index Patterns â†’ Create index pattern
ç´¢å¼•æ¨¡å¼ï¼šlogs-*
æ—¶é—´å­—æ®µï¼š@timestamp
```

**2. å¸¸ç”¨æŸ¥è¯¢ç¤ºä¾‹**

```
# æŸ¥è¯¢ERRORæ—¥å¿—
level: "ERROR"

# æŸ¥è¯¢ç‰¹å®šåº”ç”¨çš„æ—¥å¿—
app: "user-service" and level: "ERROR"

# æŸ¥è¯¢æ…¢æŸ¥è¯¢
response_time > 1000

# æŸ¥è¯¢ç‰¹å®šæ—¶é—´èŒƒå›´
@timestamp >= "2024-01-04T00:00:00" and @timestamp < "2024-01-05T00:00:00"

# ç»„åˆæŸ¥è¯¢
level: "ERROR" and (app: "user-service" or app: "order-service")
```

**3. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨**

```
Visualize â†’ Create visualization

å¸¸ç”¨å›¾è¡¨ç±»å‹ï¼š
- Line Chart: æ—¥å¿—è¶‹åŠ¿å›¾
- Pie Chart: æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
- Data Table: æ—¥å¿—è¯¦æƒ…è¡¨
- Metric: é”™è¯¯æ•°é‡ç»Ÿè®¡
- Heat Map: æ—¶é—´çƒ­åŠ›å›¾
```

**4. åˆ›å»ºDashboard**

```
Dashboard â†’ Create dashboard â†’ Add panels

æ¨èé¢æ¿ï¼š
- æ—¥å¿—æ€»é‡è¶‹åŠ¿
- é”™è¯¯æ—¥å¿—æ•°é‡
- æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
- Top 10 é”™è¯¯æ¶ˆæ¯
- å“åº”æ—¶é—´åˆ†å¸ƒ
- åº”ç”¨æ—¥å¿—åˆ†å¸ƒ
```


## âœ¨ æœ€ä½³å®è·µ

### 4.1 æ€§èƒ½ä¼˜åŒ–

#### 1. Elasticsearchä¼˜åŒ–

```yaml
# elasticsearch.yml
# JVMå †å†…å­˜è®¾ç½®ï¼ˆä¸è¶…è¿‡ç‰©ç†å†…å­˜çš„50%ï¼Œä¸è¶…è¿‡32GBï¼‰
ES_JAVA_OPTS: "-Xms4g -Xmx4g"

# åˆ†ç‰‡æ•°é‡ä¼˜åŒ–
# å•ä¸ªåˆ†ç‰‡å¤§å°æ§åˆ¶åœ¨20-50GB
# åˆ†ç‰‡æ•° = æ•°æ®æ€»é‡ / å•åˆ†ç‰‡å¤§å°

# å‰¯æœ¬æ•°é‡
number_of_replicas: 1  # ç”Ÿäº§ç¯å¢ƒè‡³å°‘1ä¸ªå‰¯æœ¬

# åˆ·æ–°é—´éš”ï¼ˆæé«˜å†™å…¥æ€§èƒ½ï¼‰
index.refresh_interval: 30s  # é»˜è®¤1s

# æ‰¹é‡å†™å…¥
bulk:
  size: 5MB
  actions: 1000
```

#### 2. Logstashä¼˜åŒ–

```yaml
# logstash.yml
# ç®¡é“å·¥ä½œçº¿ç¨‹æ•°
pipeline.workers: 4  # CPUæ ¸å¿ƒæ•°

# æ‰¹é‡å¤§å°
pipeline.batch.size: 125

# æ‰¹é‡å»¶è¿Ÿ
pipeline.batch.delay: 50

# é˜Ÿåˆ—ç±»å‹ï¼ˆæŒä¹…åŒ–é˜Ÿåˆ—ï¼‰
queue.type: persisted
queue.max_bytes: 1GB
```

#### 3. Filebeatä¼˜åŒ–

```yaml
# filebeat.yml
# æ‰¹é‡å‘é€
output.logstash:
  hosts: ["logstash:5044"]
  bulk_max_size: 2048
  worker: 2

# æ—¥å¿—é‡‡é›†ä¼˜åŒ–
filebeat.inputs:
  - type: log
    paths:
      - /var/log/app/*.log
    # æ‰«æé¢‘ç‡
    scan_frequency: 10s
    # å…³é—­æ–‡ä»¶è¶…æ—¶
    close_inactive: 5m
```

### 4.2 é…ç½®è§„èŒƒ

#### 1. ç´¢å¼•å‘½åè§„èŒƒ

```
# æ¨èæ ¼å¼ï¼š{ç±»å‹}-{åº”ç”¨}-{æ—¥æœŸ}
logs-user-service-2024.01.04
logs-order-service-2024.01.04
metrics-system-2024.01.04

# ä½¿ç”¨åˆ«å
POST /_aliases
{
  "actions": [
    {
      "add": {
        "index": "logs-user-service-2024.01.04",
        "alias": "logs-user-service"
      }
    }
  ]
}
```

#### 2. å­—æ®µæ˜ å°„è§„èŒƒ

```json
{
  "mappings": {
    "properties": {
      "@timestamp": {"type": "date"},
      "level": {"type": "keyword"},
      "message": {
        "type": "text",
        "fields": {
          "keyword": {"type": "keyword", "ignore_above": 256}
        }
      },
      "app": {"type": "keyword"},
      "env": {"type": "keyword"},
      "host": {"type": "keyword"},
      "response_time": {"type": "long"},
      "user_id": {"type": "keyword"},
      "request_id": {"type": "keyword"}
    }
  }
}
```

#### 3. Grokæ¨¡å¼è§„èŒƒ

```ruby
# å®šä¹‰è‡ªå®šä¹‰Grokæ¨¡å¼
filter {
  grok {
    patterns_dir => ["/etc/logstash/patterns"]
    match => {
      "message" => "%{CUSTOM_PATTERN}"
    }
  }
}

# /etc/logstash/patterns/custom
CUSTOM_TIMESTAMP %{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}
CUSTOM_LOGLEVEL (TRACE|DEBUG|INFO|WARN|ERROR|FATAL)
CUSTOM_PATTERN %{CUSTOM_TIMESTAMP:timestamp} \[%{DATA:thread}\] %{CUSTOM_LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:msg}
```

### 4.3 å¸¸è§é™·é˜±

#### âš ï¸ é™·é˜±1ï¼šç´¢å¼•çˆ†ç‚¸

```bash
# âŒ é”™è¯¯ï¼šæ¯ä¸ªåº”ç”¨å®ä¾‹åˆ›å»ºç‹¬ç«‹ç´¢å¼•
logs-user-service-instance1-2024.01.04
logs-user-service-instance2-2024.01.04

# âœ… æ­£ç¡®ï¼šæŒ‰åº”ç”¨å’Œæ—¥æœŸåˆ›å»ºç´¢å¼•
logs-user-service-2024.01.04

# ä½¿ç”¨å­—æ®µåŒºåˆ†å®ä¾‹
{
  "app": "user-service",
  "instance": "instance1"
}
```

#### âš ï¸ é™·é˜±2ï¼šå­—æ®µæ˜ å°„å†²çª

```bash
# âŒ é”™è¯¯ï¼šåŒä¸€å­—æ®µä¸åŒç±»å‹
# ç´¢å¼•1ï¼šuser_id ä¸º long
# ç´¢å¼•2ï¼šuser_id ä¸º keyword

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç´¢å¼•æ¨¡æ¿ç»Ÿä¸€å­—æ®µç±»å‹
PUT _index_template/logs-template
{
  "index_patterns": ["logs-*"],
  "template": {
    "mappings": {
      "properties": {
        "user_id": {"type": "keyword"}
      }
    }
  }
}
```

#### âš ï¸ é™·é˜±3ï¼šLogstashé…ç½®é”™è¯¯å¯¼è‡´æ•°æ®ä¸¢å¤±

```ruby
# âŒ é”™è¯¯ï¼šGrokè§£æå¤±è´¥æ—¶ä¸¢å¼ƒæ•°æ®
filter {
  grok {
    match => {"message" => "%{PATTERN}"}
  }
}

# âœ… æ­£ç¡®ï¼šæ·»åŠ æ ‡ç­¾ï¼Œä¿ç•™åŸå§‹æ•°æ®
filter {
  grok {
    match => {"message" => "%{PATTERN}"}
    tag_on_failure => ["_grokparsefailure"]
  }
  
  # è§£æå¤±è´¥æ—¶ä¿ç•™åŸå§‹æ¶ˆæ¯
  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => {"parse_error" => "true"}
    }
  }
}
```

#### âš ï¸ é™·é˜±4ï¼šç£ç›˜ç©ºé—´ä¸è¶³

```bash
# ç›‘æ§ç£ç›˜ä½¿ç”¨ç‡
GET _cat/allocation?v

# è®¾ç½®ç£ç›˜æ°´ä½çº¿
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "85%",
    "cluster.routing.allocation.disk.watermark.high": "90%",
    "cluster.routing.allocation.disk.watermark.flood_stage": "95%"
  }
}

# é…ç½®ILMè‡ªåŠ¨åˆ é™¤æ—§æ•°æ®
PUT _ilm/policy/logs-policy
{
  "policy": {
    "phases": {
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

#### âš ï¸ é™·é˜±5ï¼šæŸ¥è¯¢æ€§èƒ½å·®

```bash
# âŒ é”™è¯¯ï¼šä½¿ç”¨é€šé…ç¬¦æŸ¥è¯¢
GET logs-*/_search
{
  "query": {
    "wildcard": {
      "message": "*error*"
    }
  }
}

# âœ… æ­£ç¡®ï¼šä½¿ç”¨matchæŸ¥è¯¢æˆ–keywordå­—æ®µ
GET logs-*/_search
{
  "query": {
    "match": {
      "message": "error"
    }
  }
}

# æˆ–è€…ä½¿ç”¨keywordå­—æ®µç²¾ç¡®åŒ¹é…
GET logs-*/_search
{
  "query": {
    "term": {
      "level": "ERROR"
    }
  }
}
```


## â“ å¸¸è§é—®é¢˜

### Q1: ELK Stackä¸ä¼ ç»Ÿæ—¥å¿—æ–¹æ¡ˆçš„åŒºåˆ«ï¼Ÿ

**A**: 

| å¯¹æ¯”é¡¹ | ä¼ ç»Ÿæ–¹æ¡ˆ | ELK Stack |
|--------|---------|-----------|
| æ—¥å¿—å­˜å‚¨ | åˆ†æ•£åœ¨å„æœåŠ¡å™¨ | é›†ä¸­å­˜å‚¨ |
| æ—¥å¿—æŸ¥è¯¢ | grep/awkå‘½ä»¤ | å¼ºå¤§çš„æœç´¢å¼•æ“ |
| å¯è§†åŒ– | æ—  | Kibanaå›¾è¡¨ |
| å®æ—¶æ€§ | éœ€è¦ç™»å½•æœåŠ¡å™¨ | å®æ—¶æŸ¥çœ‹ |
| æ‰©å±•æ€§ | éš¾ä»¥æ‰©å±• | æ°´å¹³æ‰©å±• |
| åˆ†æèƒ½åŠ› | æœ‰é™ | èšåˆåˆ†æ |

### Q2: å¦‚ä½•é€‰æ‹©Filebeatè¿˜æ˜¯Logstashé‡‡é›†æ—¥å¿—ï¼Ÿ

**A**: 

**Filebeatä¼˜åŠ¿**ï¼š
- è½»é‡çº§ï¼Œèµ„æºå ç”¨å°‘
- éƒ¨ç½²ç®€å•
- é€‚åˆå¤§è§„æ¨¡éƒ¨ç½²

**Logstashä¼˜åŠ¿**ï¼š
- åŠŸèƒ½å¼ºå¤§ï¼Œæ”¯æŒå¤æ‚å¤„ç†
- æ’ä»¶ä¸°å¯Œ
- é€‚åˆå¤æ‚æ—¥å¿—è§£æ

**æ¨èæ–¹æ¡ˆ**ï¼š
```
åº”ç”¨æœåŠ¡å™¨ â†’ Filebeat â†’ Logstash â†’ Elasticsearch
              (é‡‡é›†)    (å¤„ç†)      (å­˜å‚¨)
```

Filebeatè´Ÿè´£é‡‡é›†ï¼ŒLogstashè´Ÿè´£å¤„ç†ï¼Œå„å¸å…¶èŒã€‚

### Q3: å¦‚ä½•å¤„ç†å¤šè¡Œæ—¥å¿—ï¼ˆå¦‚Javaå¼‚å¸¸å †æ ˆï¼‰ï¼Ÿ

**A**: 

**Filebeaté…ç½®**ï¼š

```yaml
filebeat.inputs:
  - type: log
    paths:
      - /var/log/app/*.log
    # å¤šè¡Œé…ç½®
    multiline.type: pattern
    multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'
    multiline.negate: true
    multiline.match: after
    multiline.max_lines: 500
```

**Logstashé…ç½®**ï¼š

```ruby
filter {
  # å¦‚æœFilebeatæœªå¤„ç†å¤šè¡Œï¼Œåœ¨Logstashå¤„ç†
  multiline {
    pattern => "^%{TIMESTAMP_ISO8601}"
    negate => true
    what => "previous"
  }
}
```

### Q4: å¦‚ä½•å®ç°æ—¥å¿—å‘Šè­¦ï¼Ÿ

**A**: 

**æ–¹å¼1ï¼šä½¿ç”¨Kibana Alerting**

```
Stack Management â†’ Alerts and Insights â†’ Rules and Connectors

åˆ›å»ºè§„åˆ™ï¼š
- æ¡ä»¶ï¼šlevel: "ERROR" 
- é˜ˆå€¼ï¼š5åˆ†é’Ÿå†…è¶…è¿‡10æ¡
- åŠ¨ä½œï¼šå‘é€é‚®ä»¶/Webhook
```

**æ–¹å¼2ï¼šä½¿ç”¨ElastAlert**

```yaml
# elastalert_rule.yaml
name: Error Log Alert
type: frequency
index: logs-*
num_events: 10
timeframe:
  minutes: 5

filter:
  - term:
      level: "ERROR"

alert:
  - email
  - slack

email:
  - "ops@example.com"

slack:
  slack_webhook_url: "https://hooks.slack.com/services/xxx"
```

### Q5: å¦‚ä½•ä¼˜åŒ–ElasticsearchæŸ¥è¯¢æ€§èƒ½ï¼Ÿ

**A**: 

**1. ä½¿ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤**

```json
GET logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        {"match": {"message": "error"}}
      ],
      "filter": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-1h",
              "lte": "now"
            }
          }
        }
      ]
    }
  }
}
```

**2. ä½¿ç”¨keywordå­—æ®µ**

```json
# âŒ æ…¢ï¼šä½¿ç”¨textå­—æ®µ
{"term": {"message": "ERROR"}}

# âœ… å¿«ï¼šä½¿ç”¨keywordå­—æ®µ
{"term": {"level": "ERROR"}}
```

**3. é™åˆ¶è¿”å›å­—æ®µ**

```json
GET logs-*/_search
{
  "_source": ["@timestamp", "level", "message"],
  "query": {...}
}
```

**4. ä½¿ç”¨èšåˆä»£æ›¿æœç´¢**

```json
# ç»Ÿè®¡é”™è¯¯æ•°é‡
GET logs-*/_search
{
  "size": 0,
  "aggs": {
    "error_count": {
      "filter": {
        "term": {"level": "ERROR"}
      }
    }
  }
}
```

### Q6: å¦‚ä½•å¤‡ä»½å’Œæ¢å¤Elasticsearchæ•°æ®ï¼Ÿ

**A**: 

**1. é…ç½®å¿«ç…§ä»“åº“**

```json
PUT _snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/mount/backups/elasticsearch"
  }
}
```

**2. åˆ›å»ºå¿«ç…§**

```json
PUT _snapshot/my_backup/snapshot_1
{
  "indices": "logs-*",
  "ignore_unavailable": true,
  "include_global_state": false
}
```

**3. æ¢å¤å¿«ç…§**

```json
POST _snapshot/my_backup/snapshot_1/_restore
{
  "indices": "logs-2024.01.04",
  "ignore_unavailable": true
}
```

**4. è‡ªåŠ¨å¿«ç…§ç­–ç•¥**

```json
PUT _slm/policy/daily-snapshots
{
  "schedule": "0 30 1 * * ?",
  "name": "<daily-snap-{now/d}>",
  "repository": "my_backup",
  "config": {
    "indices": ["logs-*"],
    "ignore_unavailable": false,
    "include_global_state": false
  },
  "retention": {
    "expire_after": "30d",
    "min_count": 5,
    "max_count": 50
  }
}
```


## ğŸ”— ç›¸å…³èµ„æº

### å®˜æ–¹èµ„æº
- [Elasticå®˜æ–¹ç½‘ç«™](https://www.elastic.co/)
- [Elasticsearchæ–‡æ¡£](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstashæ–‡æ¡£](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Kibanaæ–‡æ¡£](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Filebeatæ–‡æ¡£](https://www.elastic.co/guide/en/beats/filebeat/current/index.html)

### æ¨èæ–‡ç« 
- [ELK Stackæ¶æ„è¯¦è§£](https://www.elastic.co/guide/en/elastic-stack/current/index.html)
- [Elasticsearchæ€§èƒ½ä¼˜åŒ–](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html)
- [Logstash Grokæ¨¡å¼](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html)
- [Kibanaå¯è§†åŒ–æŒ‡å—](https://www.elastic.co/guide/en/kibana/current/visualize.html)

### ç›¸å…³æŠ€æœ¯
- [SLF4Jå®Œæ•´æ•™ç¨‹](SLF4J-å®Œæ•´æ•™ç¨‹.md) - æ—¥å¿—é—¨é¢
- [Logbackå®Œæ•´æ•™ç¨‹](Logback-å®Œæ•´æ•™ç¨‹.md) - æ—¥å¿—æ¡†æ¶
- [Prometheuså®Œæ•´æ•™ç¨‹](../04-ç›‘æ§ä½“ç³»/Prometheus-å®Œæ•´æ•™ç¨‹.md) - ç›‘æ§ç³»ç»Ÿ

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ELK Stackçš„æ¶æ„å’Œå·¥ä½œåŸç†
- [ ] æŒæ¡Elasticsearchçš„åŸºæœ¬æ“ä½œå’ŒæŸ¥è¯¢
- [ ] æŒæ¡Logstashçš„é…ç½®å’Œæ—¥å¿—å¤„ç†
- [ ] æŒæ¡Kibanaçš„æŸ¥è¯¢å’Œå¯è§†åŒ–
- [ ] æŒæ¡Filebeatçš„æ—¥å¿—é‡‡é›†é…ç½®
- [ ] ç†è§£Grokæ¨¡å¼çš„ç¼–å†™æ–¹æ³•
- [ ] æŒæ¡ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†
- [ ] äº†è§£ELKæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] æŒæ¡æ—¥å¿—å‘Šè­¦çš„é…ç½®æ–¹æ³•
- [ ] äº†è§£æ•°æ®å¤‡ä»½å’Œæ¢å¤æ–¹æ¡ˆ
- [ ] èƒ½å¤Ÿåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œç»´æŠ¤ELK Stack

## ğŸ“Š ELKéƒ¨ç½²æ¶æ„

### å°å‹éƒ¨ç½²ï¼ˆå•æœºï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      å•å°æœåŠ¡å™¨ï¼ˆ8GB+ RAMï¼‰        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Elasticsearch (4GB)             â”‚
â”‚  Logstash (2GB)                  â”‚
â”‚  Kibana (1GB)                    â”‚
â”‚  Filebeat (è½»é‡çº§)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‚ç”¨åœºæ™¯ï¼š
- å¼€å‘/æµ‹è¯•ç¯å¢ƒ
- æ—¥å¿—é‡ < 10GB/å¤©
- å•åº”ç”¨æ—¥å¿—æ”¶é›†
```

### ä¸­å‹éƒ¨ç½²ï¼ˆé›†ç¾¤ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ES Master 1  â”‚  â”‚ ES Master 2  â”‚  â”‚ ES Master 3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚ ES Data 1    â”‚ ES Data 2    â”‚ ES Data 3    â”‚      â”‚
â”‚ (16GB RAM)   â”‚ (16GB RAM)   â”‚ (16GB RAM)   â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚ Logstash 1   â”‚ Logstash 2   â”‚                     â”‚
â”‚ (8GB RAM)    â”‚ (8GB RAM)    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚   Kibana     â”‚                                    â”‚
â”‚  (4GB RAM)   â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         åº”ç”¨æœåŠ¡å™¨ï¼ˆFilebeatï¼‰                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‚ç”¨åœºæ™¯ï¼š
- ç”Ÿäº§ç¯å¢ƒ
- æ—¥å¿—é‡ 10-100GB/å¤©
- å¤šåº”ç”¨æ—¥å¿—æ”¶é›†
- éœ€è¦é«˜å¯ç”¨
```

### å¤§å‹éƒ¨ç½²ï¼ˆåˆ†å±‚é›†ç¾¤ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è´Ÿè½½å‡è¡¡ï¼ˆNginx/HAProxyï¼‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Logstash 1   â”‚ â”‚ Logstash 2  â”‚ â”‚ Logstash 3  â”‚
â”‚ (16GB RAM)   â”‚ â”‚ (16GB RAM)  â”‚ â”‚ (16GB RAM)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Elasticsearch Cluster                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Master Nodes (3å°)                              â”‚
â”‚  Hot Data Nodes (5å°, SSD, 32GB RAM)            â”‚
â”‚  Warm Data Nodes (5å°, HDD, 32GB RAM)           â”‚
â”‚  Cold Data Nodes (3å°, HDD, 16GB RAM)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kibana Cluster (2å°)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‚ç”¨åœºæ™¯ï¼š
- å¤§å‹ç”Ÿäº§ç¯å¢ƒ
- æ—¥å¿—é‡ > 100GB/å¤©
- å¤šç§Ÿæˆ·ã€å¤šåº”ç”¨
- éœ€è¦é«˜å¯ç”¨å’Œé«˜æ€§èƒ½
```

## ğŸ“ è¿›é˜¶å­¦ä¹ è·¯å¾„

1. **åŸºç¡€é˜¶æ®µ**ï¼ˆ1å‘¨ï¼‰
   - ç†è§£ELKæ¶æ„
   - æŒæ¡Dockeréƒ¨ç½²
   - å­¦ä¹ åŸºæœ¬æŸ¥è¯¢

2. **è¿›é˜¶é˜¶æ®µ**ï¼ˆ2å‘¨ï¼‰
   - æŒæ¡Logstashé…ç½®
   - å­¦ä¹ Grokæ¨¡å¼
   - æŒæ¡Kibanaå¯è§†åŒ–

3. **é«˜çº§é˜¶æ®µ**ï¼ˆ1ä¸ªæœˆï¼‰
   - æ€§èƒ½ä¼˜åŒ–
   - é›†ç¾¤éƒ¨ç½²
   - ç›‘æ§å‘Šè­¦
   - æ•°æ®å¤‡ä»½

4. **å®æˆ˜é˜¶æ®µ**ï¼ˆæŒç»­ï¼‰
   - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
   - é—®é¢˜æ’æŸ¥
   - å®¹é‡è§„åˆ’
   - å®‰å…¨åŠ å›º

## ğŸ”§ å¸¸ç”¨è¿ç»´å‘½ä»¤

### Elasticsearch

```bash
# æŸ¥çœ‹é›†ç¾¤å¥åº·çŠ¶æ€
GET _cluster/health

# æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯
GET _cat/nodes?v

# æŸ¥çœ‹ç´¢å¼•åˆ—è¡¨
GET _cat/indices?v

# æŸ¥çœ‹åˆ†ç‰‡åˆ†é…
GET _cat/shards?v

# åˆ é™¤ç´¢å¼•
DELETE logs-2024.01.01

# å¼ºåˆ¶åˆå¹¶æ®µ
POST logs-*/_forcemerge?max_num_segments=1

# æ¸…ç†ç¼“å­˜
POST logs-*/_cache/clear
```

### Logstash

```bash
# æµ‹è¯•é…ç½®æ–‡ä»¶
bin/logstash -f config/logstash.conf --config.test_and_exit

# å¯åŠ¨Logstash
bin/logstash -f config/logstash.conf

# æŸ¥çœ‹ç®¡é“çŠ¶æ€
curl -XGET 'localhost:9600/_node/stats/pipelines?pretty'
```

### Filebeat

```bash
# æµ‹è¯•é…ç½®
filebeat test config

# æµ‹è¯•è¾“å‡º
filebeat test output

# å¯åŠ¨Filebeat
filebeat -e -c filebeat.yml
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2024-01-04  
**ç»´æŠ¤è€…**: @author erik.zhou
