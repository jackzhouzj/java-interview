# vLLM - å®Œæ•´æ•™ç¨‹

> @author erik.zhou

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

### ç‰ˆæœ¬ä¿¡æ¯
- **vLLMç‰ˆæœ¬**ï¼š0.2+
- **æœ€æ–°ç¨³å®šç‰ˆ**ï¼š0.2.x
- **æ¨èç‰ˆæœ¬**ï¼šæœ€æ–°ç‰ˆ

### å­¦ä¹ éš¾åº¦
- **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­ (è¾ƒéš¾)
- **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š15-20å°æ—¶
- **é‡è¦ç¨‹åº¦**ï¼šâ­â­â­â­â­ (å¿…å­¦)

### å‰ç½®çŸ¥è¯†
- PythonåŸºç¡€
- PyTorchåŸºç¡€
- TransformeråŸç†
- GPUç¼–ç¨‹åŸºç¡€
- LLMåŸºç¡€çŸ¥è¯†

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£vLLMçš„æ ¸å¿ƒåŸç†
- [ ] æŒæ¡PagedAttentionæœºåˆ¶
- [ ] èƒ½å¤Ÿéƒ¨ç½²å’Œé…ç½®vLLMæœåŠ¡
- [ ] ç†è§£è¿ç»­æ‰¹å¤„ç†æŠ€æœ¯
- [ ] æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] èƒ½å¤Ÿç›‘æ§å’Œè°ƒè¯•vLLM
- [ ] äº†è§£ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

## ğŸ“– ç›®å½•

1. [vLLMç®€ä»‹](#1-vllmç®€ä»‹)
2. [ç¯å¢ƒæ­å»º](#2-ç¯å¢ƒæ­å»º)
3. [æ ¸å¿ƒåŸç†](#3-æ ¸å¿ƒåŸç†)
4. [åŸºç¡€ä½¿ç”¨](#4-åŸºç¡€ä½¿ç”¨)
5. [APIæœåŠ¡](#5-apiæœåŠ¡)
6. [æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
7. [é«˜çº§ç‰¹æ€§](#7-é«˜çº§ç‰¹æ€§)
8. [ç”Ÿäº§éƒ¨ç½²](#8-ç”Ÿäº§éƒ¨ç½²)
9. [ç›‘æ§å’Œè°ƒè¯•](#9-ç›‘æ§å’Œè°ƒè¯•)
10. [æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)

---

## 1. vLLMç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯vLLM

vLLMæ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“ç”¨çš„LLMæ¨ç†å’ŒæœåŠ¡åº“ï¼Œä¸“æ³¨äºé«˜ååé‡å’Œå†…å­˜æ•ˆç‡ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ”¥ **PagedAttention**ï¼šé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
- ğŸ”¥ **è¿ç»­æ‰¹å¤„ç†**ï¼šåŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
- ğŸ”¥ **é«˜ååé‡**ï¼šæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«24å€
- ğŸ”¥ **å†…å­˜ä¼˜åŒ–**ï¼šå‡å°‘å†…å­˜æµªè´¹
- ğŸ”¥ **æ˜“äºä½¿ç”¨**ï¼šå…¼å®¹HuggingFaceæ¨¡å‹

### 1.2 vLLM vs å…¶ä»–æ¨ç†å¼•æ“

| ç‰¹æ€§ | vLLM | TGI | llama.cpp |
|------|------|-----|-----------|
| ååé‡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| å†…å­˜æ•ˆç‡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| GPUæ”¯æŒ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| CPUæ”¯æŒ | â­â­ | â­â­â­ | â­â­â­â­â­ |
| æ˜“ç”¨æ€§ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| æ¨¡å‹æ”¯æŒ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |

### 1.3 åº”ç”¨åœºæ™¯

- **é«˜å¹¶å‘æœåŠ¡**ï¼šå¤„ç†å¤§é‡å¹¶å‘è¯·æ±‚
- **æ‰¹é‡æ¨ç†**ï¼šæ‰¹é‡å¤„ç†å¤§é‡æ–‡æœ¬
- **å®æ—¶åº”ç”¨**ï¼šä½å»¶è¿Ÿè¦æ±‚çš„åº”ç”¨
- **èµ„æºå—é™**ï¼šGPUå†…å­˜æœ‰é™çš„åœºæ™¯
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šä¼ä¸šçº§LLMæœåŠ¡

---

## 2. ç¯å¢ƒæ­å»º

### 2.1 ç³»ç»Ÿè¦æ±‚

```bash
# GPUè¦æ±‚
- NVIDIA GPU (è®¡ç®—èƒ½åŠ› >= 7.0)
- CUDA 11.8 æˆ–æ›´é«˜
- è‡³å°‘ 16GB GPUå†…å­˜ï¼ˆæ¨è 24GB+ï¼‰

# ç³»ç»Ÿè¦æ±‚
- Linux (æ¨è Ubuntu 20.04+)
- Python 3.8+
```

### 2.2 å®‰è£…vLLM

```bash
# ğŸ”¥ ä½¿ç”¨pipå®‰è£…
pip install vllm

# ä»æºç å®‰è£…ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

### 2.3 å®‰è£…ä¾èµ–

```bash
# å®‰è£…CUDAå·¥å…·åŒ…
# è®¿é—® https://developer.nvidia.com/cuda-downloads

# å®‰è£…PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# éªŒè¯GPU
python -c "import torch; print(torch.cuda.is_available())"
```

---

## 3. æ ¸å¿ƒåŸç†

### 3.1 PagedAttention

PagedAttentionæ˜¯vLLMçš„æ ¸å¿ƒåˆ›æ–°ï¼Œå€Ÿé‰´äº†æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæŠ€æœ¯ã€‚

**ä¼ ç»ŸAttentionçš„é—®é¢˜**ï¼š
```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šé¢„åˆ†é…å›ºå®šå¤§å°çš„KVç¼“å­˜
# é—®é¢˜ï¼š
# 1. å†…å­˜æµªè´¹ï¼ˆå®é™…é•¿åº¦ < æœ€å¤§é•¿åº¦ï¼‰
# 2. å†…å­˜ç¢ç‰‡
# 3. æ— æ³•åŠ¨æ€è°ƒæ•´

max_length = 2048
kv_cache = torch.zeros(batch_size, max_length, hidden_size)
# å¤§é‡å†…å­˜è¢«æµªè´¹
```

**PagedAttentionçš„è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# PagedAttentionï¼šæŒ‰éœ€åˆ†é…å†…å­˜å—
# ä¼˜åŠ¿ï¼š
# 1. å‡å°‘å†…å­˜æµªè´¹ï¼ˆæ¥è¿‘é›¶æµªè´¹ï¼‰
# 2. æ”¯æŒæ›´å¤§çš„æ‰¹å¤„ç†
# 3. åŠ¨æ€å†…å­˜ç®¡ç†

# å†…å­˜è¢«åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼ˆpagesï¼‰
# åªåœ¨éœ€è¦æ—¶åˆ†é…æ–°å—
# ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜
```

### 3.2 è¿ç»­æ‰¹å¤„ç†

```python
# ğŸ”¥ è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
# ä¼ ç»Ÿæ‰¹å¤„ç†ï¼šç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
# é—®é¢˜ï¼šæ…¢è¯·æ±‚æ‹–ç´¯å¿«è¯·æ±‚

# vLLMè¿ç»­æ‰¹å¤„ç†ï¼š
# 1. è¯·æ±‚å®Œæˆåç«‹å³ç§»é™¤
# 2. æ–°è¯·æ±‚ç«‹å³åŠ å…¥æ‰¹æ¬¡
# 3. æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡

# ç¤ºä¾‹ï¼š
# æ‰¹æ¬¡1: [req1, req2, req3, req4]
# req2å®Œæˆ -> ç«‹å³ç§»é™¤
# æ‰¹æ¬¡2: [req1, req3, req4, req5]  # req5åŠ å…¥
```

### 3.3 å†…å­˜ä¼˜åŒ–

```python
# å†…å­˜ä¼˜åŒ–æŠ€æœ¯
# 1. KVç¼“å­˜å…±äº«ï¼ˆç”¨äºbeam searchï¼‰
# 2. å†…å­˜æ± ç®¡ç†
# 3. é¢„å–å’Œç¼“å­˜
# 4. é›¶æ‹·è´æŠ€æœ¯
```

---

## 4. åŸºç¡€ä½¿ç”¨

### 4.1 ç¦»çº¿æ¨ç†

```python
from vllm import LLM, SamplingParams

# ğŸ”¥ åˆ›å»ºLLMå®ä¾‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,  # GPUæ•°é‡
    dtype="float16"
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=256
)

# å•ä¸ªprompt
prompts = ["Hello, my name is"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt}")
    print(f"Generated: {generated_text}")
```

### 4.2 æ‰¹é‡æ¨ç†

```python
# ğŸ”¥ æ‰¹é‡å¤„ç†å¤šä¸ªprompt
prompts = [
    "The future of AI is",
    "Machine learning is",
    "Deep learning enables",
    "Natural language processing helps"
]

# æ‰¹é‡ç”Ÿæˆ
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
    print("-" * 50)
```

### 4.3 æµå¼ç”Ÿæˆ

```python
# æµå¼è¾“å‡º
from vllm import LLM, SamplingParams

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
sampling_params = SamplingParams(
    temperature=0.8,
    max_tokens=256
)

# ä½¿ç”¨streamå‚æ•°
prompts = ["Tell me a story about"]

for output in llm.generate(prompts, sampling_params, use_tqdm=True):
    print(output.outputs[0].text, end="", flush=True)
```

---

## 5. APIæœåŠ¡

### 5.1 å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡

```bash
# ğŸ”¥ å¯åŠ¨vLLMæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1

# é«˜çº§é…ç½®
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --dtype float16 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9
```

### 5.2 ä½¿ç”¨OpenAIå®¢æˆ·ç«¯

```python
from openai import OpenAI

# ğŸ”¥ è¿æ¥vLLMæœåŠ¡
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"  # vLLMä¸éœ€è¦APIå¯†é’¥
)

# Chat Completions
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is machine learning?"}
    ],
    temperature=0.7,
    max_tokens=256
)

print(response.choices[0].message.content)
```

### 5.3 æµå¼å“åº”

```python
# æµå¼Chat Completions
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "Tell me a story"}
    ],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 GPUé…ç½®

```python
from vllm import LLM

# ğŸ”¥ ä¼˜åŒ–GPUä½¿ç”¨
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2,  # ä½¿ç”¨2ä¸ªGPU
    gpu_memory_utilization=0.9,  # ä½¿ç”¨90%çš„GPUå†…å­˜
    dtype="float16",  # ä½¿ç”¨åŠç²¾åº¦
    max_model_len=4096,  # æœ€å¤§åºåˆ—é•¿åº¦
    swap_space=4,  # CPUäº¤æ¢ç©ºé—´ï¼ˆGBï¼‰
)
```

### 6.2 æ‰¹å¤„ç†ä¼˜åŒ–

```python
# ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    max_num_batched_tokens=8192,  # æ‰¹æ¬¡æœ€å¤§tokenæ•°
    max_num_seqs=256,  # æ‰¹æ¬¡æœ€å¤§åºåˆ—æ•°
)
```

### 6.3 é‡‡æ ·å‚æ•°ä¼˜åŒ–

```python
from vllm import SamplingParams

# ğŸ”¥ ä¼˜åŒ–é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    top_k=50,
    max_tokens=256,
    presence_penalty=0.0,
    frequency_penalty=0.0,
    repetition_penalty=1.0,
    length_penalty=1.0,
    stop=["</s>", "\n\n"],  # åœæ­¢æ ‡è®°
    n=1,  # ç”Ÿæˆæ•°é‡
    best_of=1,  # ä»nä¸ªä¸­é€‰æœ€å¥½çš„
    use_beam_search=False  # æ˜¯å¦ä½¿ç”¨beam search
)
```

---

## 7. é«˜çº§ç‰¹æ€§

### 7.1 å¤šGPUæ¨ç†

```python
# ğŸ”¥ å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    tensor_parallel_size=4,  # ä½¿ç”¨4ä¸ªGPU
    dtype="float16"
)

# æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    pipeline_parallel_size=2,  # æµæ°´çº¿å¹¶è¡Œ
    tensor_parallel_size=2,  # å¼ é‡å¹¶è¡Œ
)
```

### 7.2 é‡åŒ–æ”¯æŒ

```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model TheBloke/Llama-2-7B-Chat-GPTQ \
    --quantization gptq \
    --dtype float16
```

### 7.3 è‡ªå®šä¹‰æ¨¡å‹

```python
from vllm import LLM

# åŠ è½½æœ¬åœ°æ¨¡å‹
llm = LLM(
    model="/path/to/local/model",
    tokenizer="/path/to/tokenizer",
    trust_remote_code=True
)
```

---

## 8. ç”Ÿäº§éƒ¨ç½²

### 8.1 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

# å®‰è£…Pythonå’Œä¾èµ–
RUN apt-get update && apt-get install -y python3-pip
RUN pip3 install vllm

# ä¸‹è½½æ¨¡å‹
RUN python3 -c "from huggingface_hub import snapshot_download; \
    snapshot_download('meta-llama/Llama-2-7b-chat-hf')"

# å¯åŠ¨æœåŠ¡
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "meta-llama/Llama-2-7b-chat-hf", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t vllm-server .
docker run --gpus all -p 8000:8000 vllm-server
```

### 8.2 Kuberneteséƒ¨ç½²

```yaml
# vllm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        image: vllm-server:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  selector:
    app: vllm
  ports:
  - port: 8000
    targetPort: 8000
  type: LoadBalancer
```

### 8.3 è´Ÿè½½å‡è¡¡

```python
# ä½¿ç”¨Nginxè¿›è¡Œè´Ÿè½½å‡è¡¡
# nginx.conf
upstream vllm_backend {
    least_conn;
    server vllm-1:8000;
    server vllm-2:8000;
    server vllm-3:8000;
}

server {
    listen 80;
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## 9. ç›‘æ§å’Œè°ƒè¯•

### 9.1 æ€§èƒ½ç›‘æ§

```python
# è·å–ç»Ÿè®¡ä¿¡æ¯
from vllm import LLM

llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

# ç”Ÿæˆå¹¶è·å–ç»Ÿè®¡
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    # æŸ¥çœ‹ç”Ÿæˆç»Ÿè®¡
    print(f"Tokens generated: {len(output.outputs[0].token_ids)}")
    print(f"Finish reason: {output.outputs[0].finish_reason}")
```

### 9.2 æ—¥å¿—é…ç½®

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# vLLMæ—¥å¿—
logger = logging.getLogger("vllm")
logger.setLevel(logging.DEBUG)
```

### 9.3 æ€§èƒ½åˆ†æ

```bash
# ä½¿ç”¨nvidia-smiç›‘æ§GPU
watch -n 1 nvidia-smi

# ä½¿ç”¨nvtop
nvtop

# ä½¿ç”¨Prometheusç›‘æ§
# å¯åŠ¨æ—¶æ·»åŠ metricsç«¯ç‚¹
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --enable-metrics
```

---

## 10. æœ€ä½³å®è·µ

### 10.1 å†…å­˜ç®¡ç†

```python
# âœ… åˆç†è®¾ç½®GPUå†…å­˜ä½¿ç”¨ç‡
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    gpu_memory_utilization=0.9  # ç•™10%ç»™ç³»ç»Ÿ
)

# âœ… è®¾ç½®åˆé€‚çš„æœ€å¤§é•¿åº¦
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    max_model_len=2048  # æ ¹æ®å®é™…éœ€æ±‚è®¾ç½®
)
```

### 10.2 æ‰¹å¤„ç†ç­–ç•¥

```python
# âœ… æ‰¹é‡å¤„ç†è¯·æ±‚
prompts = [...]  # æ”¶é›†å¤šä¸ªè¯·æ±‚
outputs = llm.generate(prompts, sampling_params)

# âŒ é¿å…å•ä¸ªè¯·æ±‚å¾ªç¯
for prompt in prompts:
    output = llm.generate([prompt], sampling_params)  # ä½æ•ˆ
```

### 10.3 é”™è¯¯å¤„ç†

```python
from vllm import LLM
from vllm.utils import is_hip

try:
    llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")
    outputs = llm.generate(prompts, sampling_params)
except RuntimeError as e:
    if "out of memory" in str(e):
        print("GPUå†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°‘batch sizeæˆ–max_model_len")
    else:
        raise
```

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£vLLMçš„æ ¸å¿ƒåŸç†
- [ ] æŒæ¡PagedAttentionæœºåˆ¶
- [ ] èƒ½å¤Ÿéƒ¨ç½²vLLMæœåŠ¡
- [ ] æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] èƒ½å¤Ÿè¿›è¡Œç”Ÿäº§éƒ¨ç½²
- [ ] äº†è§£ç›‘æ§å’Œè°ƒè¯•æ–¹æ³•
- [ ] æŒæ¡æœ€ä½³å®è·µ

---

## ğŸ”— ç›¸å…³èµ„æº

- [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [vLLMè®ºæ–‡](https://arxiv.org/abs/2309.06180)
- [æ€§èƒ½åŸºå‡†æµ‹è¯•](https://github.com/vllm-project/vllm/tree/main/benchmarks)

---

@author erik.zhou
