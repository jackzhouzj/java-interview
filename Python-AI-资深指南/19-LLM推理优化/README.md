# LLM推理优化

## 📋 模块概述

本模块涵盖LLM推理优化的核心技术，包括vLLM、Text Generation Inference、llama.cpp等工具，以及推理加速和批处理优化技术。这是生产环境部署LLM的必备知识。

## 📚 学习内容

### 1. vLLM
- PagedAttention机制
- 连续批处理
- 高吞吐量推理
- GPU优化

### 2. Text Generation Inference
- Hugging Face推理服务
- 模型优化
- 流式生成
- 分布式推理

### 3. llama.cpp
- CPU推理优化
- 量化技术
- 本地部署
- 跨平台支持

### 4. 推理加速技术
- 模型量化
- 知识蒸馏
- 剪枝技术
- 算子融合

### 5. 批处理优化
- 动态批处理
- 请求调度
- 内存管理
- 性能监控

## 🎯 学习目标

- [ ] 理解LLM推理优化的核心原理
- [ ] 掌握vLLM的使用和配置
- [ ] 能够使用TGI部署模型
- [ ] 了解llama.cpp的优化技术
- [ ] 掌握推理加速和批处理优化

## 📖 子模块

1. [vLLM完整教程](vLLM-完整教程.md) 🔥
2. [Text Generation Inference完整教程](Text-Generation-Inference-完整教程.md)
3. [llama.cpp完整教程](llama.cpp-完整教程.md)
4. [推理加速技术完整教程](推理加速技术-完整教程.md)
5. [批处理优化完整教程](批处理优化-完整教程.md)

## ⏱️ 预计学习时长

- vLLM：15-20小时
- Text Generation Inference：15-20小时
- llama.cpp：10-15小时
- 推理加速技术：15-20小时
- 批处理优化：10-15小时
- 总计：65-90小时

## 📝 前置知识

- Python基础
- PyTorch基础
- Transformer原理
- GPU编程基础
- 系统优化知识

## 🔗 相关资源

- [vLLM官方文档](https://docs.vllm.ai/)
- [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [模型优化论文集](https://github.com/horseee/Awesome-Efficient-LLM)

---

**@author erik.zhou**
