# LangGraph - å®Œæ•´æ•™ç¨‹

> @author erik.zhou

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

### ç‰ˆæœ¬ä¿¡æ¯
- **LangGraphç‰ˆæœ¬**ï¼š1.0+
- **æœ€æ–°ç¨³å®šç‰ˆ**ï¼š1.0.x
- **æ¨èç‰ˆæœ¬**ï¼š1.0.0+ï¼ˆ2024-2025æœ€æ–°ç‰ˆæœ¬ï¼‰

### å­¦ä¹ éš¾åº¦
- **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­ (è¾ƒéš¾)
- **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š15-20å°æ—¶
- **é‡è¦ç¨‹åº¦**ï¼šâ­â­â­â­â­ (å¿…å­¦)

### å‰ç½®çŸ¥è¯†
- LangChain 0.3+åŸºç¡€
- Python 3.9+å¼‚æ­¥ç¼–ç¨‹
- çŠ¶æ€æœºæ¦‚å¿µ
- Agentå¼€å‘ç»éªŒ
- TypedDictå’Œç±»å‹æ³¨è§£

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£LangGraphçš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] æŒæ¡çŠ¶æ€å›¾çš„æ„å»ºæ–¹æ³•
- [ ] èƒ½å¤Ÿå®ç°å¾ªç¯å’Œæ¡ä»¶æµç¨‹
- [ ] æŒæ¡å¤šAgentåä½œå¼€å‘
- [ ] ç†è§£äººæœºäº¤äº’æµç¨‹è®¾è®¡
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„å·¥ä½œæµ
- [ ] æŒæ¡çŠ¶æ€æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹

## ğŸ“– ç›®å½•

1. [LangGraphç®€ä»‹](#1-langgraphç®€ä»‹)
2. [ç¯å¢ƒæ­å»º](#2-ç¯å¢ƒæ­å»º)
3. [æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
4. [çŠ¶æ€å›¾åŸºç¡€](#4-çŠ¶æ€å›¾åŸºç¡€)
5. [èŠ‚ç‚¹å’Œè¾¹](#5-èŠ‚ç‚¹å’Œè¾¹)
6. [æ¡ä»¶è·¯ç”±](#6-æ¡ä»¶è·¯ç”±)
7. [å¾ªç¯å’Œè¿­ä»£](#7-å¾ªç¯å’Œè¿­ä»£)
8. [å¤šAgentåä½œ](#8-å¤šagentåä½œ)
9. [äººæœºäº¤äº’](#9-äººæœºäº¤äº’)
10. [æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹](#10-æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹)
11. [å®æˆ˜æ¡ˆä¾‹](#11-å®æˆ˜æ¡ˆä¾‹)

---

## 1. LangGraphç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯LangGraph

LangGraphæ˜¯LangChainç”Ÿæ€ç³»ç»Ÿä¸­ç”¨äºæ„å»ºæœ‰çŠ¶æ€ã€å¤šå‚ä¸è€…åº”ç”¨çš„åº“ã€‚å®ƒæ‰©å±•äº†LangChainçš„èƒ½åŠ›ï¼Œä½¿å¾—å¯ä»¥åˆ›å»ºå…·æœ‰å¾ªç¯å’Œæ¡ä»¶çš„å¤æ‚å·¥ä½œæµã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ”¥ **çŠ¶æ€ç®¡ç†**ï¼šå†…ç½®çŠ¶æ€ç®¡ç†æœºåˆ¶
- ğŸ”¥ **å¾ªç¯æ”¯æŒ**ï¼šæ”¯æŒå¾ªç¯å’Œè¿­ä»£æµç¨‹
- ğŸ”¥ **æ¡ä»¶åˆ†æ”¯**ï¼šåŸºäºçŠ¶æ€çš„æ¡ä»¶è·¯ç”±
- ğŸ”¥ **å¤šAgent**ï¼šå¤šä¸ªAgentåä½œ
- ğŸ”¥ **äººæœºäº¤äº’**ï¼šæ”¯æŒäººå·¥ä»‹å…¥
- ğŸ”¥ **æŒä¹…åŒ–**ï¼šçŠ¶æ€æŒä¹…åŒ–å’Œæ¢å¤

### 1.2 LangGraph vs LangChain

| ç‰¹æ€§ | LangChain | LangGraph |
|------|-----------|-----------|
| æµç¨‹ç±»å‹ | çº¿æ€§/é¡ºåº | å›¾çŠ¶/å¾ªç¯ |
| çŠ¶æ€ç®¡ç† | æœ‰é™ | å¼ºå¤§ |
| æ¡ä»¶åˆ†æ”¯ | åŸºç¡€ | é«˜çº§ |
| å¾ªç¯æ”¯æŒ | ä¸æ”¯æŒ | æ”¯æŒ |
| å¤šAgent | åŸºç¡€ | é«˜çº§ |
| äººæœºäº¤äº’ | æœ‰é™ | åŸç”Ÿæ”¯æŒ |

### 1.3 åº”ç”¨åœºæ™¯

- **å¤æ‚å·¥ä½œæµ**ï¼šéœ€è¦å¾ªç¯å’Œæ¡ä»¶çš„æµç¨‹
- **å¤šAgentç³»ç»Ÿ**ï¼šå¤šä¸ªAgentåä½œå®Œæˆä»»åŠ¡
- **äººæœºåä½œ**ï¼šéœ€è¦äººå·¥å®¡æ ¸æˆ–ä»‹å…¥
- **çŠ¶æ€æœºåº”ç”¨**ï¼šåŸºäºçŠ¶æ€çš„å†³ç­–ç³»ç»Ÿ
- **æ¸¸æˆAI**ï¼šå¤æ‚çš„æ¸¸æˆé€»è¾‘
- **è‡ªåŠ¨åŒ–æµç¨‹**ï¼šä¼ä¸šçº§å·¥ä½œæµè‡ªåŠ¨åŒ–

---

## 2. ç¯å¢ƒæ­å»º

### 2.1 å®‰è£…LangGraph

```bash
# ğŸ”¥ å®‰è£…LangGraphï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
pip install langgraph

# å®‰è£…ç›¸å…³ä¾èµ–
pip install langchain-core langchain-openai

# å¯é€‰ï¼šå®‰è£…æ£€æŸ¥ç‚¹å­˜å‚¨
pip install langgraph-checkpoint-sqlite  # SQLiteå­˜å‚¨
pip install langgraph-checkpoint-postgres  # PostgreSQLå­˜å‚¨
```

### 2.2 åŸºç¡€é…ç½®

```python
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = "your-api-key"
```

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1 LangGraphæ¶æ„

```
LangGraphæ ¸å¿ƒç»„ä»¶
â”œâ”€â”€ StateGraph (çŠ¶æ€å›¾)
â”‚   â”œâ”€â”€ State (çŠ¶æ€)
â”‚   â”œâ”€â”€ Nodes (èŠ‚ç‚¹)
â”‚   â””â”€â”€ Edges (è¾¹)
â”œâ”€â”€ Conditional Edges (æ¡ä»¶è¾¹)
â”œâ”€â”€ Checkpointer (æ£€æŸ¥ç‚¹)
â””â”€â”€ Human-in-the-Loop (äººæœºäº¤äº’)
```

### 3.2 æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

#### Stateï¼ˆçŠ¶æ€ï¼‰
- å›¾ä¸­å…±äº«çš„æ•°æ®ç»“æ„
- åœ¨èŠ‚ç‚¹é—´ä¼ é€’
- å¯ä»¥è¢«èŠ‚ç‚¹ä¿®æ”¹

#### Nodeï¼ˆèŠ‚ç‚¹ï¼‰
- æ‰§è¡Œå…·ä½“æ“ä½œçš„å‡½æ•°
- æ¥æ”¶çŠ¶æ€ä½œä¸ºè¾“å…¥
- è¿”å›çŠ¶æ€æ›´æ–°

#### Edgeï¼ˆè¾¹ï¼‰
- è¿æ¥èŠ‚ç‚¹çš„è·¯å¾„
- å®šä¹‰æ‰§è¡Œé¡ºåº
- å¯ä»¥æ˜¯æ¡ä»¶çš„

#### Conditional Edgeï¼ˆæ¡ä»¶è¾¹ï¼‰
- åŸºäºçŠ¶æ€çš„åŠ¨æ€è·¯ç”±
- å®ç°æ¡ä»¶åˆ†æ”¯
- æ”¯æŒå¤æ‚å†³ç­–é€»è¾‘

---

## 4. çŠ¶æ€å›¾åŸºç¡€

### 4.1 åˆ›å»ºç®€å•çŠ¶æ€å›¾

```python
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, START, END
import operator

# ğŸ”¥ å®šä¹‰çŠ¶æ€ï¼ˆä½¿ç”¨TypedDictï¼‰
class State(TypedDict):
    messages: Annotated[list[str], operator.add]  # ä½¿ç”¨operator.addè‡ªåŠ¨åˆå¹¶åˆ—è¡¨
    count: int

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def node_1(state: State) -> dict:
    """ç¬¬ä¸€ä¸ªèŠ‚ç‚¹"""
    print("æ‰§è¡ŒèŠ‚ç‚¹1")
    return {
        "messages": ["èŠ‚ç‚¹1æ‰§è¡Œ"],
        "count": state["count"] + 1
    }

def node_2(state: State) -> dict:
    """ç¬¬äºŒä¸ªèŠ‚ç‚¹"""
    print("æ‰§è¡ŒèŠ‚ç‚¹2")
    return {
        "messages": ["èŠ‚ç‚¹2æ‰§è¡Œ"],
        "count": state["count"] + 1
    }

# ğŸ”¥ åˆ›å»ºçŠ¶æ€å›¾ï¼ˆæ–°APIï¼‰
workflow = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("node_1", node_1)
workflow.add_node("node_2", node_2)

# ğŸ”¥ æ·»åŠ è¾¹ï¼ˆä½¿ç”¨STARTå’ŒENDå¸¸é‡ï¼‰
workflow.add_edge(START, "node_1")
workflow.add_edge("node_1", "node_2")
workflow.add_edge("node_2", END)

# ç¼–è¯‘å›¾
app = workflow.compile()

# è¿è¡Œ
initial_state = {"messages": [], "count": 0}
result = app.invoke(initial_state)
print(result)
# è¾“å‡ºï¼š{'messages': ['èŠ‚ç‚¹1æ‰§è¡Œ', 'èŠ‚ç‚¹2æ‰§è¡Œ'], 'count': 2}
```

### 4.2 å¯è§†åŒ–çŠ¶æ€å›¾

```python
from IPython.display import Image, display

# ç”Ÿæˆå›¾çš„å¯è§†åŒ–
try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # å¦‚æœæ— æ³•æ˜¾ç¤ºï¼Œæ‰“å°Mermaidä»£ç 
    print(app.get_graph().draw_mermaid())
```

---

## 5. èŠ‚ç‚¹å’Œè¾¹

### 5.1 èŠ‚ç‚¹ç±»å‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from typing import TypedDict

class State(TypedDict):
    messages: list
    input: str
    output: str
    count: int
    should_continue: bool

# ğŸ”¥ LLMèŠ‚ç‚¹
def llm_node(state: State) -> dict:
    """ä½¿ç”¨LLMçš„èŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    messages = [HumanMessage(content=state["input"])]
    response = llm.invoke(messages)
    
    return {
        "messages": [response],
        "output": response.content
    }

# ğŸ”¥ å·¥å…·èŠ‚ç‚¹
def tool_node(state: State) -> dict:
    """ä½¿ç”¨å·¥å…·çš„èŠ‚ç‚¹"""
    # è°ƒç”¨å¤–éƒ¨å·¥å…·æˆ–API
    result = f"å¤„ç†ç»“æœï¼š{state['input']}"
    return {"output": result}

# ğŸ”¥ å†³ç­–èŠ‚ç‚¹
def decision_node(state: State) -> dict:
    """åšå‡ºå†³ç­–çš„èŠ‚ç‚¹"""
    should_continue = state["count"] < 5
    return {"should_continue": should_continue}

# ğŸ”¥ å¸¦é‡è¯•ç­–ç•¥çš„èŠ‚ç‚¹
from langgraph.types import RetryPolicy

workflow.add_node(
    "api_call",
    api_call_function,
    retry_policy=RetryPolicy(
        max_attempts=3,  # æœ€å¤šé‡è¯•3æ¬¡
        backoff_factor=2.0  # æŒ‡æ•°é€€é¿
    )
)
```

### 5.2 è¾¹çš„ç±»å‹

```python
from langgraph.graph import START, END

# ğŸ”¥ æ™®é€šè¾¹ï¼ˆå›ºå®šè·¯ç”±ï¼‰
workflow.add_edge("node_a", "node_b")

# ğŸ”¥ å…¥å£è¾¹ï¼ˆä½¿ç”¨STARTå¸¸é‡ï¼‰
workflow.add_edge(START, "start_node")

# ğŸ”¥ ç»“æŸè¾¹ï¼ˆä½¿ç”¨ENDå¸¸é‡ï¼‰
workflow.add_edge("final_node", END)

# ğŸ”¥ æ¡ä»¶è¾¹ï¼ˆåŠ¨æ€è·¯ç”±ï¼‰
workflow.add_conditional_edges(
    "decision_node",
    lambda state: "continue" if state["should_continue"] else "end",
    {
        "continue": "next_node",
        "end": END
    }
)

# ğŸ”¥ å¤šè·¯æ¡ä»¶è¾¹
def route_by_type(state: State) -> str:
    """æ ¹æ®ç±»å‹è·¯ç”±"""
    return state["type"]

workflow.add_conditional_edges(
    "classifier",
    route_by_type,
    {
        "type_a": "handler_a",
        "type_b": "handler_b",
        "type_c": "handler_c",
    }
)
```

---

## 6. æ¡ä»¶è·¯ç”±

### 6.1 åŸºäºçŠ¶æ€çš„è·¯ç”±

```python
from typing import Literal

# ğŸ”¥ å®šä¹‰è·¯ç”±å‡½æ•°
def route_based_on_count(state: State) -> Literal["continue", "end"]:
    """åŸºäºè®¡æ•°å™¨çš„è·¯ç”±"""
    if state["count"] < 3:
        return "continue"
    return "end"

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "counter_node",
    route_based_on_count,
    {
        "continue": "process_node",
        "end": END
    }
)
```

### 6.2 å¤æ‚æ¡ä»¶è·¯ç”±

```python
def complex_router(state: State) -> str:
    """å¤æ‚çš„è·¯ç”±é€»è¾‘"""
    if state["error"]:
        return "error_handler"
    elif state["needs_review"]:
        return "human_review"
    elif state["is_complete"]:
        return "finalize"
    else:
        return "continue_processing"

workflow.add_conditional_edges(
    "decision_node",
    complex_router,
    {
        "error_handler": "error_node",
        "human_review": "review_node",
        "finalize": "final_node",
        "continue_processing": "process_node"
    }
)
```

---

## 7. å¾ªç¯å’Œè¿­ä»£

### 7.1 ç®€å•å¾ªç¯

```python
from typing import TypedDict

class LoopState(TypedDict):
    count: int
    max_iterations: int
    result: str

def loop_node(state: LoopState) -> LoopState:
    """å¾ªç¯å¤„ç†èŠ‚ç‚¹"""
    print(f"è¿­ä»£ {state['count']}")
    return {
        "count": state["count"] + 1,
        "result": state["result"] + f"è¿­ä»£{state['count']} "
    }

def should_continue(state: LoopState) -> Literal["continue", "end"]:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯"""
    if state["count"] < state["max_iterations"]:
        return "continue"
    return "end"

# ğŸ”¥ æ„å»ºå¾ªç¯å›¾
workflow = StateGraph(LoopState)
workflow.add_node("loop", loop_node)

workflow.add_conditional_edges(
    "loop",
    should_continue,
    {
        "continue": "loop",  # å¾ªç¯å›è‡ªå·±
        "end": END
    }
)

workflow.set_entry_point("loop")
app = workflow.compile()

# è¿è¡Œ
result = app.invoke({
    "count": 0,
    "max_iterations": 5,
    "result": ""
})
print(result)
```

### 7.2 Agentå¾ªç¯

```python
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_core.messages import HumanMessage

class AgentState(TypedDict):
    messages: list
    iterations: int

def agent_node(state: AgentState) -> AgentState:
    """Agentæ‰§è¡ŒèŠ‚ç‚¹"""
    # åˆ›å»ºAgent
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    # ... Agenté€»è¾‘
    
    return {
        "messages": state["messages"] + [response],
        "iterations": state["iterations"] + 1
    }

def should_continue_agent(state: AgentState) -> str:
    """åˆ¤æ–­Agentæ˜¯å¦åº”è¯¥ç»§ç»­"""
    last_message = state["messages"][-1]
    
    # å¦‚æœæ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œç»“æŸ
    if "FINAL ANSWER" in last_message.content:
        return "end"
    
    # å¦‚æœè¿­ä»£æ¬¡æ•°è¿‡å¤šï¼Œç»“æŸ
    if state["iterations"] > 10:
        return "end"
    
    return "continue"

# æ„å»ºAgentå¾ªç¯
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)

workflow.add_conditional_edges(
    "agent",
    should_continue_agent,
    {
        "continue": "agent",
        "end": END
    }
)

workflow.set_entry_point("agent")
app = workflow.compile()
```

---

## 8. å¤šAgentåä½œ

### 8.1 é¡ºåºåä½œ

```python
from typing import TypedDict

class MultiAgentState(TypedDict):
    task: str
    research_result: str
    writing_result: str
    review_result: str

def research_agent(state: MultiAgentState) -> MultiAgentState:
    """ç ”ç©¶Agent"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"ç ”ç©¶ä»¥ä¸‹ä¸»é¢˜ï¼š{state['task']}"
    result = llm.invoke(prompt)
    
    return {"research_result": result.content}

def writing_agent(state: MultiAgentState) -> MultiAgentState:
    """å†™ä½œAgent"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"åŸºäºä»¥ä¸‹ç ”ç©¶ç»“æœå†™ä¸€ç¯‡æ–‡ç« ï¼š\n{state['research_result']}"
    result = llm.invoke(prompt)
    
    return {"writing_result": result.content}

def review_agent(state: MultiAgentState) -> MultiAgentState:
    """å®¡æ ¸Agent"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    prompt = f"å®¡æ ¸ä»¥ä¸‹æ–‡ç« ï¼š\n{state['writing_result']}"
    result = llm.invoke(prompt)
    
    return {"review_result": result.content}

# ğŸ”¥ æ„å»ºå¤šAgentå·¥ä½œæµ
workflow = StateGraph(MultiAgentState)

workflow.add_node("research", research_agent)
workflow.add_node("writing", writing_agent)
workflow.add_node("review", review_agent)

workflow.add_edge("research", "writing")
workflow.add_edge("writing", "review")
workflow.add_edge("review", END)

workflow.set_entry_point("research")
app = workflow.compile()

# è¿è¡Œ
result = app.invoke({"task": "äººå·¥æ™ºèƒ½çš„æœªæ¥"})
print(result["review_result"])
```

### 8.2 å¹¶è¡Œåä½œ

```python
from typing import TypedDict, Annotated
import operator

class ParallelState(TypedDict):
    task: str
    results: Annotated[list, operator.add]  # åˆå¹¶å¤šä¸ªç»“æœ

def agent_1(state: ParallelState) -> ParallelState:
    """Agent 1"""
    result = f"Agent1å¤„ç†: {state['task']}"
    return {"results": [result]}

def agent_2(state: ParallelState) -> ParallelState:
    """Agent 2"""
    result = f"Agent2å¤„ç†: {state['task']}"
    return {"results": [result]}

def aggregator(state: ParallelState) -> ParallelState:
    """èšåˆç»“æœ"""
    combined = "\n".join(state["results"])
    return {"final_result": combined}

# æ„å»ºå¹¶è¡Œå·¥ä½œæµ
workflow = StateGraph(ParallelState)

workflow.add_node("agent_1", agent_1)
workflow.add_node("agent_2", agent_2)
workflow.add_node("aggregator", aggregator)

# å¹¶è¡Œæ‰§è¡Œ
workflow.add_edge("agent_1", "aggregator")
workflow.add_edge("agent_2", "aggregator")
workflow.add_edge("aggregator", END)

# è®¾ç½®å¤šä¸ªå…¥å£ç‚¹ï¼ˆå¹¶è¡Œï¼‰
workflow.set_entry_point("agent_1")
workflow.set_entry_point("agent_2")

app = workflow.compile()
```

---

## 9. äººæœºäº¤äº’

### 9.1 äººå·¥å®¡æ ¸èŠ‚ç‚¹ï¼ˆæœ€æ–°APIï¼‰

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

class ReviewState(TypedDict):
    content: str
    approved: bool
    feedback: str

def generate_content(state: ReviewState) -> dict:
    """ç”Ÿæˆå†…å®¹"""
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-4o-mini")
    content = llm.invoke("å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« ").content
    return {"content": content}

def human_review(state: ReviewState) -> dict:
    """äººå·¥å®¡æ ¸ï¼ˆè¿™é‡Œä¼šæš‚åœç­‰å¾…äººå·¥è¾“å…¥ï¼‰"""
    print(f"è¯·å®¡æ ¸ä»¥ä¸‹å†…å®¹ï¼š\n{state['content']}")
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šç­‰å¾…å¤–éƒ¨è¾“å…¥
    # é€šè¿‡interrupt()æš‚åœæ‰§è¡Œ
    return state

def finalize(state: ReviewState) -> dict:
    """æœ€ç»ˆå¤„ç†"""
    if state.get("approved"):
        return {"result": "å†…å®¹å·²å‘å¸ƒ"}
    else:
        return {"result": f"å†…å®¹è¢«æ‹’ç»ï¼ŒåŸå› ï¼š{state.get('feedback', 'æœªæä¾›')}"}

# ğŸ”¥ ä½¿ç”¨æ£€æŸ¥ç‚¹æ”¯æŒäººæœºäº¤äº’
memory = MemorySaver()

workflow = StateGraph(ReviewState)
workflow.add_node("generate", generate_content)
workflow.add_node("review", human_review)
workflow.add_node("finalize", finalize)

workflow.add_edge(START, "generate")
workflow.add_edge("generate", "review")

# ğŸ”¥ æ¡ä»¶è¾¹ï¼šæ ¹æ®å®¡æ ¸ç»“æœå†³å®šä¸‹ä¸€æ­¥
def should_finalize(state: ReviewState) -> str:
    if state.get("approved") is not None:
        return "finalize"
    return END

workflow.add_conditional_edges(
    "review",
    should_finalize,
    {
        "finalize": "finalize",
        END: END
    }
)
workflow.add_edge("finalize", END)

# ä½¿ç”¨æ£€æŸ¥ç‚¹ç¼–è¯‘
app = workflow.compile(checkpointer=memory)

# ğŸ”¥ ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¼šåœ¨reviewèŠ‚ç‚¹æš‚åœï¼‰
config = {"configurable": {"thread_id": "review-1"}}
result = app.invoke({"content": "", "approved": None, "feedback": ""}, config)

# ğŸ”¥ äººå·¥å®¡æ ¸åç»§ç»­ï¼ˆæ›´æ–°çŠ¶æ€ï¼‰
# æ–¹å¼1ï¼šæ‰¹å‡†
app.update_state(config, {"approved": True})
result = app.invoke(None, config)

# æ–¹å¼2ï¼šæ‹’ç»
app.update_state(config, {"approved": False, "feedback": "å†…å®¹éœ€è¦æ”¹è¿›"})
result = app.invoke(None, config)
```

---

## 10. æŒä¹…åŒ–å’Œæ£€æŸ¥ç‚¹

### 10.1 ä½¿ç”¨æ£€æŸ¥ç‚¹

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END

# ğŸ”¥ åˆ›å»ºæ£€æŸ¥ç‚¹ï¼ˆå†…å­˜å­˜å‚¨ï¼‰
checkpointer = MemorySaver()

# ç¼–è¯‘æ—¶æ·»åŠ æ£€æŸ¥ç‚¹
app = workflow.compile(checkpointer=checkpointer)

# ğŸ”¥ ä½¿ç”¨é…ç½®è¿è¡Œï¼ˆå¿…é¡»æä¾›thread_idï¼‰
config = {"configurable": {"thread_id": "conversation-1"}}
result = app.invoke(initial_state, config)

# ç»§ç»­ä¹‹å‰çš„ä¼šè¯ï¼ˆä½¿ç”¨ç›¸åŒçš„thread_idï¼‰
result = app.invoke(new_input, config)

# ğŸ”¥ ä½¿ç”¨SQLiteæŒä¹…åŒ–å­˜å‚¨ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
from langgraph.checkpoint.sqlite import SqliteSaver

# åˆ›å»ºSQLiteæ£€æŸ¥ç‚¹
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")
app = workflow.compile(checkpointer=checkpointer)
```

### 10.2 è·å–çŠ¶æ€å†å²å’Œæ—¶é—´æ—…è¡Œ

```python
# ğŸ”¥ è·å–æ‰€æœ‰æ£€æŸ¥ç‚¹ï¼ˆçŠ¶æ€å†å²ï¼‰
for checkpoint in app.get_state_history(config):
    print(f"æ­¥éª¤: {checkpoint.metadata.get('step')}")
    print(f"çŠ¶æ€: {checkpoint.values}")
    print("---")

# ğŸ”¥ è·å–å½“å‰çŠ¶æ€
current_state = app.get_state(config)
print(f"å½“å‰çŠ¶æ€: {current_state.values}")
print(f"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {current_state.next}")

# ğŸ”¥ æ—¶é—´æ—…è¡Œï¼šå›åˆ°ä¹‹å‰çš„æ£€æŸ¥ç‚¹
# è·å–ç‰¹å®šæ£€æŸ¥ç‚¹ID
checkpoint_id = "specific-checkpoint-id"
config_with_checkpoint = {
    "configurable": {
        "thread_id": "conversation-1",
        "checkpoint_id": checkpoint_id
    }
}
result = app.invoke(new_input, config_with_checkpoint)
```

---

## 11. å®æˆ˜æ¡ˆä¾‹

### 11.1 å®¢æœæœºå™¨äºº

```python
from typing import TypedDict, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

class CustomerServiceState(TypedDict):
    messages: list
    category: str
    resolved: bool

def classify_query(state: CustomerServiceState) -> CustomerServiceState:
    """åˆ†ç±»ç”¨æˆ·æŸ¥è¯¢"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    prompt = f"""
    åˆ†ç±»ä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š
    - technical: æŠ€æœ¯é—®é¢˜
    - billing: è´¦å•é—®é¢˜
    - general: ä¸€èˆ¬å’¨è¯¢
    
    ç”¨æˆ·æŸ¥è¯¢: {state['messages'][-1].content}
    
    åªè¿”å›ç±»åˆ«åç§°ã€‚
    """
    
    category = llm.invoke(prompt).content.strip().lower()
    return {"category": category}

def handle_technical(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†æŠ€æœ¯é—®é¢˜"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(f"æŠ€æœ¯æ”¯æŒ: {state['messages'][-1].content}")
    return {
        "messages": state["messages"] + [AIMessage(content=response.content)],
        "resolved": True
    }

def handle_billing(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†è´¦å•é—®é¢˜"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(f"è´¦å•å’¨è¯¢: {state['messages'][-1].content}")
    return {
        "messages": state["messages"] + [AIMessage(content=response.content)],
        "resolved": True
    }

def handle_general(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†ä¸€èˆ¬å’¨è¯¢"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(f"ä¸€èˆ¬å’¨è¯¢: {state['messages'][-1].content}")
    return {
        "messages": state["messages"] + [AIMessage(content=response.content)],
        "resolved": True
    }

def route_by_category(state: CustomerServiceState) -> str:
    """æ ¹æ®ç±»åˆ«è·¯ç”±"""
    return state["category"]

# ğŸ”¥ æ„å»ºå®¢æœæœºå™¨äºº
workflow = StateGraph(CustomerServiceState)

workflow.add_node("classify", classify_query)
workflow.add_node("technical", handle_technical)
workflow.add_node("billing", handle_billing)
workflow.add_node("general", handle_general)

workflow.set_entry_point("classify")

workflow.add_conditional_edges(
    "classify",
    route_by_category,
    {
        "technical": "technical",
        "billing": "billing",
        "general": "general"
    }
)

workflow.add_edge("technical", END)
workflow.add_edge("billing", END)
workflow.add_edge("general", END)

app = workflow.compile()

# ä½¿ç”¨
result = app.invoke({
    "messages": [HumanMessage(content="æˆ‘çš„è´¦å•æœ‰é—®é¢˜")],
    "category": "",
    "resolved": False
})

print(result["messages"][-1].content)
```

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£LangGraphçš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿæ„å»ºçŠ¶æ€å›¾
- [ ] æŒæ¡æ¡ä»¶è·¯ç”±çš„ä½¿ç”¨
- [ ] èƒ½å¤Ÿå®ç°å¾ªç¯æµç¨‹
- [ ] ç†è§£å¤šAgentåä½œæ¨¡å¼
- [ ] æŒæ¡äººæœºäº¤äº’å®ç°
- [ ] äº†è§£çŠ¶æ€æŒä¹…åŒ–æœºåˆ¶

---

## ğŸ”— ç›¸å…³èµ„æº

- [LangGraphå®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
- [LangGraphç¤ºä¾‹](https://github.com/langchain-ai/langgraph/tree/main/examples)

---

@author erik.zhou
