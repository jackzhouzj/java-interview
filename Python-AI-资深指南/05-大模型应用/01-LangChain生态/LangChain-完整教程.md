# LangChain - å®Œæ•´æ•™ç¨‹

> @author erik.zhou

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

### ç‰ˆæœ¬ä¿¡æ¯
- **LangChainç‰ˆæœ¬**ï¼š0.3+
- **æœ€æ–°ç¨³å®šç‰ˆ**ï¼š0.3.x
- **æ¨èç‰ˆæœ¬**ï¼š0.3.0+ï¼ˆ2024-2025æœ€æ–°ç‰ˆæœ¬ï¼‰

### å­¦ä¹ éš¾åº¦
- **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­ (ä¸­ç­‰)
- **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š30-40å°æ—¶
- **é‡è¦ç¨‹åº¦**ï¼šâ­â­â­â­â­ (å¿…å­¦)

### å‰ç½®çŸ¥è¯†
- Python 3.9+åŸºç¡€
- å¼‚æ­¥ç¼–ç¨‹åŸºç¡€
- LLMåŸºæœ¬æ¦‚å¿µ
- APIè°ƒç”¨ç»éªŒ
- ç±»å‹æ³¨è§£ï¼ˆType Hintsï¼‰

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
- [ ] æŒæ¡LLMé›†æˆå’Œè°ƒç”¨æ–¹æ³•
- [ ] ç†Ÿç»ƒä½¿ç”¨Promptæ¨¡æ¿
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„Chain
- [ ] æŒæ¡Agentå¼€å‘æŠ€å·§
- [ ] ç†è§£Memoryç®¡ç†æœºåˆ¶
- [ ] èƒ½å¤Ÿé›†æˆå¤–éƒ¨å·¥å…·
- [ ] æŒæ¡å›è°ƒå’Œç›‘æ§æ–¹æ³•

## ğŸ“– ç›®å½•

1. [LangChainç®€ä»‹](#1-langchainç®€ä»‹)
2. [ç¯å¢ƒæ­å»º](#2-ç¯å¢ƒæ­å»º)
3. [æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
4. [LLMé›†æˆ](#4-llmé›†æˆ)
5. [Promptæ¨¡æ¿](#5-promptæ¨¡æ¿)
6. [Chainé“¾å¼è°ƒç”¨](#6-chainé“¾å¼è°ƒç”¨)
7. [Agentæ™ºèƒ½ä½“](#7-agentæ™ºèƒ½ä½“)
8. [Memoryè®°å¿†](#8-memoryè®°å¿†)
9. [å·¥å…·é›†æˆ](#9-å·¥å…·é›†æˆ)
10. [å›è°ƒå’Œç›‘æ§](#10-å›è°ƒå’Œç›‘æ§)
11. [æœ€ä½³å®è·µ](#11-æœ€ä½³å®è·µ)
12. [å®æˆ˜æ¡ˆä¾‹](#12-å®æˆ˜æ¡ˆä¾‹)

---

## 1. LangChainç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯LangChain

LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ¥å£å’Œå·¥å…·ï¼Œç®€åŒ–äº†LLMåº”ç”¨çš„å¼€å‘æµç¨‹ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ”¥ **æ¨¡å—åŒ–è®¾è®¡**ï¼šç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆ
- ğŸ”¥ **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒOpenAIã€Anthropicã€HuggingFaceç­‰
- ğŸ”¥ **é“¾å¼è°ƒç”¨**ï¼šå°†å¤šä¸ªç»„ä»¶ä¸²è”æˆå¤æ‚æµç¨‹
- ğŸ”¥ **Agentæ¡†æ¶**ï¼šæ„å»ºè‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“
- ğŸ”¥ **ä¸°å¯Œçš„å·¥å…·**ï¼šå†…ç½®å¤§é‡å®ç”¨å·¥å…·å’Œé›†æˆ

### 1.2 LangChainç”Ÿæ€ç³»ç»Ÿ

```
LangChainç”Ÿæ€ç³»ç»Ÿ
â”œâ”€â”€ LangChain Core      # æ ¸å¿ƒæ¡†æ¶
â”œâ”€â”€ LangGraph          # çŠ¶æ€å›¾å’Œå·¥ä½œæµ
â”œâ”€â”€ LangSmith          # è°ƒè¯•å’Œç›‘æ§
â”œâ”€â”€ LangServe          # APIéƒ¨ç½²
â””â”€â”€ LangChain Hub      # æ¨¡æ¿å’Œèµ„æºå…±äº«
```

### 1.3 åº”ç”¨åœºæ™¯

- **é—®ç­”ç³»ç»Ÿ**ï¼šåŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”
- **èŠå¤©æœºå™¨äºº**ï¼šå¯¹è¯å¼AIåŠ©æ‰‹
- **ä»£ç åŠ©æ‰‹**ï¼šä»£ç ç”Ÿæˆå’Œè§£é‡Š
- **æ•°æ®åˆ†æ**ï¼šè‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®
- **å†…å®¹ç”Ÿæˆ**ï¼šæ–‡ç« ã€æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
- **å·¥ä½œæµè‡ªåŠ¨åŒ–**ï¼šå¤æ‚ä»»åŠ¡è‡ªåŠ¨åŒ–

---

## 2. ç¯å¢ƒæ­å»º

### 2.1 å®‰è£…LangChain

```bash
# ğŸ”¥ æ¨èï¼šå®‰è£…æ ¸å¿ƒåŒ…ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
pip install langchain-core

# å®‰è£…OpenAIé›†æˆ
pip install langchain-openai

# å®‰è£…Anthropicé›†æˆ
pip install langchain-anthropic

# å®‰è£…ç¤¾åŒºé›†æˆ
pip install langchain-community

# å®‰è£…LangChainä¸»åŒ…ï¼ˆåŒ…å«å¸¸ç”¨ç»„ä»¶ï¼‰
pip install langchain

# å¯é€‰ï¼šå®‰è£…å®éªŒæ€§åŠŸèƒ½
pip install langchain-experimental
```

### 2.2 é…ç½®APIå¯†é’¥

```python
import os

# è®¾ç½®OpenAI APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = "your-api-key"

# æˆ–ä½¿ç”¨.envæ–‡ä»¶
# åˆ›å»º.envæ–‡ä»¶ï¼Œæ·»åŠ ï¼š
# OPENAI_API_KEY=your-api-key

# ä½¿ç”¨python-dotenvåŠ è½½
from dotenv import load_dotenv
load_dotenv()
```

### 2.3 éªŒè¯å®‰è£…

```python
from langchain_openai import ChatOpenAI

# åˆ›å»ºLLMå®ä¾‹
llm = ChatOpenAI(model="gpt-3.5-turbo")

# æµ‹è¯•è°ƒç”¨
response = llm.invoke("Hello, LangChain!")
print(response.content)
```

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1 LangChainæ¶æ„

```
LangChainæ ¸å¿ƒç»„ä»¶
â”œâ”€â”€ Models (æ¨¡å‹)
â”‚   â”œâ”€â”€ LLMs (å¤§è¯­è¨€æ¨¡å‹)
â”‚   â”œâ”€â”€ Chat Models (èŠå¤©æ¨¡å‹)
â”‚   â””â”€â”€ Embeddings (åµŒå…¥æ¨¡å‹)
â”œâ”€â”€ Prompts (æç¤ºè¯)
â”‚   â”œâ”€â”€ Prompt Templates (æ¨¡æ¿)
â”‚   â””â”€â”€ Example Selectors (ç¤ºä¾‹é€‰æ‹©å™¨)
â”œâ”€â”€ Chains (é“¾)
â”‚   â”œâ”€â”€ LLMChain (åŸºç¡€é“¾)
â”‚   â”œâ”€â”€ Sequential Chain (é¡ºåºé“¾)
â”‚   â””â”€â”€ Router Chain (è·¯ç”±é“¾)
â”œâ”€â”€ Agents (æ™ºèƒ½ä½“)
â”‚   â”œâ”€â”€ Agent Types (æ™ºèƒ½ä½“ç±»å‹)
â”‚   â””â”€â”€ Tools (å·¥å…·)
â”œâ”€â”€ Memory (è®°å¿†)
â”‚   â”œâ”€â”€ Buffer Memory (ç¼“å†²è®°å¿†)
â”‚   â””â”€â”€ Vector Store Memory (å‘é‡å­˜å‚¨è®°å¿†)
â””â”€â”€ Callbacks (å›è°ƒ)
    â”œâ”€â”€ Logging (æ—¥å¿—)
    â””â”€â”€ Tracing (è¿½è¸ª)
```

### 3.2 æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

#### Modelsï¼ˆæ¨¡å‹ï¼‰
- **LLMs**ï¼šæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰
- **Chat Models**ï¼šå¯¹è¯æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰
- **Embeddings**ï¼šæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹

#### Promptsï¼ˆæç¤ºè¯ï¼‰
- ç»“æ„åŒ–çš„è¾“å…¥æ¨¡æ¿
- æ”¯æŒå˜é‡æ›¿æ¢
- å¯å¤ç”¨çš„æç¤ºè¯è®¾è®¡

#### Chainsï¼ˆé“¾ï¼‰
- å°†å¤šä¸ªç»„ä»¶ä¸²è”
- å®ç°å¤æ‚çš„å¤„ç†æµç¨‹
- æ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯

#### Agentsï¼ˆæ™ºèƒ½ä½“ï¼‰
- è‡ªä¸»å†³ç­–çš„AIç³»ç»Ÿ
- å¯ä»¥ä½¿ç”¨å·¥å…·
- åŠ¨æ€è§„åˆ’æ‰§è¡Œæ­¥éª¤

#### Memoryï¼ˆè®°å¿†ï¼‰
- ä¿å­˜å¯¹è¯å†å²
- ä¸Šä¸‹æ–‡ç®¡ç†
- é•¿æœŸè®°å¿†å­˜å‚¨

---

## 4. LLMé›†æˆ

### 4.1 OpenAIé›†æˆ

```python
from langchain_openai import ChatOpenAI, OpenAI

# ğŸ”¥ Chatæ¨¡å‹ï¼ˆæ¨èï¼‰
chat_model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# åŸºç¡€LLMæ¨¡å‹
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    temperature=0.7
)

# è°ƒç”¨æ¨¡å‹
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
]

response = chat_model.invoke(messages)
print(response.content)
```

### 4.2 å…¶ä»–æ¨¡å‹é›†æˆ

```python
# ğŸ”¥ Anthropic Claudeï¼ˆæ¨èç”¨äºå¤æ‚æ¨ç†ï¼‰
from langchain_anthropic import ChatAnthropic
claude = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",  # æœ€æ–°Claudeæ¨¡å‹
    temperature=0.7
)

# Google Gemini
from langchain_google_genai import ChatGoogleGenerativeAI
gemini = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",  # æœ€æ–°Geminiæ¨¡å‹
    temperature=0.7
)

# ğŸ”¥ æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰- æ¨èç”¨äºéšç§åœºæ™¯
from langchain_community.llms import Ollama
local_llm = Ollama(
    model="llama3.2",  # æœ€æ–°Llamaæ¨¡å‹
    temperature=0.7
)

# DeepSeekï¼ˆé«˜æ€§ä»·æ¯”é€‰æ‹©ï¼‰
from langchain_openai import ChatOpenAI
deepseek = ChatOpenAI(
    model="deepseek-chat",
    base_url="https://api.deepseek.com/v1",
    api_key="your-deepseek-api-key"
)
```

### 4.3 æµå¼è¾“å‡º

```python
# ğŸ”¥ æµå¼å“åº”
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", streaming=True)

for chunk in llm.stream("è®²ä¸€ä¸ªç¬‘è¯"):
    print(chunk.content, end="", flush=True)
```

---

## 5. Promptæ¨¡æ¿

### 5.1 åŸºç¡€æ¨¡æ¿

```python
from langchain_core.prompts import PromptTemplate

# ğŸ”¥ åˆ›å»ºPromptæ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚
è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}
"""

prompt = PromptTemplate(
    input_variables=["role", "question"],
    template=template
)

# æ ¼å¼åŒ–Prompt
formatted_prompt = prompt.format(
    role="Pythonä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)
print(formatted_prompt)
```

### 5.2 Chat Promptæ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# ğŸ”¥ Chatæ¨¡æ¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}"),
    ("human", "{question}"),
])

# æ ¼å¼åŒ–
messages = chat_template.format_messages(
    role="Pythonä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)

# è°ƒç”¨æ¨¡å‹
response = chat_model.invoke(messages)
print(response.content)
```

### 5.3 Few-shot Prompt

```python
from langchain_core.prompts import FewShotPromptTemplate

# ç¤ºä¾‹
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "hot", "output": "cold"},
]

# ç¤ºä¾‹æ¨¡æ¿
example_template = """
Input: {input}
Output: {output}
"""

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template=example_template
)

# Few-shotæ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºä»¥ä¸‹è¯çš„åä¹‰è¯ï¼š",
    suffix="Input: {input}\nOutput:",
    input_variables=["input"]
)

# ä½¿ç”¨
print(few_shot_prompt.format(input="big"))
```

---

## 6. Chainé“¾å¼è°ƒç”¨

### 6.1 LLMChain

```python
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# ğŸ”¥ åˆ›å»ºLLMChain
llm = ChatOpenAI(model="gpt-3.5-turbo")

prompt = PromptTemplate(
    input_variables=["product"],
    template="ä¸º{product}å†™ä¸€ä¸ªå¹¿å‘Šè¯­"
)

chain = LLMChain(llm=llm, prompt=prompt)

# è¿è¡ŒChain
result = chain.run(product="æ™ºèƒ½æ‰‹è¡¨")
print(result)
```

### 6.2 Sequential Chain

```python
from langchain.chains import SequentialChain

# ç¬¬ä¸€ä¸ªChainï¼šç”Ÿæˆå‰§æœ¬å¤§çº²
synopsis_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["title"],
        template="ä¸ºç”µå½±ã€Š{title}ã€‹å†™ä¸€ä¸ªå‰§æœ¬å¤§çº²"
    ),
    output_key="synopsis"
)

# ç¬¬äºŒä¸ªChainï¼šç”Ÿæˆè¯„è®º
review_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["synopsis"],
        template="ä¸ºä»¥ä¸‹å‰§æœ¬å†™ä¸€ä¸ªè¯„è®ºï¼š\n{synopsis}"
    ),
    output_key="review"
)

# ğŸ”¥ ç»„åˆChain
overall_chain = SequentialChain(
    chains=[synopsis_chain, review_chain],
    input_variables=["title"],
    output_variables=["synopsis", "review"]
)

# è¿è¡Œ
result = overall_chain({"title": "AIçš„æœªæ¥"})
print(result["review"])
```

### 6.3 LCEL (LangChain Expression Language)

```python
# ğŸ”¥ ç°ä»£åŒ–çš„Chainå†™æ³•ï¼ˆå¼ºçƒˆæ¨èï¼Œè¿™æ˜¯LangChainçš„æ ¸å¿ƒç‰¹æ€§ï¼‰
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
model = ChatOpenAI(model="gpt-4o-mini")  # ä½¿ç”¨æœ€æ–°æ¨¡å‹
output_parser = StrOutputParser()

# ğŸ”¥ ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ç»„åˆï¼ˆLCELæ ¸å¿ƒè¯­æ³•ï¼‰
chain = prompt | model | output_parser

# è°ƒç”¨
result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(result)

# ğŸ”¥ æµå¼è°ƒç”¨ï¼ˆå®æ—¶è¾“å‡ºï¼‰
for chunk in chain.stream({"topic": "ç¨‹åºå‘˜"}):
    print(chunk, end="", flush=True)

# ğŸ”¥ æ‰¹é‡è°ƒç”¨ï¼ˆæé«˜æ•ˆç‡ï¼‰
results = chain.batch([
    {"topic": "ç¨‹åºå‘˜"},
    {"topic": "AI"},
    {"topic": "Python"}
])

# ğŸ”¥ å¼‚æ­¥è°ƒç”¨
import asyncio
async def async_example():
    result = await chain.ainvoke({"topic": "ç¨‹åºå‘˜"})
    print(result)

asyncio.run(async_example())
```

---

## 7. Agentæ™ºèƒ½ä½“

### 7.1 AgentåŸºç¡€ï¼ˆæœ€æ–°APIï¼‰

```python
# ğŸ”¥ ä½¿ç”¨æœ€æ–°çš„create_agent APIï¼ˆæ¨èï¼‰
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.tools import tool

# ğŸ”¥ ä½¿ç”¨@toolè£…é¥°å™¨å®šä¹‰å·¥å…·ï¼ˆç°ä»£åŒ–å†™æ³•ï¼‰
@tool
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯ã€‚
    
    Args:
        location: åœ°ç‚¹åç§°ï¼Œä¾‹å¦‚"åŒ—äº¬"ã€"ä¸Šæµ·"
    """
    return f"{location}çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œæ¸©åº¦25åº¦"

@tool
def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚"25 * 4 + 10"
    """
    try:
        return str(eval(expression))
    except:
        return "è®¡ç®—é”™è¯¯"

# ğŸ”¥ ä½¿ç”¨init_chat_modelåˆå§‹åŒ–æ¨¡å‹ï¼ˆæ”¯æŒå¤šç§æ¨¡å‹ï¼‰
model = init_chat_model(
    model="gpt-4o-mini",
    model_provider="openai",
    temperature=0
)

# ğŸ”¥ åˆ›å»ºAgentï¼ˆæ–°APIæ›´ç®€æ´ï¼‰
system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚
ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š
- get_weather: è·å–å¤©æ°”ä¿¡æ¯
- calculate: è®¡ç®—æ•°å­¦è¡¨è¾¾å¼

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚"""

agent = create_agent(
    model=model,
    tools=[get_weather, calculate],
    system_prompt=system_prompt
)

# è¿è¡ŒAgent
result = agent.invoke({"messages": [{"role": "user", "content": "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]})
print(result["messages"][-1].content)

result = agent.invoke({"messages": [{"role": "user", "content": "è®¡ç®— 25 * 4 + 10"}]})
print(result["messages"][-1].content)
```

### 7.2 å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰

```python
# ğŸ”¥ ç°ä»£åŒ–çš„å·¥å…·è°ƒç”¨æ–¹å¼ï¼ˆæ¨èï¼‰
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig, chain
from langchain_openai import ChatOpenAI

# å®šä¹‰å·¥å…·
@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    return f"æœç´¢ç»“æœï¼š{query}"

# åˆ›å»ºå¸¦å·¥å…·çš„æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini")
model_with_tools = model.bind_tools([search_database])

# ğŸ”¥ åˆ›å»ºå·¥å…·è°ƒç”¨é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"),
    ("human", "{user_input}"),
    ("placeholder", "{messages}"),
])

model_chain = prompt | model_with_tools

@chain
def tool_chain(user_input: str, config: RunnableConfig):
    """å¤„ç†å·¥å…·è°ƒç”¨çš„é“¾"""
    input_ = {"user_input": user_input}
    ai_msg = model_chain.invoke(input_, config=config)
    
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
    if ai_msg.tool_calls:
        tool_msgs = search_database.batch(ai_msg.tool_calls, config=config)
        return model_chain.invoke({**input_, "messages": [ai_msg, *tool_msgs]}, config=config)
    
    return ai_msg

# ä½¿ç”¨
result = tool_chain.invoke("æœç´¢LangChainç›¸å…³ä¿¡æ¯")
print(result.content)
```

---

## 8. Memoryè®°å¿†

### 8.1 ConversationBufferMemory

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

# ğŸ”¥ åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
llm = ChatOpenAI(model="gpt-3.5-turbo")
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# å¤šè½®å¯¹è¯
print(conversation.predict(input="ä½ å¥½ï¼Œæˆ‘å«Alice"))
print(conversation.predict(input="æˆ‘å–œæ¬¢Pythonç¼–ç¨‹"))
print(conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"))  # ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯
```

### 8.2 ConversationBufferWindowMemory

```python
from langchain.memory import ConversationBufferWindowMemory

# åªä¿ç•™æœ€è¿‘kè½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=2)

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
```

### 8.3 ConversationSummaryMemory

```python
from langchain.memory import ConversationSummaryMemory

# ğŸ”¥ è‡ªåŠ¨æ€»ç»“å¯¹è¯å†å²
memory = ConversationSummaryMemory(llm=llm)

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
```

---

## 9. å·¥å…·é›†æˆ

### 9.1 å†…ç½®å·¥å…·

```python
# ğŸ”¥ ä½¿ç”¨å†…ç½®å·¥å…·
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Wikipediaå·¥å…·
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
result = wikipedia.run("Python programming language")
print(result)

# ğŸ”¥ Webæœç´¢å·¥å…·ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
from langchain_community.tools import DuckDuckGoSearchRun

search = DuckDuckGoSearchRun()
result = search.run("LangChain latest features")
print(result)

# ğŸ”¥ æ–‡ä»¶ç³»ç»Ÿå·¥å…·
from langchain_community.tools import FileManagementToolkit
from langchain_community.agent_toolkits import FileManagementToolkit

toolkit = FileManagementToolkit(root_dir="./workspace")
tools = toolkit.get_tools()
```

### 9.2 è‡ªå®šä¹‰å·¥å…·

```python
from langchain.tools import tool

# ğŸ”¥ ä½¿ç”¨è£…é¥°å™¨åˆ›å»ºå·¥å…·
@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯"""
    # å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
    return f"æœç´¢ç»“æœï¼š{query}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """å‘é€é‚®ä»¶"""
    return f"é‚®ä»¶å·²å‘é€åˆ°{to}"

# å°†å·¥å…·æ·»åŠ åˆ°Agent
tools = [search_database, send_email]
```

---

## 10. å›è°ƒå’Œç›‘æ§

### 10.1 å›è°ƒå¤„ç†å™¨

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain_openai import ChatOpenAI

# ğŸ”¥ ä½¿ç”¨å›è°ƒ
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    callbacks=[StdOutCallbackHandler()]
)

response = llm.invoke("Hello")
```

### 10.2 è‡ªå®šä¹‰å›è°ƒ

```python
from langchain.callbacks.base import BaseCallbackHandler

class MyCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLMå¼€å§‹: {prompts}")
    
    def on_llm_end(self, response, **kwargs):
        print(f"LLMç»“æŸ: {response}")
    
    def on_llm_error(self, error, **kwargs):
        print(f"LLMé”™è¯¯: {error}")

# ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
llm = ChatOpenAI(callbacks=[MyCallbackHandler()])
```

---

## 11. æœ€ä½³å®è·µ

### 11.1 é”™è¯¯å¤„ç†

```python
from langchain.schema import OutputParserException

try:
    result = chain.invoke({"input": "test"})
except OutputParserException as e:
    print(f"è§£æé”™è¯¯: {e}")
except Exception as e:
    print(f"å…¶ä»–é”™è¯¯: {e}")
```

### 11.2 æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨ç¼“å­˜
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# æ‰¹é‡å¤„ç†
results = llm.batch([
    "é—®é¢˜1",
    "é—®é¢˜2",
    "é—®é¢˜3"
])
```

---

## 12. å®æˆ˜æ¡ˆä¾‹

### 12.1 RAGé—®ç­”ç³»ç»Ÿï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

```python
# ğŸ”¥ æ„å»ºRAGç³»ç»Ÿï¼ˆæœ€å¸¸ç”¨çš„LLMåº”ç”¨æ¨¡å¼ï¼‰
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. å‡†å¤‡æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
documents = [
    "LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘LLMåº”ç”¨çš„æ¡†æ¶ã€‚",
    "LangChainæ”¯æŒå¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬OpenAIã€Anthropicç­‰ã€‚",
    "LCELæ˜¯LangChainçš„æ ¸å¿ƒç‰¹æ€§ï¼Œç”¨äºæ„å»ºé“¾å¼è°ƒç”¨ã€‚"
]

# ğŸ”¥ åˆ›å»ºå‘é‡å­˜å‚¨
vectorstore = InMemoryVectorStore.from_texts(
    texts=documents,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

# 2. æ„å»ºRAGé“¾
prompt = ChatPromptTemplate.from_template("""
æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

model = ChatOpenAI(model="gpt-4o-mini")

# ğŸ”¥ ä½¿ç”¨LCELæ„å»ºRAGé“¾
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
print(answer)
```

### 12.2 ç»“æ„åŒ–è¾“å‡º

```python
# ğŸ”¥ ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼ˆPydanticæ¨¡å‹ï¼‰
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

class Person(BaseModel):
    """äººç‰©ä¿¡æ¯"""
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é¾„")
    occupation: str = Field(description="èŒä¸š")

# ğŸ”¥ ä½¿ç”¨with_structured_output
model = ChatOpenAI(model="gpt-4o-mini")
structured_llm = model.with_structured_output(Person)

# è°ƒç”¨
result = structured_llm.invoke("å¼ ä¸‰æ˜¯ä¸€ä½35å²çš„è½¯ä»¶å·¥ç¨‹å¸ˆ")
print(f"å§“åï¼š{result.name}, å¹´é¾„ï¼š{result.age}, èŒä¸šï¼š{result.occupation}")
```

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿé›†æˆä¸åŒçš„LLM
- [ ] æŒæ¡Promptæ¨¡æ¿çš„ä½¿ç”¨
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„Chain
- [ ] ç†è§£Agentçš„å·¥ä½œåŸç†
- [ ] æŒæ¡Memoryç®¡ç†
- [ ] èƒ½å¤Ÿé›†æˆå’Œåˆ›å»ºå·¥å…·
- [ ] äº†è§£å›è°ƒå’Œç›‘æ§æœºåˆ¶

---

## ğŸ”— ç›¸å…³èµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain GitHub](https://github.com/langchain-ai/langchain)
- [LangChainæ•™ç¨‹](https://python.langchain.com/docs/get_started/introduction)

---

@author erik.zhou
