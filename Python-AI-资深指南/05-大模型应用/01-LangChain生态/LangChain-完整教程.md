# LangChain - å®Œæ•´æ•™ç¨‹

> @author erik.zhou

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

### ç‰ˆæœ¬ä¿¡æ¯
- **LangChainç‰ˆæœ¬**ï¼š0.1+
- **æœ€æ–°ç¨³å®šç‰ˆ**ï¼š0.1.x
- **æ¨èç‰ˆæœ¬**ï¼š0.1.0+

### å­¦ä¹ éš¾åº¦
- **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­ (ä¸­ç­‰)
- **é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š30-40å°æ—¶
- **é‡è¦ç¨‹åº¦**ï¼šâ­â­â­â­â­ (å¿…å­¦)

### å‰ç½®çŸ¥è¯†
- PythonåŸºç¡€
- å¼‚æ­¥ç¼–ç¨‹åŸºç¡€
- LLMåŸºæœ¬æ¦‚å¿µ
- APIè°ƒç”¨ç»éªŒ

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
- [ ] æŒæ¡LLMé›†æˆå’Œè°ƒç”¨æ–¹æ³•
- [ ] ç†Ÿç»ƒä½¿ç”¨Promptæ¨¡æ¿
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„Chain
- [ ] æŒæ¡Agentå¼€å‘æŠ€å·§
- [ ] ç†è§£Memoryç®¡ç†æœºåˆ¶
- [ ] èƒ½å¤Ÿé›†æˆå¤–éƒ¨å·¥å…·
- [ ] æŒæ¡å›è°ƒå’Œç›‘æ§æ–¹æ³•

## ğŸ“– ç›®å½•

1. [LangChainç®€ä»‹](#1-langchainç®€ä»‹)
2. [ç¯å¢ƒæ­å»º](#2-ç¯å¢ƒæ­å»º)
3. [æ ¸å¿ƒæ¦‚å¿µ](#3-æ ¸å¿ƒæ¦‚å¿µ)
4. [LLMé›†æˆ](#4-llmé›†æˆ)
5. [Promptæ¨¡æ¿](#5-promptæ¨¡æ¿)
6. [Chainé“¾å¼è°ƒç”¨](#6-chainé“¾å¼è°ƒç”¨)
7. [Agentæ™ºèƒ½ä½“](#7-agentæ™ºèƒ½ä½“)
8. [Memoryè®°å¿†](#8-memoryè®°å¿†)
9. [å·¥å…·é›†æˆ](#9-å·¥å…·é›†æˆ)
10. [å›è°ƒå’Œç›‘æ§](#10-å›è°ƒå’Œç›‘æ§)
11. [æœ€ä½³å®è·µ](#11-æœ€ä½³å®è·µ)
12. [å®æˆ˜æ¡ˆä¾‹](#12-å®æˆ˜æ¡ˆä¾‹)

---

## 1. LangChainç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯LangChain

LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ¥å£å’Œå·¥å…·ï¼Œç®€åŒ–äº†LLMåº”ç”¨çš„å¼€å‘æµç¨‹ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- ğŸ”¥ **æ¨¡å—åŒ–è®¾è®¡**ï¼šç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»„åˆ
- ğŸ”¥ **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒOpenAIã€Anthropicã€HuggingFaceç­‰
- ğŸ”¥ **é“¾å¼è°ƒç”¨**ï¼šå°†å¤šä¸ªç»„ä»¶ä¸²è”æˆå¤æ‚æµç¨‹
- ğŸ”¥ **Agentæ¡†æ¶**ï¼šæ„å»ºè‡ªä¸»å†³ç­–çš„æ™ºèƒ½ä½“
- ğŸ”¥ **ä¸°å¯Œçš„å·¥å…·**ï¼šå†…ç½®å¤§é‡å®ç”¨å·¥å…·å’Œé›†æˆ

### 1.2 LangChainç”Ÿæ€ç³»ç»Ÿ

```
LangChainç”Ÿæ€ç³»ç»Ÿ
â”œâ”€â”€ LangChain Core      # æ ¸å¿ƒæ¡†æ¶
â”œâ”€â”€ LangGraph          # çŠ¶æ€å›¾å’Œå·¥ä½œæµ
â”œâ”€â”€ LangSmith          # è°ƒè¯•å’Œç›‘æ§
â”œâ”€â”€ LangServe          # APIéƒ¨ç½²
â””â”€â”€ LangChain Hub      # æ¨¡æ¿å’Œèµ„æºå…±äº«
```

### 1.3 åº”ç”¨åœºæ™¯

- **é—®ç­”ç³»ç»Ÿ**ï¼šåŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”
- **èŠå¤©æœºå™¨äºº**ï¼šå¯¹è¯å¼AIåŠ©æ‰‹
- **ä»£ç åŠ©æ‰‹**ï¼šä»£ç ç”Ÿæˆå’Œè§£é‡Š
- **æ•°æ®åˆ†æ**ï¼šè‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®
- **å†…å®¹ç”Ÿæˆ**ï¼šæ–‡ç« ã€æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
- **å·¥ä½œæµè‡ªåŠ¨åŒ–**ï¼šå¤æ‚ä»»åŠ¡è‡ªåŠ¨åŒ–

---

## 2. ç¯å¢ƒæ­å»º

### 2.1 å®‰è£…LangChain

```bash
# å®‰è£…æ ¸å¿ƒåŒ…
pip install langchain

# å®‰è£…OpenAIé›†æˆ
pip install langchain-openai

# å®‰è£…ç¤¾åŒºé›†æˆ
pip install langchain-community

# å®‰è£…å®Œæ•´ç‰ˆï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰
pip install langchain[all]
```

### 2.2 é…ç½®APIå¯†é’¥

```python
import os

# è®¾ç½®OpenAI APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = "your-api-key"

# æˆ–ä½¿ç”¨.envæ–‡ä»¶
# åˆ›å»º.envæ–‡ä»¶ï¼Œæ·»åŠ ï¼š
# OPENAI_API_KEY=your-api-key

# ä½¿ç”¨python-dotenvåŠ è½½
from dotenv import load_dotenv
load_dotenv()
```

### 2.3 éªŒè¯å®‰è£…

```python
from langchain_openai import ChatOpenAI

# åˆ›å»ºLLMå®ä¾‹
llm = ChatOpenAI(model="gpt-3.5-turbo")

# æµ‹è¯•è°ƒç”¨
response = llm.invoke("Hello, LangChain!")
print(response.content)
```

---

## 3. æ ¸å¿ƒæ¦‚å¿µ

### 3.1 LangChainæ¶æ„

```
LangChainæ ¸å¿ƒç»„ä»¶
â”œâ”€â”€ Models (æ¨¡å‹)
â”‚   â”œâ”€â”€ LLMs (å¤§è¯­è¨€æ¨¡å‹)
â”‚   â”œâ”€â”€ Chat Models (èŠå¤©æ¨¡å‹)
â”‚   â””â”€â”€ Embeddings (åµŒå…¥æ¨¡å‹)
â”œâ”€â”€ Prompts (æç¤ºè¯)
â”‚   â”œâ”€â”€ Prompt Templates (æ¨¡æ¿)
â”‚   â””â”€â”€ Example Selectors (ç¤ºä¾‹é€‰æ‹©å™¨)
â”œâ”€â”€ Chains (é“¾)
â”‚   â”œâ”€â”€ LLMChain (åŸºç¡€é“¾)
â”‚   â”œâ”€â”€ Sequential Chain (é¡ºåºé“¾)
â”‚   â””â”€â”€ Router Chain (è·¯ç”±é“¾)
â”œâ”€â”€ Agents (æ™ºèƒ½ä½“)
â”‚   â”œâ”€â”€ Agent Types (æ™ºèƒ½ä½“ç±»å‹)
â”‚   â””â”€â”€ Tools (å·¥å…·)
â”œâ”€â”€ Memory (è®°å¿†)
â”‚   â”œâ”€â”€ Buffer Memory (ç¼“å†²è®°å¿†)
â”‚   â””â”€â”€ Vector Store Memory (å‘é‡å­˜å‚¨è®°å¿†)
â””â”€â”€ Callbacks (å›è°ƒ)
    â”œâ”€â”€ Logging (æ—¥å¿—)
    â””â”€â”€ Tracing (è¿½è¸ª)
```

### 3.2 æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

#### Modelsï¼ˆæ¨¡å‹ï¼‰
- **LLMs**ï¼šæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰
- **Chat Models**ï¼šå¯¹è¯æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰
- **Embeddings**ï¼šæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹

#### Promptsï¼ˆæç¤ºè¯ï¼‰
- ç»“æ„åŒ–çš„è¾“å…¥æ¨¡æ¿
- æ”¯æŒå˜é‡æ›¿æ¢
- å¯å¤ç”¨çš„æç¤ºè¯è®¾è®¡

#### Chainsï¼ˆé“¾ï¼‰
- å°†å¤šä¸ªç»„ä»¶ä¸²è”
- å®ç°å¤æ‚çš„å¤„ç†æµç¨‹
- æ”¯æŒæ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯

#### Agentsï¼ˆæ™ºèƒ½ä½“ï¼‰
- è‡ªä¸»å†³ç­–çš„AIç³»ç»Ÿ
- å¯ä»¥ä½¿ç”¨å·¥å…·
- åŠ¨æ€è§„åˆ’æ‰§è¡Œæ­¥éª¤

#### Memoryï¼ˆè®°å¿†ï¼‰
- ä¿å­˜å¯¹è¯å†å²
- ä¸Šä¸‹æ–‡ç®¡ç†
- é•¿æœŸè®°å¿†å­˜å‚¨

---

## 4. LLMé›†æˆ

### 4.1 OpenAIé›†æˆ

```python
from langchain_openai import ChatOpenAI, OpenAI

# ğŸ”¥ Chatæ¨¡å‹ï¼ˆæ¨èï¼‰
chat_model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# åŸºç¡€LLMæ¨¡å‹
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    temperature=0.7
)

# è°ƒç”¨æ¨¡å‹
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
]

response = chat_model.invoke(messages)
print(response.content)
```

### 4.2 å…¶ä»–æ¨¡å‹é›†æˆ

```python
# Anthropic Claude
from langchain_anthropic import ChatAnthropic
claude = ChatAnthropic(model="claude-3-opus-20240229")

# Google Gemini
from langchain_google_genai import ChatGoogleGenerativeAI
gemini = ChatGoogleGenerativeAI(model="gemini-pro")

# HuggingFace
from langchain_community.llms import HuggingFaceHub
hf_llm = HuggingFaceHub(
    repo_id="google/flan-t5-large",
    model_kwargs={"temperature": 0.7}
)

# æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰
from langchain_community.llms import Ollama
local_llm = Ollama(model="llama2")
```

### 4.3 æµå¼è¾“å‡º

```python
# ğŸ”¥ æµå¼å“åº”
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", streaming=True)

for chunk in llm.stream("è®²ä¸€ä¸ªç¬‘è¯"):
    print(chunk.content, end="", flush=True)
```

---

## 5. Promptæ¨¡æ¿

### 5.1 åŸºç¡€æ¨¡æ¿

```python
from langchain_core.prompts import PromptTemplate

# ğŸ”¥ åˆ›å»ºPromptæ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚
è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}
"""

prompt = PromptTemplate(
    input_variables=["role", "question"],
    template=template
)

# æ ¼å¼åŒ–Prompt
formatted_prompt = prompt.format(
    role="Pythonä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)
print(formatted_prompt)
```

### 5.2 Chat Promptæ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# ğŸ”¥ Chatæ¨¡æ¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}"),
    ("human", "{question}"),
])

# æ ¼å¼åŒ–
messages = chat_template.format_messages(
    role="Pythonä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)

# è°ƒç”¨æ¨¡å‹
response = chat_model.invoke(messages)
print(response.content)
```

### 5.3 Few-shot Prompt

```python
from langchain_core.prompts import FewShotPromptTemplate

# ç¤ºä¾‹
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "hot", "output": "cold"},
]

# ç¤ºä¾‹æ¨¡æ¿
example_template = """
Input: {input}
Output: {output}
"""

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template=example_template
)

# Few-shotæ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºä»¥ä¸‹è¯çš„åä¹‰è¯ï¼š",
    suffix="Input: {input}\nOutput:",
    input_variables=["input"]
)

# ä½¿ç”¨
print(few_shot_prompt.format(input="big"))
```

---

## 6. Chainé“¾å¼è°ƒç”¨

### 6.1 LLMChain

```python
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# ğŸ”¥ åˆ›å»ºLLMChain
llm = ChatOpenAI(model="gpt-3.5-turbo")

prompt = PromptTemplate(
    input_variables=["product"],
    template="ä¸º{product}å†™ä¸€ä¸ªå¹¿å‘Šè¯­"
)

chain = LLMChain(llm=llm, prompt=prompt)

# è¿è¡ŒChain
result = chain.run(product="æ™ºèƒ½æ‰‹è¡¨")
print(result)
```

### 6.2 Sequential Chain

```python
from langchain.chains import SequentialChain

# ç¬¬ä¸€ä¸ªChainï¼šç”Ÿæˆå‰§æœ¬å¤§çº²
synopsis_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["title"],
        template="ä¸ºç”µå½±ã€Š{title}ã€‹å†™ä¸€ä¸ªå‰§æœ¬å¤§çº²"
    ),
    output_key="synopsis"
)

# ç¬¬äºŒä¸ªChainï¼šç”Ÿæˆè¯„è®º
review_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["synopsis"],
        template="ä¸ºä»¥ä¸‹å‰§æœ¬å†™ä¸€ä¸ªè¯„è®ºï¼š\n{synopsis}"
    ),
    output_key="review"
)

# ğŸ”¥ ç»„åˆChain
overall_chain = SequentialChain(
    chains=[synopsis_chain, review_chain],
    input_variables=["title"],
    output_variables=["synopsis", "review"]
)

# è¿è¡Œ
result = overall_chain({"title": "AIçš„æœªæ¥"})
print(result["review"])
```

### 6.3 LCEL (LangChain Expression Language)

```python
# ğŸ”¥ ç°ä»£åŒ–çš„Chainå†™æ³•ï¼ˆæ¨èï¼‰
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
model = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()

# ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ç»„åˆ
chain = prompt | model | output_parser

# è°ƒç”¨
result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(result)

# æµå¼è°ƒç”¨
for chunk in chain.stream({"topic": "ç¨‹åºå‘˜"}):
    print(chunk, end="", flush=True)
```

---

## 7. Agentæ™ºèƒ½ä½“

### 7.1 AgentåŸºç¡€

```python
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import Tool

# å®šä¹‰å·¥å…·
def get_weather(location: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    return f"{location}çš„å¤©æ°”æ˜¯æ™´å¤©ï¼Œæ¸©åº¦25åº¦"

def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        return str(eval(expression))
    except:
        return "è®¡ç®—é”™è¯¯"

tools = [
    Tool(
        name="Weather",
        func=get_weather,
        description="è·å–æŒ‡å®šåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯ã€‚è¾“å…¥åº”è¯¥æ˜¯åœ°ç‚¹åç§°ã€‚"
    ),
    Tool(
        name="Calculator",
        func=calculate,
        description="è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚è¾“å…¥åº”è¯¥æ˜¯æ•°å­¦è¡¨è¾¾å¼ã€‚"
    )
]

# ğŸ”¥ åˆ›å»ºAgent
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# è¿è¡ŒAgent
result = agent_executor.invoke({"input": "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"})
print(result["output"])

result = agent_executor.invoke({"input": "è®¡ç®— 25 * 4 + 10"})
print(result["output"])
```

### 7.2 ReAct Agent

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain import hub

# ä½¿ç”¨Hubä¸­çš„ReAct prompt
prompt = hub.pull("hwchase17/react")

# åˆ›å»ºReAct Agent
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

# è¿è¡Œ
result = agent_executor.invoke({
    "input": "åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿå¦‚æœæ¸©åº¦è¶…è¿‡20åº¦ï¼Œè®¡ç®—20*2"
})
```

---

## 8. Memoryè®°å¿†

### 8.1 ConversationBufferMemory

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI

# ğŸ”¥ åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
llm = ChatOpenAI(model="gpt-3.5-turbo")
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# å¤šè½®å¯¹è¯
print(conversation.predict(input="ä½ å¥½ï¼Œæˆ‘å«Alice"))
print(conversation.predict(input="æˆ‘å–œæ¬¢Pythonç¼–ç¨‹"))
print(conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"))  # ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯
```

### 8.2 ConversationBufferWindowMemory

```python
from langchain.memory import ConversationBufferWindowMemory

# åªä¿ç•™æœ€è¿‘kè½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=2)

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
```

### 8.3 ConversationSummaryMemory

```python
from langchain.memory import ConversationSummaryMemory

# ğŸ”¥ è‡ªåŠ¨æ€»ç»“å¯¹è¯å†å²
memory = ConversationSummaryMemory(llm=llm)

conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
```

---

## 9. å·¥å…·é›†æˆ

### 9.1 å†…ç½®å·¥å…·

```python
from langchain.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Wikipediaå·¥å…·
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())

# ä½¿ç”¨å·¥å…·
result = wikipedia.run("Python programming language")
print(result)
```

### 9.2 è‡ªå®šä¹‰å·¥å…·

```python
from langchain.tools import tool

# ğŸ”¥ ä½¿ç”¨è£…é¥°å™¨åˆ›å»ºå·¥å…·
@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯"""
    # å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘
    return f"æœç´¢ç»“æœï¼š{query}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """å‘é€é‚®ä»¶"""
    return f"é‚®ä»¶å·²å‘é€åˆ°{to}"

# å°†å·¥å…·æ·»åŠ åˆ°Agent
tools = [search_database, send_email]
```

---

## 10. å›è°ƒå’Œç›‘æ§

### 10.1 å›è°ƒå¤„ç†å™¨

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain_openai import ChatOpenAI

# ğŸ”¥ ä½¿ç”¨å›è°ƒ
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    callbacks=[StdOutCallbackHandler()]
)

response = llm.invoke("Hello")
```

### 10.2 è‡ªå®šä¹‰å›è°ƒ

```python
from langchain.callbacks.base import BaseCallbackHandler

class MyCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLMå¼€å§‹: {prompts}")
    
    def on_llm_end(self, response, **kwargs):
        print(f"LLMç»“æŸ: {response}")
    
    def on_llm_error(self, error, **kwargs):
        print(f"LLMé”™è¯¯: {error}")

# ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
llm = ChatOpenAI(callbacks=[MyCallbackHandler()])
```

---

## 11. æœ€ä½³å®è·µ

### 11.1 é”™è¯¯å¤„ç†

```python
from langchain.schema import OutputParserException

try:
    result = chain.invoke({"input": "test"})
except OutputParserException as e:
    print(f"è§£æé”™è¯¯: {e}")
except Exception as e:
    print(f"å…¶ä»–é”™è¯¯: {e}")
```

### 11.2 æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨ç¼“å­˜
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# æ‰¹é‡å¤„ç†
results = llm.batch([
    "é—®é¢˜1",
    "é—®é¢˜2",
    "é—®é¢˜3"
])
```

---

## 12. å®æˆ˜æ¡ˆä¾‹

### 12.1 ç®€å•é—®ç­”ç³»ç»Ÿ

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# æ„å»ºChain
prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜ï¼š{question}")
model = ChatOpenAI(model="gpt-3.5-turbo")
output_parser = StrOutputParser()

chain = prompt | model | output_parser

# ä½¿ç”¨
answer = chain.invoke({"question": "ä»€ä¹ˆæ˜¯LangChainï¼Ÿ"})
print(answer)
```

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿé›†æˆä¸åŒçš„LLM
- [ ] æŒæ¡Promptæ¨¡æ¿çš„ä½¿ç”¨
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„Chain
- [ ] ç†è§£Agentçš„å·¥ä½œåŸç†
- [ ] æŒæ¡Memoryç®¡ç†
- [ ] èƒ½å¤Ÿé›†æˆå’Œåˆ›å»ºå·¥å…·
- [ ] äº†è§£å›è°ƒå’Œç›‘æ§æœºåˆ¶

---

## ğŸ”— ç›¸å…³èµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain GitHub](https://github.com/langchain-ai/langchain)
- [LangChainæ•™ç¨‹](https://python.langchain.com/docs/get_started/introduction)

---

@author erik.zhou
