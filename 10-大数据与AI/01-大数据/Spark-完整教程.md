# Spark å®Œæ•´æ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [æŠ€æœ¯æ¦‚è¿°](#æŠ€æœ¯æ¦‚è¿°)
- [å­¦ä¹ ç›®æ ‡](#å­¦ä¹ ç›®æ ‡)
- [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å®æˆ˜åº”ç”¨](#å®æˆ˜åº”ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ç›¸å…³èµ„æº](#ç›¸å…³èµ„æº)

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

- **ç‰ˆæœ¬**: 3.5.0 (æœ€æ–°ç¨³å®šç‰ˆ)
- **å®˜æ–¹æ–‡æ¡£**: https://spark.apache.org/
- **å­¦ä¹ éš¾åº¦**: â­â­â­â­ (4/5æ˜Ÿ)
- **é‡è¦ç¨‹åº¦**: â­â­â­â­â­ (5/5æ˜Ÿ)
- **å‰ç½®çŸ¥è¯†**: 
  - Java/ScalaåŸºç¡€
  - HadoopåŸºç¡€æ¦‚å¿µ
  - åˆ†å¸ƒå¼ç³»ç»ŸåŸç†
  - å‡½æ•°å¼ç¼–ç¨‹æ€æƒ³

**æ–‡æ¡£æ¥æº**: Apache Sparkå®˜æ–¹æ–‡æ¡£ (Context7)

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- [ ] ç†è§£Sparkçš„æ ¸å¿ƒæ¶æ„å’Œè®¾è®¡ç†å¿µ
- [ ] æŒæ¡RDDç¼–ç¨‹æ¨¡å‹
- [ ] æŒæ¡DataFrameå’ŒDataset API
- [ ] ç†è§£Spark SQLçš„ä½¿ç”¨
- [ ] äº†è§£Sparkçš„å†…å­˜è®¡ç®—åŸç†
- [ ] æŒæ¡Sparkæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] èƒ½å¤Ÿåœ¨Javaé¡¹ç›®ä¸­é›†æˆSpark

## ğŸ“– åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯Spark

Apache Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§è§„æ¨¡æ•°æ®å¤„ç†å¼•æ“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡ã€‚å®ƒçš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯ï¼š

- **å†…å­˜è®¡ç®—**: æ•°æ®ç¼“å­˜åœ¨å†…å­˜ä¸­ï¼Œæ¯”Hadoop MapReduceå¿«10-100å€
- **é€šç”¨æ€§**: æ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
- **æ˜“ç”¨æ€§**: æä¾›Javaã€Scalaã€Pythonã€Rç­‰å¤šç§è¯­è¨€API
- **å…¼å®¹æ€§**: å¯ä»¥è¿è¡Œåœ¨Hadoopã€Kubernetesç­‰å¤šç§å¹³å°

### 1.2 Spark vs Hadoop MapReduce

| ç‰¹æ€§ | Spark | Hadoop MapReduce |
|------|-------|------------------|
| è®¡ç®—æ¨¡å‹ | å†…å­˜è®¡ç®— | ç£ç›˜è®¡ç®— |
| æ€§èƒ½ | å¿«10-100å€ | è¾ƒæ…¢ |
| æ˜“ç”¨æ€§ | APIç®€æ´ | ä»£ç å†—é•¿ |
| å®æ—¶æ€§ | æ”¯æŒæµå¤„ç† | ä»…æ‰¹å¤„ç† |
| è¿­ä»£è®¡ç®— | é«˜æ•ˆ | ä½æ•ˆ |

### 1.3 Sparkç”Ÿæ€ç³»ç»Ÿ

```
Spark Core (æ ¸å¿ƒå¼•æ“)
    â†“
â”œâ”€â”€ Spark SQL (ç»“æ„åŒ–æ•°æ®å¤„ç†)
â”œâ”€â”€ Spark Streaming (æµå¤„ç†)
â”œâ”€â”€ MLlib (æœºå™¨å­¦ä¹ )
â””â”€â”€ GraphX (å›¾è®¡ç®—)
```

### 1.4 åº”ç”¨åœºæ™¯

- **æ•°æ®åˆ†æ**: äº¤äº’å¼æ•°æ®æŸ¥è¯¢å’Œåˆ†æ
- **ETLå¤„ç†**: æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€åŠ è½½
- **æœºå™¨å­¦ä¹ **: å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
- **å®æ—¶è®¡ç®—**: å®æ—¶æ•°æ®æµå¤„ç†
- **å›¾è®¡ç®—**: ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿ

## ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ (é‡ç‚¹)

### 2.1 RDD (Resilient Distributed Dataset) ğŸ”¥

RDDæ˜¯Sparkæœ€åŸºæœ¬çš„æ•°æ®æŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜çš„ã€å¯åˆ†åŒºçš„ã€å¯å¹¶è¡Œè®¡ç®—çš„æ•°æ®é›†åˆã€‚

#### 2.1.1 RDDç‰¹æ€§

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼¹æ€§**: æ•°æ®ä¸¢å¤±å¯ä»¥è‡ªåŠ¨æ¢å¤
2. **åˆ†å¸ƒå¼**: æ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤çš„å¤šä¸ªèŠ‚ç‚¹
3. **ä¸å¯å˜**: ä¸€æ—¦åˆ›å»ºå°±ä¸èƒ½ä¿®æ”¹
4. **å¯åˆ†åŒº**: æ•°æ®è¢«åˆ‡åˆ†æˆå¤šä¸ªåˆ†åŒºå¹¶è¡Œå¤„ç†
5. **å¯ç¼“å­˜**: å¯ä»¥å°†æ•°æ®ç¼“å­˜åœ¨å†…å­˜ä¸­

**RDDçš„äº”å¤§å±æ€§**:
```java
1. åˆ†åŒºåˆ—è¡¨ (List of Partitions)
2. è®¡ç®—å‡½æ•° (Compute Function)
3. ä¾èµ–å…³ç³» (Dependencies)
4. åˆ†åŒºå™¨ (Partitioner) - å¯é€‰
5. ä¼˜å…ˆä½ç½® (Preferred Locations) - å¯é€‰
```

#### 2.1.2 RDDåˆ›å»ºæ–¹å¼

**æ–¹å¼1: å¹¶è¡ŒåŒ–é›†åˆ**
```java
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;

import java.util.Arrays;
import java.util.List;

/**
 * RDDåˆ›å»ºç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class RDDCreationExample {
    
    public static void main(String[] args) {
        SparkConf conf = new SparkConf()
            .setAppName("RDD Creation")
            .setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        // ä»é›†åˆåˆ›å»ºRDD
        List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
        JavaRDD<Integer> rdd = sc.parallelize(data);
        
        System.out.println("Count: " + rdd.count());
        
        sc.close();
    }
}
```

**æ–¹å¼2: è¯»å–å¤–éƒ¨æ•°æ®**
```java
// è¯»å–æ–‡æœ¬æ–‡ä»¶
JavaRDD<String> textRDD = sc.textFile("hdfs://localhost:9000/data/input.txt");

// è¯»å–å¤šä¸ªæ–‡ä»¶
JavaRDD<String> multiRDD = sc.textFile("hdfs://localhost:9000/data/*.txt");

// è¯»å–æ•´ä¸ªç›®å½•
JavaRDD<String> dirRDD = sc.wholeTextFiles("hdfs://localhost:9000/data/");
```

#### 2.1.3 RDDè½¬æ¢æ“ä½œ (Transformation) (âš ï¸ éš¾ç‚¹)

è½¬æ¢æ“ä½œæ˜¯æƒ°æ€§æ±‚å€¼çš„ï¼Œåªæœ‰é‡åˆ°è¡ŒåŠ¨æ“ä½œæ‰ä¼šçœŸæ­£æ‰§è¡Œã€‚

**å¸¸ç”¨è½¬æ¢æ“ä½œ**:

```java
/**
 * RDDè½¬æ¢æ“ä½œç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class RDDTransformationExample {
    
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("RDD Transformation").setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
        JavaRDD<Integer> rdd = sc.parallelize(data);
        
        // map: å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å‡½æ•°
        JavaRDD<Integer> squaredRDD = rdd.map(x -> x * x);
        
        // filter: è¿‡æ»¤å…ƒç´ 
        JavaRDD<Integer> evenRDD = rdd.filter(x -> x % 2 == 0);
        
        // flatMap: å°†æ¯ä¸ªå…ƒç´ æ˜ å°„ä¸ºå¤šä¸ªå…ƒç´ 
        JavaRDD<String> words = sc.parallelize(Arrays.asList("hello world", "spark java"));
        JavaRDD<String> wordsRDD = words.flatMap(line -> Arrays.asList(line.split(" ")).iterator());
        
        // distinct: å»é‡
        JavaRDD<Integer> distinctRDD = rdd.distinct();
        
        // union: åˆå¹¶ä¸¤ä¸ªRDD
        JavaRDD<Integer> unionRDD = rdd.union(squaredRDD);
        
        // intersection: äº¤é›†
        JavaRDD<Integer> intersectionRDD = rdd.intersection(evenRDD);
        
        // subtract: å·®é›†
        JavaRDD<Integer> subtractRDD = rdd.subtract(evenRDD);
        
        // groupBy: åˆ†ç»„
        JavaPairRDD<Integer, Iterable<Integer>> groupedRDD = rdd.groupBy(x -> x % 2);
        
        sc.close();
    }
}
```

**é”®å€¼å¯¹RDDæ“ä½œ**:
```java
import org.apache.spark.api.java.JavaPairRDD;
import scala.Tuple2;

/**
 * PairRDDæ“ä½œç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class PairRDDExample {
    
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("PairRDD").setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        List<Tuple2<String, Integer>> data = Arrays.asList(
            new Tuple2<>("apple", 3),
            new Tuple2<>("banana", 2),
            new Tuple2<>("apple", 5)
        );
        JavaPairRDD<String, Integer> pairRDD = sc.parallelizePairs(data);
        
        // reduceByKey: æŒ‰keyèšåˆ
        JavaPairRDD<String, Integer> sumRDD = pairRDD.reduceByKey((a, b) -> a + b);
        
        // groupByKey: æŒ‰keyåˆ†ç»„
        JavaPairRDD<String, Iterable<Integer>> groupedRDD = pairRDD.groupByKey();
        
        // mapValues: åªå¯¹valueè¿›è¡Œmapæ“ä½œ
        JavaPairRDD<String, Integer> doubledRDD = pairRDD.mapValues(v -> v * 2);
        
        // sortByKey: æŒ‰keyæ’åº
        JavaPairRDD<String, Integer> sortedRDD = pairRDD.sortByKey();
        
        // join: è¿æ¥ä¸¤ä¸ªPairRDD
        JavaPairRDD<String, Integer> otherRDD = sc.parallelizePairs(
            Arrays.asList(new Tuple2<>("apple", 10), new Tuple2<>("orange", 8))
        );
        JavaPairRDD<String, Tuple2<Integer, Integer>> joinedRDD = pairRDD.join(otherRDD);
        
        sc.close();
    }
}
```

#### 2.1.4 RDDè¡ŒåŠ¨æ“ä½œ (Action)

è¡ŒåŠ¨æ“ä½œä¼šè§¦å‘å®é™…çš„è®¡ç®—å¹¶è¿”å›ç»“æœã€‚

```java
/**
 * RDDè¡ŒåŠ¨æ“ä½œç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class RDDActionExample {
    
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("RDD Action").setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
        JavaRDD<Integer> rdd = sc.parallelize(data);
        
        // count: è®¡æ•°
        long count = rdd.count();
        System.out.println("Count: " + count);
        
        // collect: æ”¶é›†æ‰€æœ‰å…ƒç´ åˆ°Driver
        List<Integer> result = rdd.collect();
        System.out.println("Result: " + result);
        
        // first: è·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        Integer first = rdd.first();
        System.out.println("First: " + first);
        
        // take: è·å–å‰nä¸ªå…ƒç´ 
        List<Integer> topN = rdd.take(3);
        System.out.println("Top 3: " + topN);
        
        // reduce: èšåˆæ‰€æœ‰å…ƒç´ 
        Integer sum = rdd.reduce((a, b) -> a + b);
        System.out.println("Sum: " + sum);
        
        // foreach: å¯¹æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œ
        rdd.foreach(x -> System.out.println("Element: " + x));
        
        // saveAsTextFile: ä¿å­˜åˆ°æ–‡ä»¶
        rdd.saveAsTextFile("hdfs://localhost:9000/output");
        
        sc.close();
    }
}
```

### 2.2 DataFrameå’ŒDataset ğŸ”¥

DataFrameæ˜¯Spark SQLçš„æ ¸å¿ƒæŠ½è±¡ï¼Œæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ã€æŒ‰åˆ—ç»„ç»‡çš„æ•°æ®é›†åˆï¼Œç±»ä¼¼äºå…³ç³»æ•°æ®åº“çš„è¡¨ã€‚

#### 2.2.1 DataFrame vs RDD

| ç‰¹æ€§ | DataFrame | RDD |
|------|-----------|-----|
| æ•°æ®ç»“æ„ | ç»“æ„åŒ–æ•°æ® | éç»“æ„åŒ–æ•°æ® |
| ä¼˜åŒ– | Catalystä¼˜åŒ–å™¨ | æ— ä¼˜åŒ– |
| æ€§èƒ½ | æ›´å¿« | è¾ƒæ…¢ |
| API | å£°æ˜å¼ | å‘½ä»¤å¼ |
| ç±»å‹å®‰å…¨ | è¿è¡Œæ—¶æ£€æŸ¥ | ç¼–è¯‘æ—¶æ£€æŸ¥ |

#### 2.2.2 åˆ›å»ºDataFrame

```java
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

/**
 * DataFrameåˆ›å»ºç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class DataFrameCreationExample {
    
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("DataFrame Creation")
            .master("local[*]")
            .getOrCreate();
        
        // æ–¹å¼1: ä»JSONæ–‡ä»¶åˆ›å»º
        Dataset<Row> df1 = spark.read().json("data/users.json");
        
        // æ–¹å¼2: ä»CSVæ–‡ä»¶åˆ›å»º
        Dataset<Row> df2 = spark.read()
            .option("header", "true")
            .option("inferSchema", "true")
            .csv("data/users.csv");
        
        // æ–¹å¼3: ä»Parquetæ–‡ä»¶åˆ›å»º
        Dataset<Row> df3 = spark.read().parquet("data/users.parquet");
        
        // æ–¹å¼4: ä»JDBCæ•°æ®åº“åˆ›å»º
        Dataset<Row> df4 = spark.read()
            .format("jdbc")
            .option("url", "jdbc:mysql://localhost:3306/test")
            .option("dbtable", "users")
            .option("user", "root")
            .option("password", "password")
            .load();
        
        // æ–¹å¼5: ä»RDDåˆ›å»º
        JavaRDD<String> rdd = spark.sparkContext()
            .textFile("data/users.txt", 1)
            .toJavaRDD();
        Dataset<Row> df5 = spark.read().json(rdd);
        
        spark.stop();
    }
}
```

#### 2.2.3 DataFrameæ“ä½œ

```java
import org.apache.spark.sql.functions;
import static org.apache.spark.sql.functions.*;

/**
 * DataFrameæ“ä½œç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class DataFrameOperationExample {
    
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("DataFrame Operations")
            .master("local[*]")
            .getOrCreate();
        
        Dataset<Row> df = spark.read()
            .option("header", "true")
            .option("inferSchema", "true")
            .csv("data/users.csv");
        
        // æŸ¥çœ‹æ•°æ®
        df.show();
        df.printSchema();
        
        // é€‰æ‹©åˆ—
        df.select("name", "age").show();
        
        // è¿‡æ»¤
        df.filter(col("age").gt(18)).show();
        df.where("age > 18").show();
        
        // åˆ†ç»„èšåˆ
        df.groupBy("gender").count().show();
        df.groupBy("gender").agg(avg("age"), max("salary")).show();
        
        // æ’åº
        df.orderBy(col("age").desc()).show();
        
        // æ·»åŠ åˆ—
        Dataset<Row> df2 = df.withColumn("age_plus_10", col("age").plus(10));
        
        // é‡å‘½ååˆ—
        Dataset<Row> df3 = df.withColumnRenamed("name", "user_name");
        
        // åˆ é™¤åˆ—
        Dataset<Row> df4 = df.drop("salary");
        
        // å»é‡
        df.distinct().show();
        df.dropDuplicates("name").show();
        
        // è¿æ¥
        Dataset<Row> orders = spark.read().json("data/orders.json");
        df.join(orders, df.col("id").equalTo(orders.col("user_id"))).show();
        
        spark.stop();
    }
}
```

### 2.3 Spark SQL ğŸ”¥

Spark SQLå…è®¸ä½¿ç”¨SQLè¯­å¥æŸ¥è¯¢ç»“æ„åŒ–æ•°æ®ã€‚

#### 2.3.1 SQLæŸ¥è¯¢

```java
/**
 * Spark SQLç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class SparkSQLExample {
    
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("Spark SQL")
            .master("local[*]")
            .getOrCreate();
        
        Dataset<Row> df = spark.read().json("data/users.json");
        
        // æ³¨å†Œä¸´æ—¶è§†å›¾
        df.createOrReplaceTempView("users");
        
        // æ‰§è¡ŒSQLæŸ¥è¯¢
        Dataset<Row> result = spark.sql(
            "SELECT name, age FROM users WHERE age > 18 ORDER BY age DESC"
        );
        result.show();
        
        // å¤æ‚æŸ¥è¯¢
        Dataset<Row> result2 = spark.sql(
            "SELECT gender, COUNT(*) as count, AVG(age) as avg_age " +
            "FROM users " +
            "GROUP BY gender " +
            "HAVING count > 10"
        );
        result2.show();
        
        // å…¨å±€ä¸´æ—¶è§†å›¾ (è·¨Session)
        df.createGlobalTempView("global_users");
        spark.sql("SELECT * FROM global_temp.global_users").show();
        
        spark.stop();
    }
}
```

### 2.4 å†…å­˜è®¡ç®—åŸç† (âš ï¸ éš¾ç‚¹)

Sparkçš„é«˜æ€§èƒ½æºäºå…¶å†…å­˜è®¡ç®—æ¨¡å‹ã€‚

#### 2.4.1 DAGæ‰§è¡Œå¼•æ“

Sparkå°†ä½œä¸šè½¬æ¢ä¸ºDAG (æœ‰å‘æ— ç¯å›¾)ï¼Œç„¶åä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ã€‚

```
Job (ä½œä¸š)
  â†“
Stage (é˜¶æ®µ) - æ ¹æ®Shuffleåˆ’åˆ†
  â†“
Task (ä»»åŠ¡) - æ¯ä¸ªåˆ†åŒºä¸€ä¸ªTask
```

**æ‰§è¡Œæµç¨‹**:
```
1. æ„å»ºRDDçš„Lineage (è¡€ç»Ÿ)
2. æ ¹æ®å®½ä¾èµ–åˆ’åˆ†Stage
3. å°†Stageåˆ’åˆ†ä¸ºTask
4. è°ƒåº¦Taskåˆ°Executoræ‰§è¡Œ
5. æ”¶é›†ç»“æœ
```

**ä¾èµ–å…³ç³»**:
- **çª„ä¾èµ–**: çˆ¶RDDçš„ä¸€ä¸ªåˆ†åŒºæœ€å¤šè¢«å­RDDçš„ä¸€ä¸ªåˆ†åŒºä½¿ç”¨ (å¦‚mapã€filter)
- **å®½ä¾èµ–**: çˆ¶RDDçš„ä¸€ä¸ªåˆ†åŒºè¢«å­RDDçš„å¤šä¸ªåˆ†åŒºä½¿ç”¨ (å¦‚groupByKeyã€reduceByKey)

#### 2.4.2 ç¼“å­˜æœºåˆ¶

```java
/**
 * RDDç¼“å­˜ç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class CacheExample {
    
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("Cache").setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        JavaRDD<String> rdd = sc.textFile("data/large_file.txt");
        
        // ç¼“å­˜åˆ°å†…å­˜
        rdd.cache();  // ç­‰ä»·äº rdd.persist(StorageLevel.MEMORY_ONLY)
        
        // å¤šæ¬¡ä½¿ç”¨ç¼“å­˜çš„RDD
        long count1 = rdd.count();  // ç¬¬ä¸€æ¬¡è®¡ç®—å¹¶ç¼“å­˜
        long count2 = rdd.count();  // ç›´æ¥ä»ç¼“å­˜è¯»å–
        
        // ä¸åŒçš„å­˜å‚¨çº§åˆ«
        rdd.persist(StorageLevel.MEMORY_AND_DISK());  // å†…å­˜+ç£ç›˜
        rdd.persist(StorageLevel.MEMORY_ONLY_SER());  // åºåˆ—åŒ–å­˜å‚¨
        rdd.persist(StorageLevel.DISK_ONLY());        // ä»…ç£ç›˜
        
        // é‡Šæ”¾ç¼“å­˜
        rdd.unpersist();
        
        sc.close();
    }
}
```

**å­˜å‚¨çº§åˆ«é€‰æ‹©**:
- `MEMORY_ONLY`: é»˜è®¤ï¼Œæ€§èƒ½æœ€å¥½ä½†å¯èƒ½OOM
- `MEMORY_AND_DISK`: å†…å­˜ä¸è¶³æ—¶æº¢å†™åˆ°ç£ç›˜
- `MEMORY_ONLY_SER`: åºåˆ—åŒ–å­˜å‚¨ï¼ŒèŠ‚çœå†…å­˜ä½†å¢åŠ CPUå¼€é”€
- `DISK_ONLY`: ä»…ç£ç›˜ï¼Œæ€§èƒ½æœ€å·®ä½†æœ€å¯é 

## ğŸ’» å®æˆ˜åº”ç”¨

### 3.1 WordCountå®Œæ•´ç¤ºä¾‹

ä½¿ç”¨Sparkå®ç°ç»å…¸çš„WordCountç¨‹åºã€‚

```java
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.SparkConf;
import scala.Tuple2;

import java.util.Arrays;

/**
 * Spark WordCountç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class SparkWordCount {
    
    public static void main(String[] args) {
        if (args.length < 2) {
            System.err.println("Usage: SparkWordCount <input> <output>");
            System.exit(1);
        }
        
        // åˆ›å»ºSparkConf
        SparkConf conf = new SparkConf()
            .setAppName("Spark WordCount");
        
        // åˆ›å»ºJavaSparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);
        
        // è¯»å–è¾“å…¥æ–‡ä»¶
        JavaRDD<String> lines = sc.textFile(args[0]);
        
        // åˆ†è¯
        JavaRDD<String> words = lines.flatMap(
            line -> Arrays.asList(line.split("\\s+")).iterator()
        );
        
        // è½¬æ¢ä¸º(word, 1)
        JavaPairRDD<String, Integer> pairs = words.mapToPair(
            word -> new Tuple2<>(word, 1)
        );
        
        // æŒ‰keyèšåˆ
        JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(
            (a, b) -> a + b
        );
        
        // ä¿å­˜ç»“æœ
        wordCounts.saveAsTextFile(args[1]);
        
        sc.close();
    }
}
```

**ä½¿ç”¨DataFrameå®ç°**:
```java
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*;

/**
 * DataFrame WordCountç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class DataFrameWordCount {
    
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("DataFrame WordCount")
            .getOrCreate();
        
        // è¯»å–æ–‡ä»¶
        Dataset<Row> lines = spark.read().text(args[0]);
        
        // åˆ†è¯å¹¶ç»Ÿè®¡
        Dataset<Row> wordCounts = lines
            .select(explode(split(col("value"), "\\s+")).as("word"))
            .groupBy("word")
            .count()
            .orderBy(col("count").desc());
        
        // ä¿å­˜ç»“æœ
        wordCounts.write().csv(args[1]);
        
        spark.stop();
    }
}
```

### 3.2 æ—¥å¿—åˆ†æå®æˆ˜

åˆ†æWebæœåŠ¡å™¨æ—¥å¿—ï¼Œç»Ÿè®¡è®¿é—®é‡ã€é”™è¯¯ç‡ç­‰æŒ‡æ ‡ã€‚

```java
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*;

import java.io.Serializable;

/**
 * æ—¥å¿—åˆ†æç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class LogAnalysis {
    
    // æ—¥å¿—è®°å½•ç±»
    public static class LogRecord implements Serializable {
        private String ip;
        private String timestamp;
        private String method;
        private String url;
        private int statusCode;
        private long responseSize;
        
        // æ„é€ å‡½æ•°ã€getterã€setterçœç•¥
    }
    
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
            .appName("Log Analysis")
            .getOrCreate();
        
        // è¯»å–æ—¥å¿—æ–‡ä»¶
        JavaRDD<String> logLines = spark.sparkContext()
            .textFile("hdfs://localhost:9000/logs/*.log", 1)
            .toJavaRDD();
        
        // è§£ææ—¥å¿—
        JavaRDD<LogRecord> logs = logLines.map(line -> parseLog(line))
            .filter(log -> log != null);
        
        // è½¬æ¢ä¸ºDataFrame
        Dataset<Row> logDF = spark.createDataFrame(logs, LogRecord.class);
        
        // ç»Ÿè®¡åˆ†æ
        
        // 1. æŒ‰çŠ¶æ€ç ç»Ÿè®¡
        logDF.groupBy("statusCode")
            .count()
            .orderBy(col("count").desc())
            .show();
        
        // 2. ç»Ÿè®¡é”™è¯¯ç‡
        long totalCount = logDF.count();
        long errorCount = logDF.filter(col("statusCode").geq(400)).count();
        double errorRate = (double) errorCount / totalCount * 100;
        System.out.println("Error Rate: " + errorRate + "%");
        
        // 3. è®¿é—®é‡æœ€é«˜çš„URL
        logDF.groupBy("url")
            .count()
            .orderBy(col("count").desc())
            .limit(10)
            .show();
        
        // 4. æŒ‰å°æ—¶ç»Ÿè®¡è®¿é—®é‡
        logDF.withColumn("hour", hour(col("timestamp")))
            .groupBy("hour")
            .count()
            .orderBy("hour")
            .show();
        
        // 5. å¹³å‡å“åº”å¤§å°
        logDF.agg(avg("responseSize").as("avg_size")).show();
        
        spark.stop();
    }
    
    private static LogRecord parseLog(String line) {
        // è§£ææ—¥å¿—è¡Œçš„é€»è¾‘
        // è¿™é‡Œç®€åŒ–å¤„ç†
        return new LogRecord();
    }
}
```

### 3.3 ä¸Spring Booté›†æˆ

åœ¨Spring Booté¡¹ç›®ä¸­é›†æˆSparkè¿›è¡Œæ•°æ®å¤„ç†ã€‚

**Mavenä¾èµ–**:
```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.12</artifactId>
    <version>3.5.0</version>
</dependency>
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.12</artifactId>
    <version>3.5.0</version>
</dependency>
```

**é…ç½®ç±»**:
```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * Sparké…ç½®ç±»
 * 
 * @author erik.zhou
 */
@Configuration
public class SparkConfig {
    
    @Value("${spark.app.name}")
    private String appName;
    
    @Value("${spark.master}")
    private String master;
    
    @Bean
    public SparkConf sparkConf() {
        return new SparkConf()
            .setAppName(appName)
            .setMaster(master)
            .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
    }
    
    @Bean
    public JavaSparkContext javaSparkContext() {
        return new JavaSparkContext(sparkConf());
    }
    
    @Bean
    public SparkSession sparkSession() {
        return SparkSession.builder()
            .config(sparkConf())
            .getOrCreate();
    }
}
```

**Serviceå±‚**:
```java
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

/**
 * Sparkæ•°æ®å¤„ç†æœåŠ¡
 * 
 * @author erik.zhou
 */
@Service
public class SparkDataService {
    
    @Autowired
    private JavaSparkContext sparkContext;
    
    @Autowired
    private SparkSession sparkSession;
    
    /**
     * å¤„ç†æ–‡æœ¬æ•°æ®
     */
    public List<String> processTextData(String filePath) {
        JavaRDD<String> lines = sparkContext.textFile(filePath);
        JavaRDD<String> words = lines.flatMap(
            line -> java.util.Arrays.asList(line.split("\\s+")).iterator()
        );
        return words.take(100);
    }
    
    /**
     * æŸ¥è¯¢ç»“æ„åŒ–æ•°æ®
     */
    public List<Row> queryData(String tableName, String condition) {
        Dataset<Row> df = sparkSession.table(tableName);
        Dataset<Row> result = df.filter(condition);
        return result.collectAsList();
    }
    
    /**
     * èšåˆç»Ÿè®¡
     */
    public long countRecords(String filePath) {
        JavaRDD<String> rdd = sparkContext.textFile(filePath);
        return rdd.count();
    }
}
```

## âœ¨ æœ€ä½³å®è·µ

### 4.1 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 4.1.1 é¿å…Shuffleæ“ä½œ

Shuffleæ˜¯Sparkä¸­æœ€è€—æ—¶çš„æ“ä½œï¼Œåº”å°½é‡é¿å…ã€‚

**ä¼˜åŒ–å‰**:
```java
// groupByKeyä¼šäº§ç”Ÿå¤§é‡Shuffle
JavaPairRDD<String, Iterable<Integer>> grouped = pairRDD.groupByKey();
JavaPairRDD<String, Integer> result = grouped.mapValues(values -> {
    int sum = 0;
    for (int v : values) {
        sum += v;
    }
    return sum;
});
```

**ä¼˜åŒ–å**:
```java
// reduceByKeyåœ¨Mapç«¯å…ˆèšåˆï¼Œå‡å°‘Shuffleæ•°æ®é‡
JavaPairRDD<String, Integer> result = pairRDD.reduceByKey((a, b) -> a + b);
```

#### 4.1.2 åˆç†ä½¿ç”¨ç¼“å­˜

å¯¹äºå¤šæ¬¡ä½¿ç”¨çš„RDDï¼Œåº”è¯¥ç¼“å­˜åˆ°å†…å­˜ä¸­ã€‚

```java
/**
 * ç¼“å­˜ä¼˜åŒ–ç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class CacheOptimization {
    
    public static void main(String[] args) {
        JavaSparkContext sc = new JavaSparkContext(new SparkConf());
        
        JavaRDD<String> data = sc.textFile("large_file.txt");
        
        // å¯¹å¤šæ¬¡ä½¿ç”¨çš„RDDè¿›è¡Œç¼“å­˜
        JavaRDD<String> filteredData = data.filter(line -> line.contains("ERROR"));
        filteredData.cache();  // ç¼“å­˜
        
        // å¤šæ¬¡ä½¿ç”¨
        long errorCount = filteredData.count();
        List<String> samples = filteredData.take(10);
        filteredData.saveAsTextFile("errors.txt");
        
        sc.close();
    }
}
```

#### 4.1.3 åˆ†åŒºä¼˜åŒ–

åˆç†è®¾ç½®åˆ†åŒºæ•°é‡å¯ä»¥æé«˜å¹¶è¡Œåº¦ã€‚

```java
/**
 * åˆ†åŒºä¼˜åŒ–ç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class PartitionOptimization {
    
    public static void main(String[] args) {
        JavaSparkContext sc = new JavaSparkContext(new SparkConf());
        
        // è¯»å–æ•°æ®æ—¶æŒ‡å®šåˆ†åŒºæ•°
        JavaRDD<String> rdd = sc.textFile("data.txt", 100);
        
        // é‡æ–°åˆ†åŒº
        JavaRDD<String> repartitioned = rdd.repartition(200);
        
        // å‡å°‘åˆ†åŒºæ•° (ä¸ä¼šäº§ç”ŸShuffle)
        JavaRDD<String> coalesced = rdd.coalesce(50);
        
        // è‡ªå®šä¹‰åˆ†åŒºå™¨
        JavaPairRDD<String, Integer> pairRDD = rdd.mapToPair(
            line -> new Tuple2<>(line.substring(0, 1), 1)
        );
        JavaPairRDD<String, Integer> partitioned = pairRDD.partitionBy(
            new org.apache.spark.HashPartitioner(100)
        );
        
        sc.close();
    }
}
```

**åˆ†åŒºæ•°é‡å»ºè®®**:
- åˆ†åŒºæ•° = CPUæ ¸å¿ƒæ•° Ã— 2~3
- æ¯ä¸ªåˆ†åŒºå¤§å°: 128MB - 1GB
- é¿å…åˆ†åŒºè¿‡å¤šæˆ–è¿‡å°‘

#### 4.1.4 å¹¿æ’­å˜é‡

å¯¹äºå°æ•°æ®é›†ï¼Œä½¿ç”¨å¹¿æ’­å˜é‡é¿å…é‡å¤ä¼ è¾“ã€‚

```java
import org.apache.spark.broadcast.Broadcast;

/**
 * å¹¿æ’­å˜é‡ç¤ºä¾‹
 * 
 * @author erik.zhou
 */
public class BroadcastExample {
    
    public static void main(String[] args) {
        JavaSparkContext sc = new JavaSparkContext(new SparkConf());
        
        // å°æ•°æ®é›†
        Map<String, String> dict = new HashMap<>();
        dict.put("A", "Apple");
        dict.put("B", "Banana");
        
        // å¹¿æ’­å˜é‡
        Broadcast<Map<String, String>> broadcastDict = sc.broadcast(dict);
        
        JavaRDD<String> codes = sc.parallelize(Arrays.asList("A", "B", "C"));
        
        // åœ¨Taskä¸­ä½¿ç”¨å¹¿æ’­å˜é‡
        JavaRDD<String> names = codes.map(code -> {
            Map<String, String> localDict = broadcastDict.value();
            return localDict.getOrDefault(code, "Unknown");
        });
        
        names.collect().forEach(System.out::println);
        
        // é”€æ¯å¹¿æ’­å˜é‡
        broadcastDict.unpersist();
        
        sc.close();
    }
}
```

#### 4.1.5 æ•°æ®å€¾æ–œå¤„ç†

æ•°æ®å€¾æ–œä¼šå¯¼è‡´æŸäº›Taskæ‰§è¡Œæ—¶é—´è¿‡é•¿ã€‚

**è§£å†³æ–¹æ¡ˆ1: åŠ ç›**
```java
/**
 * æ•°æ®å€¾æ–œå¤„ç† - åŠ ç›æ³•
 * 
 * @author erik.zhou
 */
public class DataSkewSolution {
    
    public static void main(String[] args) {
        JavaSparkContext sc = new JavaSparkContext(new SparkConf());
        
        JavaPairRDD<String, Integer> skewedRDD = sc.parallelizePairs(
            Arrays.asList(
                new Tuple2<>("hot_key", 1),
                new Tuple2<>("hot_key", 2),
                // ... å¤§é‡hot_key
                new Tuple2<>("normal_key", 1)
            )
        );
        
        // åŠ ç›: ç»™keyæ·»åŠ éšæœºå‰ç¼€
        JavaPairRDD<String, Integer> saltedRDD = skewedRDD.mapToPair(
            tuple -> new Tuple2<>(
                tuple._1 + "_" + new Random().nextInt(10),
                tuple._2
            )
        );
        
        // èšåˆ
        JavaPairRDD<String, Integer> aggregated = saltedRDD.reduceByKey((a, b) -> a + b);
        
        // å»ç›: ç§»é™¤éšæœºå‰ç¼€
        JavaPairRDD<String, Integer> result = aggregated.mapToPair(
            tuple -> new Tuple2<>(
                tuple._1.split("_")[0],
                tuple._2
            )
        ).reduceByKey((a, b) -> a + b);
        
        sc.close();
    }
}
```

### 4.2 å†…å­˜ç®¡ç†

#### 4.2.1 å†…å­˜é…ç½®

**å…³é”®å‚æ•°**:
```bash
# Executorå†…å­˜
spark.executor.memory=4g

# Driverå†…å­˜
spark.driver.memory=2g

# å †å¤–å†…å­˜
spark.memory.offHeap.enabled=true
spark.memory.offHeap.size=2g

# å†…å­˜åˆ†é…æ¯”ä¾‹
spark.memory.fraction=0.6  # æ‰§è¡Œå’Œå­˜å‚¨å†…å­˜å æ¯”
spark.memory.storageFraction=0.5  # å­˜å‚¨å†…å­˜å æ¯”
```

#### 4.2.2 åºåˆ—åŒ–ä¼˜åŒ–

ä½¿ç”¨Kryoåºåˆ—åŒ–å™¨æé«˜æ€§èƒ½ã€‚

```java
SparkConf conf = new SparkConf()
    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .registerKryoClasses(new Class[]{
        MyClass1.class,
        MyClass2.class
    });
```

### 4.3 ç›‘æ§ä¸è°ƒä¼˜

#### 4.3.1 Spark UI

è®¿é—® `http://driver-node:4040` æŸ¥çœ‹ï¼š
- Jobs: ä½œä¸šæ‰§è¡Œæƒ…å†µ
- Stages: é˜¶æ®µè¯¦æƒ…å’ŒDAG
- Storage: RDDç¼“å­˜æƒ…å†µ
- Environment: é…ç½®ä¿¡æ¯
- Executors: ExecutorçŠ¶æ€

#### 4.3.2 å…³é”®æŒ‡æ ‡

**æ€§èƒ½æŒ‡æ ‡**:
- Taskæ‰§è¡Œæ—¶é—´
- Shuffleè¯»å†™é‡
- GCæ—¶é—´
- å†…å­˜ä½¿ç”¨ç‡

**ä¼˜åŒ–ç›®æ ‡**:
- å‡å°‘Shuffleæ•°æ®é‡
- é™ä½GCé¢‘ç‡
- æé«˜CPUåˆ©ç”¨ç‡
- é¿å…æ•°æ®å€¾æ–œ

## âš ï¸ å¸¸è§é™·é˜±

### é™·é˜±1: è¿‡åº¦ä½¿ç”¨collect()

**é—®é¢˜**: collect()ä¼šå°†æ‰€æœ‰æ•°æ®æ‹‰åˆ°Driverï¼Œå¯èƒ½å¯¼è‡´OOM

**è§£å†³æ–¹æ¡ˆ**:
```java
// é”™è¯¯: æ”¶é›†å¤§é‡æ•°æ®
List<String> allData = largeRDD.collect();  // å¯èƒ½OOM

// æ­£ç¡®: åªæ”¶é›†å¿…è¦çš„æ•°æ®
List<String> sample = largeRDD.take(100);
long count = largeRDD.count();
```

### é™·é˜±2: åœ¨Executorä¸­åˆ›å»ºSparkContext

**é—®é¢˜**: SparkContextåªèƒ½åœ¨Driverä¸­åˆ›å»º

**è§£å†³æ–¹æ¡ˆ**:
```java
// é”™è¯¯: åœ¨mapä¸­åˆ›å»ºSparkContext
rdd.map(x -> {
    SparkContext sc = new SparkContext();  // é”™è¯¯!
    return x;
});

// æ­£ç¡®: ä½¿ç”¨å¹¿æ’­å˜é‡æˆ–å¤–éƒ¨èµ„æº
```

### é™·é˜±3: å¿˜è®°å…³é—­èµ„æº

**é—®é¢˜**: ä¸å…³é—­SparkContextä¼šå¯¼è‡´èµ„æºæ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
```java
JavaSparkContext sc = new JavaSparkContext(conf);
try {
    // å¤„ç†é€»è¾‘
} finally {
    sc.close();  // ç¡®ä¿å…³é—­
}
```

### é™·é˜±4: ä½¿ç”¨éåºåˆ—åŒ–å¯¹è±¡

**é—®é¢˜**: åœ¨RDDæ“ä½œä¸­ä½¿ç”¨éåºåˆ—åŒ–å¯¹è±¡ä¼šå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```java
// é”™è¯¯: ä½¿ç”¨éåºåˆ—åŒ–å¯¹è±¡
class MyClass {  // æ²¡æœ‰å®ç°Serializable
    private int value;
}

// æ­£ç¡®: å®ç°Serializable
class MyClass implements Serializable {
    private int value;
}
```

## â“ å¸¸è§é—®é¢˜

### Q1: Sparké€‚åˆä»€ä¹ˆæ ·çš„åœºæ™¯ï¼Ÿ

**A**: Sparké€‚åˆä»¥ä¸‹åœºæ™¯ï¼š
- **è¿­ä»£è®¡ç®—**: æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—ç­‰éœ€è¦å¤šæ¬¡è¿­ä»£çš„åœºæ™¯
- **äº¤äº’å¼æŸ¥è¯¢**: æ•°æ®æ¢ç´¢å’Œåˆ†æ
- **å®æ—¶æµå¤„ç†**: å‡†å®æ—¶æ•°æ®å¤„ç† (Spark Streaming)
- **æ‰¹å¤„ç†**: æ›¿ä»£Hadoop MapReduceçš„æ‰¹å¤„ç†ä»»åŠ¡
- **å¤šè¯­è¨€æ”¯æŒ**: éœ€è¦ä½¿ç”¨Javaã€Scalaã€Pythonã€Rçš„åœºæ™¯

**ä¸é€‚åˆçš„åœºæ™¯**:
- è¶…ä½å»¶è¿Ÿè¦æ±‚ (æ¯«ç§’çº§ï¼Œä½¿ç”¨Flink)
- çº¯æµå¤„ç† (ä½¿ç”¨Flinkæˆ–Kafka Streams)
- å°æ•°æ®é‡ (å•æœºå¤„ç†æ›´é«˜æ•ˆ)

### Q2: RDDã€DataFrameã€Datasetæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸‰è€…å¯¹æ¯”ï¼š

| ç‰¹æ€§ | RDD | DataFrame | Dataset |
|------|-----|-----------|---------|
| ç±»å‹å®‰å…¨ | ç¼–è¯‘æ—¶ | è¿è¡Œæ—¶ | ç¼–è¯‘æ—¶ |
| ä¼˜åŒ– | æ—  | Catalystä¼˜åŒ– | Catalystä¼˜åŒ– |
| API | å‡½æ•°å¼ | å£°æ˜å¼ | ä¸¤è€…ç»“åˆ |
| æ€§èƒ½ | è¾ƒæ…¢ | å¿« | æœ€å¿« |
| é€‚ç”¨åœºæ™¯ | éç»“æ„åŒ–æ•°æ® | ç»“æ„åŒ–æ•°æ® | å¼ºç±»å‹æ•°æ® |

**é€‰æ‹©å»ºè®®**:
- ç»“æ„åŒ–æ•°æ®ä¼˜å…ˆä½¿ç”¨DataFrame/Dataset
- éœ€è¦ç±»å‹å®‰å…¨ä½¿ç”¨Dataset
- éç»“æ„åŒ–æ•°æ®æˆ–éœ€è¦åº•å±‚æ§åˆ¶ä½¿ç”¨RDD

### Q3: å¦‚ä½•é€‰æ‹©Sparkçš„éƒ¨ç½²æ¨¡å¼ï¼Ÿ

**A**: Sparkæ”¯æŒå¤šç§éƒ¨ç½²æ¨¡å¼ï¼š

**1. Localæ¨¡å¼**:
- é€‚ç”¨åœºæ™¯: å¼€å‘æµ‹è¯•
- é…ç½®: `setMaster("local[*]")`

**2. Standaloneæ¨¡å¼**:
- é€‚ç”¨åœºæ™¯: å°è§„æ¨¡é›†ç¾¤
- ä¼˜ç‚¹: éƒ¨ç½²ç®€å•
- ç¼ºç‚¹: èµ„æºç®¡ç†åŠŸèƒ½å¼±

**3. YARNæ¨¡å¼**:
- é€‚ç”¨åœºæ™¯: å·²æœ‰Hadoopé›†ç¾¤
- ä¼˜ç‚¹: èµ„æºç®¡ç†æˆç†Ÿ
- é…ç½®: `setMaster("yarn")`

**4. Kubernetesæ¨¡å¼**:
- é€‚ç”¨åœºæ™¯: äº‘åŸç”Ÿç¯å¢ƒ
- ä¼˜ç‚¹: å®¹å™¨åŒ–éƒ¨ç½²
- é…ç½®: `setMaster("k8s://...")`

**æ¨è**: ç”Ÿäº§ç¯å¢ƒä¼˜å…ˆé€‰æ‹©YARNæˆ–Kubernetesã€‚

### Q4: Sparkå’ŒFlinkå¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**: ä¸¤è€…å¯¹æ¯”ï¼š

| ç‰¹æ€§ | Spark | Flink |
|------|-------|-------|
| è®¡ç®—æ¨¡å‹ | å¾®æ‰¹å¤„ç† | çœŸæµå¤„ç† |
| å»¶è¿Ÿ | ç§’çº§ | æ¯«ç§’çº§ |
| ååé‡ | é«˜ | è¾ƒé«˜ |
| çŠ¶æ€ç®¡ç† | è¾ƒå¼± | å¼º |
| ç”Ÿæ€ | æˆç†Ÿ | å‘å±•ä¸­ |
| å­¦ä¹ æ›²çº¿ | å¹³ç¼“ | é™¡å³­ |

**é€‰æ‹©å»ºè®®**:
- **æ‰¹å¤„ç†**: Spark
- **å®æ—¶æµå¤„ç†**: Flink
- **å‡†å®æ—¶ + æ‰¹å¤„ç†**: Spark
- **å¤æ‚äº‹ä»¶å¤„ç†**: Flink
- **æœºå™¨å­¦ä¹ **: Spark (MLlibæ›´æˆç†Ÿ)

### Q5: å¦‚ä½•è°ƒè¯•Sparkç¨‹åºï¼Ÿ

**A**: è°ƒè¯•æ–¹æ³•ï¼š

**1. æœ¬åœ°æ¨¡å¼è°ƒè¯•**:
```java
SparkConf conf = new SparkConf()
    .setMaster("local[*]")  // æœ¬åœ°æ¨¡å¼
    .setAppName("Debug");
```

**2. ä½¿ç”¨æ—¥å¿—**:
```java
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MySparkApp {
    private static final Logger logger = LoggerFactory.getLogger(MySparkApp.class);
    
    public static void main(String[] args) {
        logger.info("Starting Spark application");
        // ...
    }
}
```

**3. æŸ¥çœ‹Spark UI**:
- è®¿é—® `http://driver:4040`
- æŸ¥çœ‹Stageçš„DAGå›¾
- æ£€æŸ¥Taskæ‰§è¡Œæ—¶é—´
- æŸ¥çœ‹Shuffleæ•°æ®é‡

**4. é‡‡æ ·è°ƒè¯•**:
```java
// ä½¿ç”¨sample()å‡å°‘æ•°æ®é‡
JavaRDD<String> sample = largeRDD.sample(false, 0.01);
sample.collect().forEach(System.out::println);
```

**5. ä½¿ç”¨toDebugString()**:
```java
// æŸ¥çœ‹RDDçš„è¡€ç»Ÿä¿¡æ¯
System.out.println(rdd.toDebugString());
```

### Q6: Sparkåœ¨Javaåç«¯é¡¹ç›®ä¸­çš„å…¸å‹åº”ç”¨ï¼Ÿ

**A**: å…¸å‹åº”ç”¨åœºæ™¯ï¼š

**1. ç¦»çº¿æŠ¥è¡¨ç”Ÿæˆ**:
```java
/**
 * æ¯æ—¥æŠ¥è¡¨ç”Ÿæˆ
 * 
 * @author erik.zhou
 */
@Service
public class ReportService {
    
    @Autowired
    private SparkSession spark;
    
    @Scheduled(cron = "0 0 2 * * ?")  // æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    public void generateDailyReport() {
        Dataset<Row> orders = spark.read()
            .jdbc("jdbc:mysql://localhost:3306/db", "orders", ...);
        
        Dataset<Row> report = orders
            .filter(col("order_date").equalTo(yesterday()))
            .groupBy("category")
            .agg(
                sum("amount").as("total_amount"),
                count("*").as("order_count")
            );
        
        report.write().mode("overwrite").parquet("hdfs://reports/daily");
    }
}
```

**2. ç”¨æˆ·è¡Œä¸ºåˆ†æ**:
```java
/**
 * ç”¨æˆ·è¡Œä¸ºåˆ†æ
 * 
 * @author erik.zhou
 */
@Service
public class UserBehaviorService {
    
    @Autowired
    private SparkSession spark;
    
    public Map<String, Long> analyzeUserBehavior(String userId) {
        Dataset<Row> logs = spark.read().parquet("hdfs://logs/user_behavior");
        
        Dataset<Row> userLogs = logs.filter(col("user_id").equalTo(userId));
        
        Map<String, Long> result = new HashMap<>();
        result.put("pageViews", userLogs.filter(col("action").equalTo("view")).count());
        result.put("clicks", userLogs.filter(col("action").equalTo("click")).count());
        result.put("purchases", userLogs.filter(col("action").equalTo("purchase")).count());
        
        return result;
    }
}
```

**3. æ•°æ®ETL**:
```java
/**
 * æ•°æ®ETLå¤„ç†
 * 
 * @author erik.zhou
 */
@Service
public class ETLService {
    
    @Autowired
    private SparkSession spark;
    
    public void etlProcess() {
        // Extract: ä»å¤šä¸ªæ•°æ®æºè¯»å–
        Dataset<Row> mysqlData = spark.read().jdbc(...);
        Dataset<Row> hdfsData = spark.read().parquet("hdfs://raw_data");
        
        // Transform: æ•°æ®æ¸…æ´—å’Œè½¬æ¢
        Dataset<Row> cleaned = mysqlData
            .join(hdfsData, "id")
            .filter(col("status").equalTo("valid"))
            .withColumn("processed_time", current_timestamp())
            .select("id", "name", "value", "processed_time");
        
        // Load: å†™å…¥ç›®æ ‡å­˜å‚¨
        cleaned.write()
            .mode("append")
            .partitionBy("date")
            .parquet("hdfs://processed_data");
    }
}
```

**4. æ¨èç³»ç»Ÿç¦»çº¿è®¡ç®—**:
```java
/**
 * æ¨èç³»ç»Ÿç¦»çº¿è®¡ç®—
 * 
 * @author erik.zhou
 */
@Service
public class RecommendationService {
    
    @Autowired
    private SparkSession spark;
    
    public void calculateRecommendations() {
        // è¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        Dataset<Row> userBehavior = spark.read().parquet("hdfs://user_behavior");
        
        // è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦
        Dataset<Row> itemSimilarity = userBehavior
            .groupBy("item_id")
            .agg(collect_list("user_id").as("users"))
            .crossJoin(userBehavior.groupBy("item_id").agg(collect_list("user_id").as("users")))
            .filter(col("item_id").notEqual(col("item_id")))
            // è®¡ç®—Jaccardç›¸ä¼¼åº¦
            .withColumn("similarity", calculateJaccardSimilarity(col("users"), col("users")));
        
        // ä¿å­˜æ¨èç»“æœ
        itemSimilarity.write().mode("overwrite").parquet("hdfs://recommendations");
    }
}
```

### Q7: å¦‚ä½•å¤„ç†Sparkçš„å†…å­˜æº¢å‡ºé—®é¢˜ï¼Ÿ

**A**: è§£å†³æ–¹æ¡ˆï¼š

**1. å¢åŠ Executorå†…å­˜**:
```bash
spark-submit \
  --executor-memory 8g \
  --driver-memory 4g \
  myapp.jar
```

**2. è°ƒæ•´å†…å­˜åˆ†é…æ¯”ä¾‹**:
```bash
--conf spark.memory.fraction=0.6 \
--conf spark.memory.storageFraction=0.5
```

**3. ä½¿ç”¨ç£ç›˜æº¢å†™**:
```java
// ä½¿ç”¨MEMORY_AND_DISKå­˜å‚¨çº§åˆ«
rdd.persist(StorageLevel.MEMORY_AND_DISK());
```

**4. å‡å°‘æ•°æ®é‡**:
```java
// è¿‡æ»¤ä¸éœ€è¦çš„æ•°æ®
JavaRDD<String> filtered = rdd.filter(line -> line.contains("important"));

// ä½¿ç”¨sampleé‡‡æ ·
JavaRDD<String> sampled = rdd.sample(false, 0.1);
```

**5. å¢åŠ åˆ†åŒºæ•°**:
```java
// å¢åŠ åˆ†åŒºæ•°ï¼Œå‡å°‘æ¯ä¸ªåˆ†åŒºçš„æ•°æ®é‡
JavaRDD<String> repartitioned = rdd.repartition(200);
```

**6. é¿å…ä½¿ç”¨collect()**:
```java
// é”™è¯¯: æ”¶é›†æ‰€æœ‰æ•°æ®
List<String> all = rdd.collect();  // OOM!

// æ­£ç¡®: åªæ”¶é›†å¿…è¦çš„æ•°æ®
List<String> sample = rdd.take(100);
rdd.saveAsTextFile("hdfs://output");  // ç›´æ¥ä¿å­˜åˆ°HDFS
```

## ğŸ”— ç›¸å…³èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Apache Sparkå®˜ç½‘](https://spark.apache.org/)
- [Sparkç¼–ç¨‹æŒ‡å—](https://spark.apache.org/docs/latest/programming-guide.html)
- [Spark SQLæŒ‡å—](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark StreamingæŒ‡å—](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Spark MLlibæŒ‡å—](https://spark.apache.org/docs/latest/ml-guide.html)

### æ¨èä¹¦ç±
- ã€ŠSparkå¿«é€Ÿå¤§æ•°æ®åˆ†æã€‹
- ã€ŠSparké«˜çº§æ•°æ®åˆ†æã€‹(ç¬¬2ç‰ˆ)
- ã€Šæ·±å…¥ç†è§£Sparkæ ¸å¿ƒæ€æƒ³ä¸æºç åˆ†æã€‹
- ã€ŠSparkæ€§èƒ½ä¼˜åŒ–æŒ‡å—ã€‹

### åœ¨çº¿èµ„æº
- [Databricksåšå®¢](https://databricks.com/blog)
- [Sparkä¸­æ–‡ç¤¾åŒº](http://spark.apache.org/zh-cn/)
- [Spark Summitè§†é¢‘](https://databricks.com/sparkaisummit)

### ç›¸å…³æŠ€æœ¯
- **Hadoop**: åˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—æ¡†æ¶
- **Flink**: æµå¤„ç†æ¡†æ¶
- **Kafka**: æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå¸¸ä¸Spark Streamingé…åˆ
- **Hive**: æ•°æ®ä»“åº“ï¼Œå¯é€šè¿‡Spark SQLè®¿é—®
- **HBase**: NoSQLæ•°æ®åº“ï¼Œå¯ä½œä¸ºSparkæ•°æ®æº

### å¼€å‘å·¥å…·
- **IntelliJ IDEA**: æ¨èçš„Sparkå¼€å‘IDE
- **Zeppelin**: äº¤äº’å¼æ•°æ®åˆ†æç¬”è®°æœ¬
- **Jupyter**: æ”¯æŒPySparkçš„ç¬”è®°æœ¬
- **Spark UI**: å†…ç½®çš„ç›‘æ§ç•Œé¢

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£Sparkçš„æ ¸å¿ƒæ¶æ„å’Œè®¾è®¡ç†å¿µ
- [ ] æŒæ¡RDDçš„åˆ›å»ºã€è½¬æ¢å’Œè¡ŒåŠ¨æ“ä½œ
- [ ] ç†è§£RDDçš„ä¾èµ–å…³ç³»å’ŒDAGæ‰§è¡Œæµç¨‹
- [ ] æŒæ¡DataFrameå’ŒDataset API
- [ ] èƒ½å¤Ÿä½¿ç”¨Spark SQLè¿›è¡Œæ•°æ®æŸ¥è¯¢
- [ ] ç†è§£Sparkçš„å†…å­˜è®¡ç®—åŸç†
- [ ] æŒæ¡Sparkæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] äº†è§£Sparkçš„éƒ¨ç½²æ¨¡å¼
- [ ] èƒ½å¤Ÿåœ¨Javaé¡¹ç›®ä¸­é›†æˆSpark
- [ ] æŒæ¡Sparkç¨‹åºçš„è°ƒè¯•æ–¹æ³•

---

**@author** erik.zhou
**æœ€åæ›´æ–°**: 2024-01-04
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æ–‡æ¡£æ¥æº**: Apache Sparkå®˜æ–¹æ–‡æ¡£ (Context7)
